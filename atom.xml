<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>落墨</title>
  
  <subtitle>生而无畏，战至终章</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://keysluomo.github.io/"/>
  <updated>2020-06-17T10:53:52.944Z</updated>
  <id>https://keysluomo.github.io/</id>
  
  <author>
    <name>落墨</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>垃圾收集器</title>
    <link href="https://keysluomo.github.io/2020/06/17/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>https://keysluomo.github.io/2020/06/17/垃圾收集器/</id>
    <published>2020-06-17T10:51:43.000Z</published>
    <updated>2020-06-17T10:53:52.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如果说收集算法是内存回收的方法论，那么垃圾收集就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。这里讨论的收集器基于JDK1.7Update14之后的HotSpot虚拟机。</p><h2 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h2><p>Serial收集器是最基本、发展历史最悠久的收集器，曾经是虚拟机新生代收集的唯一选择。我们从名字就可知道，这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。<br>如图示意了Serial、Serial Old收集器的运行过程。<br><img src="https://uploadfiles.nowcoder.com/images/20191112/9094293_1573563026701_55FE04FFAFD803A09D03B580CA415161 &quot;图片标题&quot;" alt="图片说明"> </p><h2 id="ParNew"><a href="#ParNew" class="headerlink" title="ParNew"></a>ParNew</h2><p>ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数（例如：-XX：SurvivorRatio、——XX：PretenureSizeThreshold、-XX：HandlePromotionFailure等）、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。<br>ParNew/Serial Old示意图：</p><p><img src="https://uploadfiles.nowcoder.com/images/20191112/9094293_1573563445447_F6D1DDBD9F7ECB1CFEACF37CD44AAFC8 &quot;图片标题&quot;" alt="ParNew/Serial Old示意图"> </p><h2 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h2><p>Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去和ParNew都一样，那他有什么特别之处呢?<br>Parallen Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能的缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量。所谓吞吐量<br>就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间），虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。<br>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度就能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要是和在后台运算而不需要太多交互的任务。</p><h2 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h2><p>Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备预案，在并发手机发生Concurrent Mode Failure时使用。<br>如图是Serial、Serial Old收集器的运行过程。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191112/9094293_1573563026701_55FE04FFAFD803A09D03B580CA415161 &quot;图片标题&quot;" alt="图片说明"></p><h2 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h2><p>Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器是在JDk1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择。由于老年代Serial Old收集器在服务端应用性能上的拖累，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew和CMS的组合给力。<br>Parallel Old收集器的工作过程如图：</p><p><img src="https://uploadfiles.nowcoder.com/images/20191112/9094293_1573564061197_FA5080FFCFF52BF8E2DA019D00ED27EB &quot;图片标题&quot;" alt="图片说明"> </p><h2 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h2><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常复合这类应用的需求。<br>从名字可以看出，CMS收集器是基于“标记清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤，<br>包括：</p><ul><li>初始标记</li><li>并发标记</li><li>重新标记</li><li>并发清除<br>其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能<br>直接关联到的对象，速度很快，并发标记阶段就是进行GC RootsTracing的过程，而重新标记阶段则是为了修正并<br>发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初<br>始标记阶段稍长一些，但远比并发标记的时间短。<br><img src="https://uploadfiles.nowcoder.com/images/20191112/9094293_1573564770931_7FEC100C446310B8080EDA25BD0AC646 &quot;图片标题&quot;" alt="图片说明"> <h2 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h2>G1（Garbage-First）收集器是当今收集器技术发展的最前沿成果之一。G1是一款面向服务端应用的垃圾收集器。HotSpot开发团队赋予它的使命是（在比较长期的）未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。</li><li>并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。</li><li>分代收集：与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更<br>好的收集效果。</li><li>空间整合：与CMS的“标记—清理”算法不同，G1从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但无论如何，这两种算法都意味着G1运作期间不会产生<br>内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</li><li>可预测的停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。<br>在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤：</li><li>初始标记（Initial Marking）</li><li>并发标记（Concurrent Marking）</li><li>最终标记（Final Marking）</li><li>筛选回收（Live Data Counting and Evacuation）<br><img src="https://uploadfiles.nowcoder.com/images/20191112/9094293_1573564943195_485886224376A4217C43402415E84EF6 &quot;图片标题&quot;" alt="图片说明"> </li></ul>]]></content>
    
    <summary type="html">
    
      如果说收集算法是内存回收的方法论，那么垃圾收集就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。这里讨论的收集器基于JDK1.7Update14之后的HotSpot虚拟机。
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="垃圾收集器" scheme="https://keysluomo.github.io/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>HDFS NameNode的工作机制</title>
    <link href="https://keysluomo.github.io/2020/06/17/HDFS-NameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/"/>
    <id>https://keysluomo.github.io/2020/06/17/HDFS-NameNode的工作机制/</id>
    <published>2020-06-17T10:47:21.000Z</published>
    <updated>2020-06-17T10:49:18.059Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191204/9094293_1575472497353_3141B75B10B6FA1DE399005016B78FB6 &quot;图片标题&quot;" alt=" "> </p><h3 id="1-第一阶段：-namenode-启动"><a href="#1-第一阶段：-namenode-启动" class="headerlink" title="1 第一阶段： namenode 启动"></a>1 第一阶段： namenode 启动</h3><p>1）第一次启动 namenode 格式化后， 创建 fsimage 和 edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>2） 客户端对元数据进行增删改的请求。<br>3） namenode 记录操作日志，更新滚动日志。<br>4） namenode 在内存中对数据进行增删改查。</p><h3 id="2-第二阶段：-Secondary-NameNode-工作"><a href="#2-第二阶段：-Secondary-NameNode-工作" class="headerlink" title="2 第二阶段： Secondary NameNode 工作"></a>2 第二阶段： Secondary NameNode 工作</h3><p>1） Secondary NameNode 询问 namenode 是否需要 checkpoint。 直接带回 namenode 是否检查结果。<br>2） Secondary NameNode 请求执行 checkpoint。<br>3） namenode 滚动正在写的 edits 日志。<br>4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。<br>5） Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。<br>6） 生成新的镜像文件 fsimage.chkpoint。<br>7） 拷贝 fsimage.chkpoint 到 namenode。<br>8） namenode 将 fsimage.chkpoint 重新命名成 fsimage。</p>]]></content>
    
    <summary type="html">
    
      NameNode的工作机制
    
    </summary>
    
      <category term="Hadoop" scheme="https://keysluomo.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://keysluomo.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HDFS(1.0)与(2.0)</title>
    <link href="https://keysluomo.github.io/2020/06/17/HDFS-1-0-%E4%B8%8E-2-0/"/>
    <id>https://keysluomo.github.io/2020/06/17/HDFS-1-0-与-2-0/</id>
    <published>2020-06-17T10:44:41.000Z</published>
    <updated>2020-06-17T10:46:18.971Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当数据集超过一个单独的物理计算机的存储能力时，便有必要将它分部到多个独立的计算机。管理跨计算机网络存储的文件系统成为分布式文件系统。因为它们是基于网络的，所有网络编程的复杂性都会随之而来，所以分布式文件系统比普通磁盘文件系统更复杂。使这个文件系统能容忍节点故障而不损失数据就是一个极大的挑战。</p><h2 id="HDFS的设计"><a href="#HDFS的设计" class="headerlink" title="HDFS的设计"></a>HDFS的设计</h2><p>HDFS是为以流式出局访问模式存储超大文件而设计的文件系统，在商用硬件的集群上运行。让我们看看它的优势：<br><strong>超大文件</strong><br>“超大文件”在这里指几百MB，几百GB甚至几百TB大小的文件。目前已经有Hadoop集群存储PB级的数据了。<br><strong>流式数据</strong><br>HDFS建立在这样一个思想上：一次写入、多次读取模式是最高效的。一个数据集通常由数据源生成或复制，接着在此基础上进行各种各样的分析。每个分析至少都会涉及数据集中的大部分数据（甚至全部）。<br><strong>商用硬件</strong><br>Hadoop不需要运行在昂贵并且高可靠的硬件上。它被设计运行在商用硬件（在各种零售店都能买到的普通硬件）的集群上，因此至少对于大的集群来说，节点故障的几率还是较高的。HDFS在面对这种故障时，被设计为能够继续运行而让用户察觉不到明显的中断。</p><h2 id="HDFS-1-0架构："><a href="#HDFS-1-0架构：" class="headerlink" title="HDFS 1.0架构："></a>HDFS 1.0架构：</h2><p> HDFS采用的是Master/Slave架构一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点。</p><h2 id="HDFS-1-0-的问题"><a href="#HDFS-1-0-的问题" class="headerlink" title="HDFS 1.0 的问题"></a>HDFS 1.0 的问题</h2><p>进入了 PB 级的大数据时代，HDFS 1.0的设计缺陷已经无法满足生产的需求，最致命的问题有以下两点：</p><ul><li>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</li><li>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。  </li></ul><p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p><ul><li>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</li><li>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。  </li></ul><h2 id="HDFS-2-0-的-HA-实现"><a href="#HDFS-2-0-的-HA-实现" class="headerlink" title="HDFS 2.0 的 HA 实现"></a>HDFS 2.0 的 HA 实现</h2><p>这里先看下 HDFS 高可用解决方案的架构设计，如下图</p><p><img src="https://uploadfiles.nowcoder.com/images/20191113/9094293_1573653745515_C6385AA30EAE8DF42909F448F818796D &quot;图片标题&quot;" alt="图片说明"> </p><p>这里与前面 1.0 的架构已经有很大变化，简单介绍一下的组件：</p><ol><li>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li><li>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于Zookeeper 的手动主备切换）；</li><li>Zookeeper 集群：为主备切换控制器提供主备选举支持；</li><li>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</li><li>DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li></ol><h2 id="HDFS-2-0-Federation-实现"><a href="#HDFS-2-0-Federation-实现" class="headerlink" title="HDFS 2.0 Federation 实现"></a>HDFS 2.0 Federation 实现</h2><p>在 1.0 中，HDFS 的架构设计有以下缺点：</p><p>namespace 扩展性差：在单一的 NN 情况下，因为所有 namespace 数据都需要加载到内存，所以物理机内存的大小限制了整个 HDFS 能够容纳文件的最大个数（namespace 指的是 HDFS 中树形目录和文件结构以及文件对应的 block 信息）；<br>性能可扩展性差：由于所有请求都需要经过 NN，单一 NN 导致所有请求都由一台机器进行处理，很容易达到单台机器的吞吐；<br>隔离性差：多租户的情况下，单一 NN 的架构无法在租户间进行隔离，会造成不可避免的相互影响。<br>而 Federation 的设计就是为了解决这些问题，采用 Federation 的最主要原因是设计实现简单，而且还能解决问题。</p><h2 id="Federation-架构"><a href="#Federation-架构" class="headerlink" title="Federation 架构"></a>Federation 架构</h2><p>Federation 的架构设计如下图所示（图片来自 HDFS Federation）：<br><img src="https://uploadfiles.nowcoder.com/images/20191113/9094293_1573657152422_869474068105B25BFF5141288796FA00 &quot;图片标题&quot;" alt="图片说明"> </p><h3 id="HDFS-Federation-架构实现"><a href="#HDFS-Federation-架构实现" class="headerlink" title="HDFS Federation 架构实现"></a>HDFS Federation 架构实现</h3><p>Federation 的核心设计思想<br>Federation 的核心思想是将一个大的 namespace 划分多个子 namespace，并且每个 namespace 分别由单独的 NameNode 负责，这些 NameNode 之间互相独立，不会影响，不需要做任何协调工作（其实跟拆集群有一些相似），集群的所有 DataNode 会被多个 NameNode 共享。</p><p>其中，每个子 namespace 和 DataNode 之间会由数据块管理层作为中介建立映射关系，数据块管理层由若干数据块池（Pool）构成，每个数据块只会唯一属于某个固定的数据块池，而一个子 namespace 可以对应多个数据块池。每个 DataNode 需要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告，并执行来自所有 NameNode 的命令。</p><ol><li>一个 block pool 由属于同一个 namespace 的数据块组成，每个 DataNode 可能会存储集群中所有 block pool 的数据块；</li><li>每个 block pool 内部自治，也就是说各自管理各自的 block，不会与其他 block pool 交流，如果一个 NameNode 挂掉了，不会影响其他 NameNode;</li><li>某个 NameNode 上的 namespace 和它对应的 block pool 一起被称为 namespace volume，它是管理的基本单位。当一个 NameNode/namespace 被删除后，其所有 DataNode 上对应的 block pool 也会被删除，当集群升级时，每个 namespace volume 可以作为一个基本单元进行升级。<br>参考链接：<br><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/</a><br><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      当数据集超过一个单独的物理计算机的存储能力时，便有必要将它分部到多个独立的计算机。管理跨计算机网络存储的文件系统成为分布式文件系统。因为它们是基于网络的，所有网络编程的复杂性都会随之而来，所以分布式文件系统比普通磁盘文件系统更复杂。使这个文件系统能容忍节点故障而不损失数据就是一个极大的挑战。
    
    </summary>
    
      <category term="Hadoop" scheme="https://keysluomo.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://keysluomo.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Flume监控之Ganglia</title>
    <link href="https://keysluomo.github.io/2020/06/17/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/"/>
    <id>https://keysluomo.github.io/2020/06/17/Flume监控之Ganglia/</id>
    <published>2020-06-17T10:39:22.000Z</published>
    <updated>2020-06-17T10:40:50.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&lt;blockquote&gt;<br>Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。今天我们来看看如何用Ganglia来监控我们的flume集群数据。<br>&lt;/blockquote&gt;</p><h1 id="Ganglia的安装与部署"><a href="#Ganglia的安装与部署" class="headerlink" title="Ganglia的安装与部署"></a>Ganglia的安装与部署</h1><h2 id="1-安装httpd服务与php"><a href="#1-安装httpd服务与php" class="headerlink" title="1.安装httpd服务与php"></a>1.安装httpd服务与php</h2><pre><code>[root@bigdata107 flume]# sudo yum -y install httpd php</code></pre><h2 id="2-安装其他依赖"><a href="#2-安装其他依赖" class="headerlink" title="2.安装其他依赖"></a>2.安装其他依赖</h2><pre><code>[root@bigdata107 flume]# sudo yum -y install rrdtool perl-rrdtool rrdtool-devel[root@bigdata107 flume]# sudo yum -y install apr-devel </code></pre><p>##3.安装ganglia</p><pre><code>[root@bigdata107 flume]# rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm[root@bigdata107 flume]# yum -y install ganglia-gmetad[root@bigdata107 flume]# yum -y install ganglia-web[root@bigdata107 flume]# yum install -y ganglia-gmond</code></pre><h2 id="4-修改配置文件"><a href="#4-修改配置文件" class="headerlink" title="4.修改配置文件"></a>4.修改配置文件</h2><pre><code>[root@bigdata107 flume]# vim /etc/httpd/conf.d/ganglia.conf</code></pre><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586182698451_54B043E608B5CEFA3BEABAAA76A297DD &quot;图片标题&quot;" alt="图片说明"> </p><pre><code>[root@bigdata107 flume]# vim /etc/ganglia/gmetad.conf</code></pre><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586182806151_B9DF49E99A6A6E6FBEC8E68B262E2535 &quot;图片标题&quot;" alt="图片说明"> </p><pre><code>[root@bigdata107 flume]# vim /etc/ganglia/gmond.conf</code></pre><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586182861540_7CFA504945AED9BA125B9EAF62A2E8C1 &quot;图片标题&quot;" alt="图片说明"> </p><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586182933318_007860250407A7F699957C0BCE7AD90F &quot;图片标题&quot;" alt="图片说明"> </p><pre><code>[root@bigdata107 flume]#  vim /etc/selinux/config</code></pre><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586182992342_C08995949E2C1C62B523784107DF362C &quot;图片标题&quot;" alt="图片说明"> </p><h2 id="5-selinux本次生效关闭必须重启-如果此时不想重启-可以临时生效"><a href="#5-selinux本次生效关闭必须重启-如果此时不想重启-可以临时生效" class="headerlink" title="5.selinux本次生效关闭必须重启,如果此时不想重启,可以临时生效"></a>5.selinux本次生效关闭必须重启,如果此时不想重启,可以临时生效</h2><pre><code>[root@bigdata107 flume]#  sudo setenforce 0</code></pre><h1 id="启动Ganglia"><a href="#启动Ganglia" class="headerlink" title="启动Ganglia"></a>启动Ganglia</h1><pre><code>[root@bigdata107 flume]# service httpd start正在启动 httpd：[root@bigdata107 flume]# service gmetad startStarting GANGLIA gmetad:  [确定][root@bigdata107 flume]# service gmond startStarting GANGLIA gmond:  [确定]</code></pre><h2 id="1-打开web-UI"><a href="#1-打开web-UI" class="headerlink" title="1.打开web UI"></a>1.打开web UI</h2><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586183489794_90CF7CF84BC2F02B381395E788655501 &quot;图片标题&quot;" alt=" "> </p><h2 id="2-通过Ganglia监控Flume"><a href="#2-通过Ganglia监控Flume" class="headerlink" title="2.通过Ganglia监控Flume"></a>2.通过Ganglia监控Flume</h2><pre><code>[root@bigdata107 flume]# vim flume-env.sh</code></pre><p>添加如下内容：<br>export JAVA_OPTS=&quot;-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.216.107:8649 -Xms100m -Xmx200m&quot;</p><h2 id="3-启动flume任务"><a href="#3-启动flume任务" class="headerlink" title="3.启动flume任务"></a>3.启动flume任务</h2><pre><code>[root@bigdata107 flume]# bin/flume-ng agent --conf conf/ --name agent1 --conf-file job/flume_telnet_logger.conf -Dflume.root.logger==INFO,console -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.216.107:8649</code></pre><h2 id="4-发送数据查看Ganglia监测图"><a href="#4-发送数据查看Ganglia监测图" class="headerlink" title="4.发送数据查看Ganglia监测图"></a>4.发送数据查看Ganglia监测图</h2><pre><code>[root@bigdata107 flume]# telnet bigdata107 44444</code></pre><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586183804208_514336376671171EC30255912BCF2C81 &quot;图片标题&quot;" alt="图片说明"></p><p><img src="https://uploadfiles.nowcoder.com/images/20200406/9094293_1586183911708_0A820C34898B48841ABF621389BC5E04 &quot;图片标题&quot;" alt="图片说明"> </p>]]></content>
    
    <summary type="html">
    
      Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。今天我们来看看如何用Ganglia来监控我们的flume集群数据。
    
    </summary>
    
      <category term="Flume" scheme="https://keysluomo.github.io/categories/Flume/"/>
    
    
      <category term="集群监控" scheme="https://keysluomo.github.io/tags/%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper分布式集群部署</title>
    <link href="https://keysluomo.github.io/2020/06/17/Zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://keysluomo.github.io/2020/06/17/Zookeeper分布式集群部署/</id>
    <published>2020-06-17T10:32:08.000Z</published>
    <updated>2020-06-17T10:34:53.807Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&lt;blockquote&gt;<br>ZooKeeper是一个开放源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。  </p><p>ZooKeeper是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。ZooKeeper可以保证如下分布式一致性特性。<br>今天我们来看看如何分布式安装</p><p>&lt;/blockquote&gt;  </p><h1 id="Zookeeper分布式集群部署"><a href="#Zookeeper分布式集群部署" class="headerlink" title="Zookeeper分布式集群部署"></a>Zookeeper分布式集群部署</h1><h2 id="下载Zookeeper"><a href="#下载Zookeeper" class="headerlink" title="下载Zookeeper"></a>下载Zookeeper</h2><p>这里可以参考官网，下载需要的版本：<br><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">https://zookeeper.apache.org/</a></p><h2 id="解压Zookeeper到我们指定的目录"><a href="#解压Zookeeper到我们指定的目录" class="headerlink" title="解压Zookeeper到我们指定的目录"></a>解压Zookeeper到我们指定的目录</h2><pre><code>[root@bigdata107 software]#  tar -zxvf zookeeper-3.4.10 -C /opt/module/</code></pre><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p><strong>1.修改zoo_sample.cfg zoo.cfg 为zoo.cfg</strong></p><pre><code>[root@bigdata107 software]#  mv zoo_sample.cfg zoo.cfg zoo.cfg</code></pre><p><strong>2.修改配置文件</strong></p><pre><code>[root@bigdata107 software]# vi zoo.cfg</code></pre><p>&lt;blockquote&gt;</p><p>#数据目录需要提前创建  </p><p>dataDir=/opt/module/zookeeper-3.4.5-cdh5.10.0/zkData  </p><p>#server.每个节点服务编号=服务器ip地址：集群通信端口：选举端口<br>server.1=bigdata107 :2888:3888<br>server.2=bigdata108 :2888:3888<br>server.3=bigdata109 :2888:3888<br>&lt;/blockquote&gt;  </p><p><strong>3.分发到各个节点</strong>  </p><pre><code>[root@bigdata107 software]#   scp -r zookeeper-3.4.10/ bigdata108:/opt/module/[root@bigdata107 software]#  scp -r zookeeper-3.4.10/ bigdata109:/opt/module/</code></pre><p><strong>4.创建相关目录</strong></p><pre><code>[root@bigdata107 zookeeper-3.4.10]# mkdir zkData</code></pre><p><strong>5.在各个节点zkData目录下，创建myid</strong></p><pre><code>#bigdata107 节点[root@bigdata107 zookeeper-3.4.10]# touch myid[root@bigdata107 zookeeper-3.4.10]# vi myid1#bigdata108 节点[root@bigdata108 zookeeper-3.4.10]# touch myid[root@bigdata108 zookeeper-3.4.10]# vi myid2#bigdata109 节点[root@bigdata109 zookeeper-3.4.10]# touch myid[root@bigdata109 zookeeper-3.4.10]# vi myid3</code></pre><p><strong>6.启动Zookeeper服务</strong></p><pre><code>#启动Zookeeper服务[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh start#查看各个节点服务状态 [root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh status#关闭各个节点服务[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh stop</code></pre>]]></content>
    
    <summary type="html">
    
      ZooKeeper是一个开放源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。
    
    </summary>
    
      <category term="Zookeeper" scheme="https://keysluomo.github.io/categories/Zookeeper/"/>
    
    
      <category term="集群部署" scheme="https://keysluomo.github.io/tags/%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>JConsole:Java监视与管理控制台</title>
    <link href="https://keysluomo.github.io/2020/06/17/JConsole-Java%E7%9B%91%E8%A7%86%E4%B8%8E%E7%AE%A1%E7%90%86%E6%8E%A7%E5%88%B6%E5%8F%B0/"/>
    <id>https://keysluomo.github.io/2020/06/17/JConsole-Java监视与管理控制台/</id>
    <published>2020-06-16T16:45:19.000Z</published>
    <updated>2020-06-16T16:46:24.911Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>JConsole（Java Monitoring and Management Console）是一种基于JMX的可视化监视、管理工具。<br>1.启动JConsole<br>通过JDK/bin目录下的“jconsole.exe”启动JConsole后，将自动搜索出本机运行的所有虚拟机进程，不需要用户自己再使用jps来查询了，如图双击选择其中一个进程即可开始监控了，也可以使用下面的“远程进程”功能来连接远程服务器，对远程虚拟机进行监控。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573274563736_6654F5DFA4EC7329FE86D88517B8FA93" alt="图片说明" title="图片标题"> </p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573275239073_438AC748ECDB4217D868DFDE95ECFD09" alt=" " title="图片标题"> </p><p>从图中可以看出机器运行了几个本地虚拟机进程，其中OOMTest是我准备的“反面教材”代码双击它进入JConsole主界面，包括“概述”、“内存”、“线程”、“类”、“VM概要”、“MBean”六个页签。如图所示</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573278160273_562106A0F5A1D726229907F196EB6D40" alt="图片说明" title="图片标题"> </p><p>“概述”页签显示的是整个虚拟机主要运行数据的概览，其中包括“堆内存使用情况”、“类”、“CPU使用情况”四种信息的曲线图，这些曲线图是后面“内存”、“线程”、“类”页签的信息汇总。</p><p>2.内存监控<br>“内存”页签相当于可视化的jsat命令，用于监视收集器管理的虚拟机内存（Java堆和永久代）的变化趋势。我们通过运行代码“OOMTest”来体验一下监视功能，运行时设置的虚拟机参数为：-Xms100m -Xmx100m -XX:+UseSerialGC,这段代码的作用是以64KB/50毫秒的速度往Java堆中填充数据，一共填充1000次，使用JConsole的“内存”页签进行监视，观察曲线和柱状图指示图的变化。</p><pre><code>import java.util.ArrayList;import java.util.List;/** * @Author:luomo * @CreateTime:2019/11/9 * @Description:OOMTest */public class OOMTest {    /**     * 内存占位符对象，一个OOMObject大约占64KB     */    static class OOMobject{        public byte[] placeholder=new byte[64*1024];    }    public static void fillHeap(int num) throws InterruptedException{        List&lt;OOMobject&gt; list =new ArrayList&lt;OOMobject&gt;();        for(int i=0;i&lt;num;i++)        {            //稍作延时，令监视曲线的变化更加明显            Thread.sleep(50);            list.add(new OOMobject());        }        System.gc();    }    public static void main(String[] args)throws Exception{        fillHeap(1000);    }}</code></pre><p>程序运行后，在“内存”页签中可以看到内存池Eden区的运行趋势呈现折线状，如图。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282506435_42550CBEF3FCED46D7B42799FAD4566A" alt="图片说明" title="图片标题"> </p><p>监视范围扩大至整个堆后，会发现曲线是一条向上增长的平滑曲线。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282418309_B397F1590C7ED6E011AB02405287D0D0" alt="图片说明" title="图片标题"> </p><p>并且从柱状图可以看出，在1000次循环执行结束，运行了System.gc()后，虽然整个新生代Eden和Survivor区都基本被清空了，但是代表老年代的柱状图仍然保持峰值状态，说明被填充进堆中的数据在System.gc（）方法执行之后仍然存活。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282569188_69314B5631685BFF2D1BB4774E598582" alt="图片说明" title="图片标题"> </p><p>这里有两个小问题供读者思考：<br>1.虚拟机启动参数只限制了Java堆为100MB，没有指定-Xmn参数，能否从监控图中估计出新生代有多大？<br>2.为何执行了System.gc（）之后，图中代表老年代的柱状图仍然显示峰值状态，代码需要如何调整才能让System.gc（）回收掉填充到堆中的对象？<br>问题一：我们可以从图中知道Eden空间大小，因为没有设置-XX：SurvivorRadio参数，所以Eden与Survivor空间比例默认值为8:1，整个新生代空间这样我们可以用Eden空间大小/占新生代空间的比例，得出新生代空间的大小。<br>问题二：执行完System.gc（）之后，空间未能回收是因为List<OOMObject>list对象仍然存活，fillHeap（）方法仍然没有退出，因此list对象在System.gc（）执行时仍然处于作用域之内。如果把System.gc移动到fillHeap()方法外调用就可以回收掉全部内存。 </OOMObject></p>]]></content>
    
    <summary type="html">
    
      JConsole（Java Monitoring and Management Console）是一种基于JMX的可视化监视、管理工具。
    
    </summary>
    
    
      <category term="JVM" scheme="https://keysluomo.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>HotSpot虚拟机对象探秘</title>
    <link href="https://keysluomo.github.io/2020/06/17/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/"/>
    <id>https://keysluomo.github.io/2020/06/17/HotSpot虚拟机对象探秘/</id>
    <published>2020-06-16T16:42:22.000Z</published>
    <updated>2020-06-16T16:43:50.166Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>我们以常用的虚拟机HotSpot和常用的内存区域Java堆为例，深入了解HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。</p><h2 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h2><p>虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有先执行相应的类加载过程。<br>    在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同把一块确定大小的内存从Java堆中划分出来。假设堆中内存是绝对完整的，所有的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，所有用过的内存都放在一边，中间放着一个值指针作为分界点的指示器，这种分配方式成为“指针碰撞”。如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191108/9094293_1573227083274_A840BD9C2CEC84B3EC48E4358DCDAED0 &quot;图片标题&quot;" alt="图片说明"> </p><p>如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个实例，并更新列表上的记录，这种分配方式称为“空闲列表”。如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191108/9094293_1573228003290_CEBD16961637C0135ADE58567B01219F &quot;图片标题&quot;" alt="图片说明"> </p><p>选择哪种分配方式是由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩功能决定。因此，在使用serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。</p><h2 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h2><p>在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（InstanceData）和对齐填充（Padding）。<br>HotSpot虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark Word”。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪<br>个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不<br>一定要经过对象本身<br>第二部分实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。<br>第三部分对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或者2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。</p><h2 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h2><p>建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中的对象的具***置，所以对象访问方式也是取决于虚拟机实现而定的。目前主流的访问方式有使用句柄和直接指针两种。</p><p>如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息，</p><p>如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference中存储的直接就是对象地址，</p>]]></content>
    
    <summary type="html">
    
      我们以常用的虚拟机HotSpot和常用的内存区域Java堆为例，深入了解HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。
    
    </summary>
    
    
      <category term="JVM" scheme="https://keysluomo.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>垃圾收集算法</title>
    <link href="https://keysluomo.github.io/2020/06/15/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/"/>
    <id>https://keysluomo.github.io/2020/06/15/垃圾收集算法/</id>
    <published>2020-06-15T12:00:00.000Z</published>
    <updated>2020-06-15T12:09:10.673Z</updated>
    
    <content type="html"><![CDATA[<h1 id="标记清除算法"><a href="#标记清除算法" class="headerlink" title="标记清除算法"></a>标记清除算法</h1><p><img src="/2020/06/15/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/1.png" alt="图片说明"><br>最基础的收集算法是“标记清除”算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经介绍过了。之所以说他是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。<br>主要不足：</p><ul><li>效率问题，标记和清除两个过程的效率都不高；</li><li>空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能导致以后再程序过程中需要分配较大的对象时，无法找到连续内存而不得不提前触发另一次垃圾收集动作</li></ul><h1 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h1><p><img src="/2020/06/15/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/2.png" alt="图片说明"><br>为了解决效率问题，一种称为复制的收集算法出现了，他将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完了，就将还存活着的对象复制到另一块上面，然后再把已经使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂的情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。<br>主要不足：</p><ul><li>将内存缩小为了原来的一半，在对象存活率较高时就要进行较多的复制操作，效率将会变低。</li></ul><h1 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h1><p><img src="/2020/06/15/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/3.png" alt="图片说明"><br>根据老年代的特点，有人提出了另外一种“标记-整理”算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。<br>主要不足：</p><ul><li>它对内存变动更频繁，需要整理所有存活对象的引用地址，在效率上比复制算法要差很多。</li></ul><h1 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h1><p>当前商业虚拟机的垃圾收集器都采用“分代收集”算法，这种算法并没有什么新的思想，知识根据对象存活周期的不同将内存划分为几块。分代收集算法分代收集算法（Generational Collection）严格来说并不是一种思想或理论，而是融合上述3种基础的算法思想，而产生的针对不同情况所采用不同算法的一套组合拳。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。</p>]]></content>
    
    <summary type="html">
    
      由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本文只是介绍几种算法的思想
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="垃圾收集算法" scheme="https://keysluomo.github.io/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>如何判断对象是否死亡?</title>
    <link href="https://keysluomo.github.io/2020/06/15/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/"/>
    <id>https://keysluomo.github.io/2020/06/15/如何判断对象是否死亡/</id>
    <published>2020-06-15T11:30:34.000Z</published>
    <updated>2020-06-15T12:24:20.305Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”，判断对象的生死存活都有那些算法？</p><h2 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h2><p>实现原理：给对象中添加一个引用计数器，每当一个地方引用它是，计数器值就加1；任何时刻计数器为0的对象就是不可能再被使用的。<br>但是主流的Java虚拟机里面没有选用引用计数算法来管理内存，其中主要的原因是它很难解决对象之间相互引用的问题。<br>如下一个简单的例子：请看测试代码：对象objA和objB都有字段instance，赋值令 objA.instance=objB及objB.instance=objA，实际上这两个对象已经不可能再被访问了，但是他们因为互相引用这对方，导致引用计数都不为0，于是引用计数算法无法通过GC收集器回收他们。</p><pre><code>public class ReferenceCountingGC {    /**     * @Author: luomo     * @CreateTime: 2019/11/4     * @Description: 引用计数算法     */    public Object instance = null;    private  static final int _1MB=1024*1024;    /**     *  这个成员属性的唯一意义就是占点内存，以便能在GC日志中看清楚是否被回收     */    private  byte[] bigSize =new byte[2 * _1MB];    public static void testGC(){        ReferenceCountingGC objA =new  ReferenceCountingGC();        ReferenceCountingGC objB =new ReferenceCountingGC();        objA.instance=objB;        objB.instance=objA;        objA =null;        objB =null;        System.gc();    }    public  static void main(String[] args){        testGC();    }}</code></pre><p>程序运行结果如下：<br><img src="/2020/06/15/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/1.png" alt=" "><br>从运行结果中可以清楚看出虚拟机并没有因为互相引用就不回收他们，这也从侧面说明虚拟机并不是通过引用计数算法来判断对象是否存活的。</p><h2 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h2><p>实现原理：在主流的语言的主流实现中，比如Java、C#、甚至是古老的Lisp都是使用的可达性分析算法来判断对象是否存活的。</p><p>这个算法的核心思路就是通过一些列的“GC Roots”对象作为起始点，从这些对象开始往下搜索，搜索所经过的路径称之为“引用链”。</p><p>当一个对象到GC Roots没有任何引用链相连的时候，证明此对象是可以被回收的。如下图所示：<br><img src="/2020/06/15/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/2.png" alt="图片说明"><br>在Java中，可作为GC Roots对象的列表：</p><ul><li>Java虚拟机栈中的引用对象。</li><li>本地方法栈中JNI（既一般说的Native方法）引用的对象。</li><li>方法区中类静态常量的引用对象。</li><li>方法区中常量的引用对象。</li></ul>]]></content>
    
    <summary type="html">
    
      在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”，判断对象的生死存活都有那些算法？
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="JVM" scheme="https://keysluomo.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Java虚拟机运行时数据区域</title>
    <link href="https://keysluomo.github.io/2020/06/15/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/"/>
    <id>https://keysluomo.github.io/2020/06/15/Java虚拟机运行时数据区域/</id>
    <published>2020-06-15T10:57:52.000Z</published>
    <updated>2020-06-15T12:21:51.783Z</updated>
    
    <content type="html"><![CDATA[<p>#概述<br>&emsp;&emsp;对于从事C、C++程序开发人员来说，在内存管理领域，他们既是拥有最高权力的“皇帝”又是从事最基础工作的“劳动人民”-既拥有每一个对象的“所有权”，又担负着每一个对象生命开始到终结的维护责任。<br>对于Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不在需要为每一个new操作去写配对的delete/free代码，不容易出现内存泄漏和内存溢出问题，由虚拟机管理内存这一切看起来都很美好。然而一旦出现内存泄漏和溢出的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将异常艰难。Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的<br>用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而<br>建立和销毁。</p><h2 id="运行时数据区域"><a href="#运行时数据区域" class="headerlink" title="运行时数据区域"></a>运行时数据区域</h2><p>&emsp;&emsp;Java 虚拟机在执行java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间。<br><img src="/2020/06/15/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/JVM.png" alt="图片说明"> </p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>&emsp;&emsp;程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环跳转、异常处理、线程恢复等基础功能都需要依赖这个技术器来完成。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>&emsp;&emsp;与程序计数器一样，Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的时Java方法执行的内存模型：每个方法在执行的相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个放法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈出栈的过程。  </p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>&emsp;&emsp;本地方法栈与虚拟机栈所发挥的作用是非常相似的，他们之间的区别不过是虚拟机栈为虚拟机执行方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native法服务。在虚拟机规范中对被地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutofMemoryError异常。</p><h3 id="Java堆"><a href="#Java堆" class="headerlink" title="Java堆"></a>Java堆</h3><p>&emsp;&emsp;对于大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有对象实例以及数组都要在堆上分配,但是随着JIT<br>编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换[2]优化技术将会导致一些微妙的变化发生，所有的<br>对象都分配在堆上也渐渐变得不是那么“绝对”了。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p> &emsp;&emsp;   方法区与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名Non-Heap（非堆），目的应该是与Java堆区分开来</p>]]></content>
    
    <summary type="html">
    
      对于从事C、C++程序开发人员来说，在内存管理领域，他们既是拥有最高权力的“皇帝”又是从事最基础工作的“劳动人民”-既拥有每一个对象的“所有权”，又担负着每一个对象生命开始到终结的维护责任。对于Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不在需要为每一个new操作去写配对的delete/free代码，不容易出现内存泄漏和内存溢出问题，由虚拟机管理内存这一切看起来都很美好。然而一旦出现内存泄漏和溢出的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将异常艰难。
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="运行时数据区域" scheme="https://keysluomo.github.io/tags/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>Java线程死锁</title>
    <link href="https://keysluomo.github.io/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/"/>
    <id>https://keysluomo.github.io/2020/06/15/Java线程死锁/</id>
    <published>2020-06-15T10:28:39.000Z</published>
    <updated>2020-06-15T10:42:53.983Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java线程死锁"><a href="#Java线程死锁" class="headerlink" title="Java线程死锁"></a>Java线程死锁</h1><blockquote><br>死锁是一种特定的程序状态，在实体之间，由于循环依赖导致彼此一直处于等待之中，没有 任何个体可以继续前进。死锁不仅仅是在线程之间会发生，存在资源独占的进程之间同样也 可能出现死锁。通常来说，我们大多是聚焦在多线程场景中的死锁，指两个或多个线程之 间，由于互相持有对方需要的锁，而永久处于阻塞的状态。<br><br></blockquote><p><strong>Java线程死锁</strong>是一个经典的多线程问题，因为不同的线程都在等待那些根本不可能被释放的锁，从而导致所有的工作都无法完成。如图所示</p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁.png" alt="图片说明"> </p><hr><h2 id="如何去定位Java线程死锁呢？"><a href="#如何去定位Java线程死锁呢？" class="headerlink" title="如何去定位Java线程死锁呢？"></a>如何去定位Java线程死锁呢？</h2><p>定位死锁最常见的方式就是利用 jstack 等工具获取线程栈，然后定位互相之间的依赖关系，进而找到死锁。如果是比较明显的死锁，往往 jstack 等就能直接定位，类似 JConsole 甚至可以在图形界面进行有限的死锁检测。<br>既然了解了用什么工具去定位线程死锁，那我们模拟一个Java线程死锁的情况，实战定位线程死锁<br><strong>死锁代码：</strong></p><pre><code>/** * @Author:luomo * @CreateTime: 2020/3/28 * @Description:模拟DeadLock */public class deadLock implements Runnable{    public static  Object obj1=new Object();    public static  Object obj2=new Object();    private int flag;    deadLock(int flag){        this.flag=flag;    }    @Override    public void run() {        if(flag==0){            synchronized (obj1){                System.out.println(Thread.currentThread().getName()+&quot;成功获取锁1&quot;);                try {                    Thread.currentThread().sleep(1000);                } catch (InterruptedException e) {                    e.printStackTrace();                }                System.out.println(Thread.currentThread().getName()+&quot;尝试获取锁2&quot;);                synchronized (obj2){                    System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);                }            }        }else{            synchronized (obj2){                System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);                try {                    Thread.currentThread().sleep(1000);                } catch (InterruptedException e) {                    e.printStackTrace();                }                System.out.println(Thread.currentThread().getName()+&quot;尝试获取锁1&quot;);                synchronized (obj1){                    System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);                }            }        }    }    public static void main(String[] args) {   deadLock d1=new deadLock(0);   deadLock d2=new deadLock(1);   Thread thread1=new Thread(d1);   Thread thread2=new Thread(d2);   thread1.start();   thread2.start();    }}</code></pre><p><strong>代码运行：</strong></p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁1.png" alt=" "> </p><p>从运行结果我们看到Thread1和Thread0同时都在争用对方已经占有的锁，进而产生死锁。</p><hr><h2 id="如何定位死锁"><a href="#如何定位死锁" class="headerlink" title="如何定位死锁"></a>如何定位死锁</h2><p>如果程序发生了死锁，我们如何去定位死锁？我们可以通过JConsole工具来发现死锁。<br>打开cmd：输入 JConsole 回车<br>我们可以看到一个可视化的工具，找到死锁进程点击连接</p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁2.png" alt="图片说明"> </p><p>我们可以看到有检查死锁的选项</p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁3.png" alt="图片说明"> </p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁4.png" alt="图片说明"> </p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁5.png" alt="图片说明"> </p><p>通过上图我们可以发现产生死锁的线程，从而定位到发生死锁的代码。</p><p>当然我们还可以使用Jstack + pid的方式来定位问题</p><p><img src="/2020/06/15/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁6.png" alt="图片说明"> </p>]]></content>
    
    <summary type="html">
    
      预览描述
    
    </summary>
    
      <category term="Java" scheme="https://keysluomo.github.io/categories/Java/"/>
    
    
      <category term="死锁" scheme="https://keysluomo.github.io/tags/%E6%AD%BB%E9%94%81/"/>
    
  </entry>
  
  <entry>
    <title>Kafka实现高吞吐之零拷贝</title>
    <link href="https://keysluomo.github.io/2020/06/15/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/"/>
    <id>https://keysluomo.github.io/2020/06/15/Kafka实现高吞吐之零拷贝/</id>
    <published>2020-06-15T10:04:27.000Z</published>
    <updated>2020-06-15T10:48:43.056Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Kafka是一个非常优秀的消息开源系统，作为分布式的消息队列之所以能够实现高吞吐，其中的一个原因就是sendFile 的零拷贝</p><h2 id="关于零拷贝"><a href="#关于零拷贝" class="headerlink" title="关于零拷贝"></a>关于零拷贝</h2><p>&quot;零拷贝&quot;中的&quot;拷贝&quot;是操作系统在I/O操作中,将数据从一个内存区域复制到另外一个内存区域. 而&quot;零&quot;并不是指0次复制, 更多的是指在用户态和内核态之前的复制是0次.</p><h2 id="CPU-COPY"><a href="#CPU-COPY" class="headerlink" title="CPU COPY"></a>CPU COPY</h2><p>通过计算机的组成原理我们知道, 内存的读写操作是需要CPU的协调数据总线,地址总线和控制总线来完成的<br>因此在&quot;拷贝&quot;发生的时候,往往需要CPU暂停现有的处理逻辑,来协助内存的读写.这种我们称为CPU COPY，CPU COPY不但占用了CPU资源,还占用了总线的带宽.</p><h2 id="DMA-COPY"><a href="#DMA-COPY" class="headerlink" title="DMA COPY"></a>DMA COPY</h2><p>DMA(DIRECT MEMORY ACCESS)是现代计算机的重要功能. 它的一个重要 的特点就是, 当需要与外设进行数据交换时, CPU只需要初始化这个动作便可以继续执行其他指令,剩下的数据传输的动作完全由DMA来完成，可以看到DMA COPY是可以避免大量的CPU中断的</p><h2 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h2><p>本文中的上下文切换时指由用户态切换到内核态, 以及由内核态切换到用户态<br>存在多次拷贝的原因？</p><ul><li><p>操作系统为了保护系统不被应用程序有意或无意地破坏,为操作系统设置了用户态和内核态两种状态.用户态想要获取系统资源(例如访问硬盘), 必须通过系统调用进入到内核态, 由内核态获取到系统资源,再切换回用户态返回应用程序.</p></li><li><p>出于&quot;readahead cache&quot;和异步写入等等性能优化的需要, 操作系统在内核态中也增加了一个&quot;内核缓冲区&quot;(kernel buffer). 读取数据时并不是直接把数据读取到应用程序的buffer, 而先读取到kernel buffer, 再由kernel buffer复制到应用程序的buffer. 因此,数据在被应用程序使用之前,可能需要被多次拷贝  </p></li></ul><p>所有<strong>涉及到数据传输</strong>的场景, 无非就一种:从硬盘上读取文件数据, 发送到网络上去。<br>这个场景我们简化为一个模型:</p><p> File.read(fileDesc, buf, len);<br> Socket.send(socket, buf, len);<br>为了方便描述,上面这两行代码, 我们给它起个名字: read-send模型</p><p>操作系统在实现这个read-send模型时,需要有以下步骤:</p><ol><li>应用程序开始读文件的操作</li><li>应用程序发起系统调用, 从用户态切换到内核态(第一次上下文切换)</li><li>内核态中把数据从硬盘文件读取到内核中间缓冲区(kernel buf)</li><li>数据从内核中间缓冲区(kernel buf)复制到(用户态)应用程序缓冲区(app buf),从内核态切换回到用户态(第二次上下文切换)</li><li>应用程序开始发送数据到网络上</li><li>应用程序发起系统调用,从用户态切换到内核态(第三次上下文切换)</li><li>内核中把数据从应用程序(app buf)的缓冲区复制到socket的缓冲区(socket)</li><li>内核中再把数据从socket的缓冲区(socket buf)发送的网卡的缓冲区(NIC buf)上</li><li>从内核态切换回到用户态(第四次上下文切换)</li></ol><p><img src="/2020/06/15/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/kafka.png" alt="图片说明"> </p><p>由上图可以很清晰地看到, 一次read-send涉及到了四次拷贝:</p><ol><li>硬盘拷贝到内核缓冲区(DMA COPY)</li><li>内核缓冲区拷贝到应用程序缓冲区(CPU COPY)</li><li>应用程序缓冲区拷贝到socket缓冲区(CPU COPY)</li><li>socket buf拷贝到网卡的buf(DMA COPY)  </li></ol><p>其中涉及到2次cpu中断, 还有4次的上下文切换，很明显,第2次和第3次的的copy只是把数据复制到app buffer又原封不动的复制回来, 为此带来了两次的cpu copy和两次上下文切换, 是完全没有必要的，linux的零拷贝技术就是为了优化掉这两次不必要的拷贝</p><h2 id="sendFile"><a href="#sendFile" class="headerlink" title="sendFile"></a>sendFile</h2><p>linux内核2.1开始引入一个叫sendFile系统调用,这个系统调用可以在内核态内把数据从内核缓冲区直接复制到套接字(SOCKET)缓冲区内, 从而可以减少上下文的切换和不必要数据的复制<br>有了sendFile这个系统调用后, 我们read-send模型就可以简化为:</p><ol><li>应用程序开始读文件的操作</li><li>应用程序发起系统调用, 从用户态切换到内核态(第一次上下文切换)</li><li>内核态中把数据从硬盘文件读取到内核中间缓冲区</li><li>通过sendFile,在内核态中把数据从内核缓冲区复制到socket的缓冲区</li><li>内核中再把数据从socket的缓冲区发送的网卡的buf上</li><li>从内核态切换到用户态(第二次上下文切换)  </li></ol><p><img src="/2020/06/15/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/kafka1.png" alt="图片说明"><br>如图所示：<br>涉及到数据拷贝变成:</p><ol><li>硬盘拷贝到内核缓冲区(DMA COPY)</li><li>内核缓冲区拷贝到socket缓冲区(CPU COPY)</li><li>socket缓冲区拷贝到网卡的buf(DMA COPY)</li></ol><p>可以看到,一次read-send模型中, 利用sendFile系统调用后, 可以将4次数据拷贝减少到3次, 4次上下文切换减少到2次, 2次CPU中断减少到1次，<br>相对传统I/O, 这种零拷贝技术通过减少两次上下文切换, 1次cpu copy, <strong>可以将I/O性能提高50%以上(网络数据, 未亲测)</strong>，开始的术语中说到, 所谓的<strong>零拷贝的&quot;零&quot;</strong>, <strong>是指用户态和内核态之间的拷贝次数为0</strong>, 从这个定义上来说, 现在的这个零拷贝技术已经是真正的&quot;零&quot;了</p>]]></content>
    
    <summary type="html">
    
      Kafka是一个非常优秀的消息开源系统，作为分布式的消息队列之所以能够实现高吞吐，其中的一个原因就是sendFile 的零拷贝
    
    </summary>
    
      <category term="Kafka" scheme="https://keysluomo.github.io/categories/Kafka/"/>
    
    
      <category term="零拷贝" scheme="https://keysluomo.github.io/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 概述</title>
    <link href="https://keysluomo.github.io/2020/06/14/Kafka-%E6%A6%82%E8%BF%B0/"/>
    <id>https://keysluomo.github.io/2020/06/14/Kafka-概述/</id>
    <published>2020-06-14T06:08:08.000Z</published>
    <updated>2020-06-14T06:11:03.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kafka诞生的背景"><a href="#kafka诞生的背景" class="headerlink" title="kafka诞生的背景"></a>kafka诞生的背景</h1><p>对于一个高效的组织，所有数据需要对该组织的所有服务和系统是可用的，以便挖掘出数据的最大价值。数据采集和数据使用是一个金字塔的结构，底部为以某种统一的方式捕获数据，这些数据需要以统一的方式建模，以方便读取和处理。捕获数据的工作做扎实后，在这个基础上以不同方法处理这些数据就变得得心应手。<br>数据捕获的来源主要有两种：一种是记录正在发生的事件数据。比如Web系统中的用户活动日志（用户的点击选择等）、交警行业中的违章事件等。随着传统行业业务活动的数字化，事件数据正在不断增长，而且这个趋势没有停止。这种类型的事件数据记录了已经发生的事情，往往比传统数据库应用要大好几个数量级。因此对于数据的捕获、数据的处理提出了重大的挑战；另一种是经过二次分析处理之后的数据。对捕获的数据进行二次分析处理后得到的数据也需要记录保存，这里的处理指的是利用批处理、图分析等专有的数据处理系统进行了处理，这些加工后的数据可以作为数据捕获的第二个来源。总之，捕获的数据越来越多，如何将这些巨量的数据以可靠的、完整的数据流方式传递给数据分析处理系统也变得越来越困难。</p><h1 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h1><p>在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。<br>1）Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。<br>2）Kafka最初是由LinkedIn公司开发，并于    2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。<br>3）Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。<br>4）无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。</p><h1 id="消息队列内部实现原理"><a href="#消息队列内部实现原理" class="headerlink" title="消息队列内部实现原理"></a>消息队列内部实现原理</h1><p><img src="https://uploadfiles.nowcoder.com/images/20190920/9094293_1568975462389_04F96407B00295C6D53D599B4F14DC90" alt="图片说明" title="图片标题"> </p><p>  （1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）<br>点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。<br>（2）发布/订阅模式（一对多，数据生产后，推送给所有订阅者）<br>发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。</p><h1 id="为什么需要消息队列"><a href="#为什么需要消息队列" class="headerlink" title="为什么需要消息队列"></a>为什么需要消息队列</h1><p>1）解耦：<br>　　允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。<br>2）冗余：<br>消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。<br>3）扩展性：<br>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。<br>4）灵活性 &amp; 峰值处理能力：<br>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。<br>5）可恢复性：<br>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。<br>6）顺序保证：<br>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）<br>7）缓冲：<br>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。<br>8）异步通信：<br>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p><h1 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h1><p><img src="https://uploadfiles.nowcoder.com/images/20190920/9094293_1568975497955_62B3F472CB3D859915FB4C3B0BBEBAAE" alt="图片说明" title="图片标题"><br>1）Producer ：消息生产者，就是向kafka broker发消息的客户端。<br>2）Consumer ：消息消费者，向kafka broker取消息的客户端<br>3）Topic ：可以理解为一个队列。<br>4） Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制-给consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。<br>5）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。<br>6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。<br>7）Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</p><h1 id="分布式模型"><a href="#分布式模型" class="headerlink" title="分布式模型"></a>分布式模型</h1><p>   Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本）。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本出现故障时，备份副本中的一个副本会被选择为新的主副本。因为每个分区的副本中只有主副本接受读写，所以每个服务器端都会作为某些分区的主副本，以及另外一些分区的备份副本，这样Kafka集群的所有服务端整体上对客户端是负载均衡的。<br>       Kafka的生产者和消费者相对于服务器端而言都是客户端。<br>Kafka生产者客户端发布消息到服务端的指定主题，会指定消息所属的分区。生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡；消息有键时，根据分区语义（例如hash）确保相同键的消息总是发送到同一分区。<br>       Kafka的消费者通过订阅主题来消费消息，并且每个消费者都会设置一个消费组名称。因为生产者发布到主题的每一条消息都只会发送给消费者组的一个消费者。所以，如果要实现传统消息系统的“队列”模型，可以让每个消费者都拥有相同的消费组名称，这样消息就会负责均衡到所有的消费者；如果要实现“发布-订阅”模型，则每个消费者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。<br>       分区是消费者现场模型的最小并行单位。如下图（图1）所示，生产者发布消息到一台服务器的3个分区时，只有一个消费者消费所有的3个分区。在下图（图2）中，3个分区分布在3台服务器上，同时有3个消费者分别消费不同的分区。假设每个服务器的吞吐量时300MB，在下图（图1）中分摊到每个分区只有100MB，而在下图（图2）中，集群整体的吞吐量有900MB。可以看到，增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。<br>       同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，这样每个消费者都可以分配到数量均等的分区。Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费者组，或者有消费者离开消费组，都会触发再平衡操作。<br><img src="https://uploadfiles.nowcoder.com/images/20190920/9094293_1568975534470_558AC9E4B632C16EBB53304E1BB3518A" alt="图片说明" title="图片标题"><br>Kafka的消费者消费消息时，只保证在一个分区内的消息的完全有序性，并不保证同一个主题汇中多个分区的消息顺序。而且，消费者读取一个分区消息的顺序和生产者写入到这个分区的顺序是一致的。比如，生产者写入“hello”和“Kafka”两条消息到分区P1，则消费者读取到的顺序也一定是“hello”和“Kafka”。如果业务上需要保证所有消息完全一致，只能通过设置一个分区完成，但这种做法的缺点是最多只能有一个消费者进行消费。一般来说，只需要保证每个分区的有序性，再对消息假设键来保证相同键的所有消息落入同一分区，就可以满足绝大多数的应用。</p>]]></content>
    
    <summary type="html">
    
      Kafka诞生的背景
    
    </summary>
    
      <category term="Kafka" scheme="https://keysluomo.github.io/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="https://keysluomo.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spark Demo(Serializable)</title>
    <link href="https://keysluomo.github.io/2020/06/14/Spark-Demo-Serializable/"/>
    <id>https://keysluomo.github.io/2020/06/14/Spark-Demo-Serializable/</id>
    <published>2020-06-14T04:44:46.000Z</published>
    <updated>2020-06-14T04:46:44.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本文中将介绍spark中Task执行序列化的开发问题</p><h1 id="开发环境准备"><a href="#开发环境准备" class="headerlink" title="开发环境准备"></a>开发环境准备</h1><p>本实验Spark运行在Windows上，为了开发Spark应用程序，在本地机器上需要有Jdk1.8和Maven环境。<br>确保我们的环境配置正常，我们可以使用快捷键 Win+R 输入cmd：<br>环境如下：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572341256892_8D8973B8667179A2319B041328F690BD" alt="图片说明" title="图片标题"><br>程序开发工具我们使用IDEA，创建maven项目，添加pom依赖</p><h1 id="编写Spark程序"><a href="#编写Spark程序" class="headerlink" title="编写Spark程序"></a>编写Spark程序</h1><h2 id="目录结构如下"><a href="#目录结构如下" class="headerlink" title="目录结构如下:"></a>目录结构如下:</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572341490332_07386CADB58D4E8A80763A83F9F6B34D" alt="图片说明" title="图片标题"> </p><h2 id="创建Serializable-scala"><a href="#创建Serializable-scala" class="headerlink" title="创建Serializable.scala:"></a>创建Serializable.scala:</h2><p>首先我们需要了解RDD中的函数传递：<br>在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要主要的是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。<br>如果我们对我们自定义的类不进行序列化：</p><pre><code>package SparkDemoimport org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}/**  * @Author: luomo  * @CreateTime: 2019/10/29  * @Description: Serializable from Driver to Executor  */object Serializable {  def main(args: Array[String]): Unit = {    //创建Spark上下文对象    val config:SparkConf =new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;Serializable&quot;)    //创建Spark上下文对象    val sc = new SparkContext(config)    val rdd:RDD[String] = sc.parallelize(Array(&quot;hadoop&quot;,&quot;spark&quot;,&quot;hive&quot;,&quot;Flink&quot;))    val search = new Search(&quot;h&quot;)    val match1:RDD[String] =search.getMatch1(rdd)    match1.collect().foreach(println)    sc.stop()  }  class Search(query:String){    //过滤出包含字符串的数据    def  isMatch(s:String):Boolean ={      s.contains(query)    }    //过滤出包含字符串的RDD    def getMatch1(rdd:RDD[String]) :RDD[String] = {      rdd.filter(isMatch)    }    //过滤出包含字符串的RDD    def getMatche2(rdd: RDD[String]): RDD[String] ={      rdd.filter(x=&gt; x.contains(query))    }  }}</code></pre><p>如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572346997711_619AA778B91D5D25CAB063D1C7D53FFE" alt="图片说明" title="图片标题"> </p><p>可见，对于自己定义的普通类，Spark是无法直接将其序列化的。<br>需要我们自定义的类继承java.io.Serializable</p><pre><code>package SparkDemoimport org.apache.spark.rdd.RDDimport org.apache.spark.{SparkConf, SparkContext}/**  * @Author: luomo  * @CreateTime: 2019/10/29  * @Description: Serializable from Driver to Executor  */object Serializable {  def main(args: Array[String]): Unit = {    //创建Spark上下文对象    val config:SparkConf =new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;Serializable&quot;)    //创建Spark上下文对象    val sc = new SparkContext(config)    val rdd:RDD[String] = sc.parallelize(Array(&quot;hadoop&quot;,&quot;spark&quot;,&quot;hive&quot;,&quot;Flink&quot;))    val search = new Search(&quot;h&quot;)    val match1:RDD[String] =search.getMatch1(rdd)    match1.collect().foreach(println)    sc.stop()  }  //自定义类  class Search(query:String) extends  java.io.Serializable {    //过滤出包含字符串的数据    def  isMatch(s:String):Boolean ={      s.contains(query)    }    //过滤出包含字符串的RDD    def getMatch1(rdd:RDD[String]) :RDD[String] = {      rdd.filter(isMatch)    }    //过滤出包含字符串的RDD    def getMatche2(rdd: RDD[String]): RDD[String] ={      rdd.filter(x=&gt; x.contains(query))    }  }}</code></pre><h1 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h1><p>如图我们过滤出包含字符h的字符串：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572351464995_DFD27DCB713FEB02D8628E6C67FD4273" alt="图片说明" title="图片标题"> </p>]]></content>
    
    <summary type="html">
    
      在本文中将介绍spark中Task执行序列化的开发问题
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
      <category term="Serializable" scheme="https://keysluomo.github.io/tags/Serializable/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark简单构建一个应用程序</title>
    <link href="https://keysluomo.github.io/2020/06/14/Apache-Spark%E7%AE%80%E5%8D%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"/>
    <id>https://keysluomo.github.io/2020/06/14/Apache-Spark简单构建一个应用程序/</id>
    <published>2020-06-14T03:52:44.000Z</published>
    <updated>2020-06-14T04:02:37.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简单的构建一个Apache-Spark应用程序"><a href="#简单的构建一个Apache-Spark应用程序" class="headerlink" title="简单的构建一个Apache Spark应用程序"></a>简单的构建一个Apache Spark应用程序</h1><h2 id="开发环境的准备："><a href="#开发环境的准备：" class="headerlink" title="开发环境的准备："></a>开发环境的准备：</h2><p>Spark 可以运行在 Linux, Max OS X, 或者是 Windows 上。这里我是在Windows上运行的。在本地机器上需要有Java8.x和maven环境，另外我们推荐使用 ItelliJ IDEA 作为 Flink 应用程序的开发 IDE。<br>首先在我们的pom.xml文件中添加Spark相关的依赖。<br>pom如下：</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;SparkDemo&lt;/groupId&gt;  &lt;artifactId&gt;SparkDemo&lt;/artifactId&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;inceptionYear&gt;2008&lt;/inceptionYear&gt;  &lt;properties&gt;    &lt;scala.version&gt;2.11.1&lt;/scala.version&gt;  &lt;/properties&gt;  &lt;repositories&gt;    &lt;repository&gt;      &lt;id&gt;scala-tools.org&lt;/id&gt;      &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt;      &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;    &lt;/repository&gt;  &lt;/repositories&gt;  &lt;pluginRepositories&gt;    &lt;pluginRepository&gt;      &lt;id&gt;scala-tools.org&lt;/id&gt;      &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt;      &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;    &lt;/pluginRepository&gt;  &lt;/pluginRepositories&gt;  &lt;dependencies&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;      &lt;artifactId&gt;scala-library&lt;/artifactId&gt;      &lt;version&gt;${scala.version}&lt;/version&gt;    &lt;/dependency&gt;    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;      &lt;version&gt;2.2.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;junit&lt;/groupId&gt;      &lt;artifactId&gt;junit&lt;/artifactId&gt;      &lt;version&gt;4.4&lt;/version&gt;      &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.specs&lt;/groupId&gt;      &lt;artifactId&gt;specs&lt;/artifactId&gt;      &lt;version&gt;1.2.5&lt;/version&gt;      &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;    &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;    &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;        &lt;executions&gt;          &lt;execution&gt;            &lt;goals&gt;              &lt;goal&gt;compile&lt;/goal&gt;              &lt;goal&gt;testCompile&lt;/goal&gt;            &lt;/goals&gt;          &lt;/execution&gt;        &lt;/executions&gt;        &lt;configuration&gt;          &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt;          &lt;args&gt;            &lt;arg&gt;-target:jvm-1.5&lt;/arg&gt;          &lt;/args&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;        &lt;configuration&gt;          &lt;downloadSources&gt;true&lt;/downloadSources&gt;          &lt;buildcommands&gt;            &lt;buildcommand&gt;ch.epfl.lamp.sdt.core.scalabuilder&lt;/buildcommand&gt;          &lt;/buildcommands&gt;          &lt;additionalProjectnatures&gt;            &lt;projectnature&gt;ch.epfl.lamp.sdt.core.scalanature&lt;/projectnature&gt;          &lt;/additionalProjectnatures&gt;          &lt;classpathContainers&gt;            &lt;classpathContainer&gt;org.eclipse.jdt.launching.JRE_CONTAINER&lt;/classpathContainer&gt;            &lt;classpathContainer&gt;ch.epfl.lamp.sdt.launching.SCALA_CONTAINER&lt;/classpathContainer&gt;          &lt;/classpathContainers&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;  &lt;reporting&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;        &lt;configuration&gt;          &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/reporting&gt;&lt;/project&gt;</code></pre><p>工作目录：<br><img src="https://uploadfiles.nowcoder.com/images/20191024/9094293_1571913840705_C0C094E4A298BFA098F8123797C7F7CC" alt="图片说明" title="图片标题"><br>编写Spark程序：<br>创建WordCount程序：</p><pre><code>package SparkDemoimport org.apache.spark.{SparkConf, SparkContext}import org.apache.spark.rdd.RDD/**  * @Author: luomo  * @CreateTime: 2019/10/24  * @Description:WordCount  */object WordCount {  def main(args: Array[String]): Unit = {    //配置文件    val conf = new SparkConf().      setAppName(&quot;wordcount&quot;) // 运行时候的作业名称      .setMaster(&quot;local&quot;)    //上下文 拿着Conf信息创建出来 写spark应用程序的对象  通往集群的入口    val sc = new SparkContext(conf)    //传入文件对象 返回RDD集合    val input = sc.textFile(&quot;E:///test.txt&quot;)    //对文件行数据 按照空格切割 返回RDD集合 得到每个单词    val lines = input.flatMap(line =&gt; line.split(&quot; &quot;))    //统计单词数量 计数 得到RDD集合 按照相同的Key先分组，之后再对组内的Value进行操作    val count = lines.map(word =&gt; (word, 1)).reduceByKey(_ + _)    //将结果遍历打印到控制台    count.foreach(x =&gt;{      println(x)    })    //将结果输出到文件中    val output = count.saveAsTextFile(&quot;E:///wordCount&quot;)    //关闭流 在内存中释放这个spark对象    //sc.stop()  }}</code></pre><p>运行程序如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191024/9094293_1571914229568_474EA976C3A9A912CB36A0631F855823" alt="图片说明" title="图片标题"> </p>]]></content>
    
    <summary type="html">
    
      预览描述
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark Error</title>
    <link href="https://keysluomo.github.io/2020/06/14/Spark-Error/"/>
    <id>https://keysluomo.github.io/2020/06/14/Spark-Error/</id>
    <published>2020-06-14T03:45:08.000Z</published>
    <updated>2020-06-14T03:48:29.263Z</updated>
    
    <content type="html"><![CDATA[<h1 id="配置historyserver时："><a href="#配置historyserver时：" class="headerlink" title="配置historyserver时："></a>配置historyserver时：</h1><p><img src="https://uploadfiles.nowcoder.com/images/20191016/9094293_1571227263283_97221A1B1C45BED99055FB1996A900CE" alt="图片说明" title="图片标题"> </p><p>因为端口被占用了</p><h1 id="SparkStreaming-updateStateByKeyde-使用时-–checkpoint报错"><a href="#SparkStreaming-updateStateByKeyde-使用时-–checkpoint报错" class="headerlink" title="SparkStreaming updateStateByKeyde 使用时 –checkpoint报错"></a>SparkStreaming updateStateByKeyde 使用时 –checkpoint报错</h1><p><img src="https://uploadfiles.nowcoder.com/images/20191016/9094293_1571227382925_460CCA764930F915D29C8DC785013F26" alt="图片说明" title="图片标题"> </p><p>原因在使用的时候，并没有设置checkPoint 检测点<br>检测点的目的就是为了保存上一次的结果数据。如果没有检测点的话，那么将无法保存上一次结果<br>如果要保证中间数据不丢失的话，可以借助其它的工具，如hdfs<br>ssc.checkpoint(“hdfs://kd0301:9000/spark_checkpoint/”)</p>]]></content>
    
    <summary type="html">
    
      预览描述
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
      <category term="Error" scheme="https://keysluomo.github.io/tags/Error/"/>
    
  </entry>
  
  <entry>
    <title>Spark压缩文件性能分析</title>
    <link href="https://keysluomo.github.io/2020/06/14/Spark%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>https://keysluomo.github.io/2020/06/14/Spark压缩文件性能分析/</id>
    <published>2020-06-14T02:52:53.000Z</published>
    <updated>2020-06-14T05:03:14.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark压缩文件性能分析"><a href="#Spark压缩文件性能分析" class="headerlink" title="Spark压缩文件性能分析"></a>Spark压缩文件性能分析</h1><p>HDFS上分布式文件存储，成为大数据平台首选存储平台。而Spark往往以HDFS文件为输入，为保持兼容性，Spark支持多种格式文件读取，大数据场景下，性能瓶颈往往是IO，而不是CPU算力，所以对文件的压缩处理成为了很必要的手段。Spark为提供兼容性，同时支持多种压缩包直接读取，方便于用户使用，不用提前对压缩格式处理，但各种压缩格式各有优缺点，若不注意将导致Spark的能力无法发挥出来。故对Spark计算压缩文件做一个分析。</p><h1 id="支持的压缩格式"><a href="#支持的压缩格式" class="headerlink" title="支持的压缩格式"></a>支持的压缩格式</h1><p>首先来看一下Spark读取HDFS文件常用的压缩格式：<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106237246_09ACB263F52577BA2C36F0D60E304A75" alt="图片说明" title="图片标题"></p><h1 id="执行对比分析"><a href="#执行对比分析" class="headerlink" title="执行对比分析"></a>执行对比分析</h1><p>实验数据：同一个文件包，json格式文件数据<br>处理逻辑：增加列，然后发送到kafka中。<br>DAG逻辑划分：两个job（read动作一个job，foreach动作一个job），每个job下面各一个stage，每个stage下面task若干<br>程序执行参数：–master yarn –deploy-mode client –executor-cores 4 –executor-memory 4G –num-executors 4</p><h2 id="非压缩文件"><a href="#非压缩文件" class="headerlink" title="非压缩文件"></a>非压缩文件</h2><p>文件大小：33.7GB<br>运行时间：9min</p><h3 id="read阶段："><a href="#read阶段：" class="headerlink" title="read阶段："></a>read阶段：</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106293920_47AEB6D162BF532944DBDAB53E3D2EAB" alt="图片说明" title="图片标题"><br>可以看到所有节点都在读取，分布式读取，速度很快。<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106336406_317ADCB2DB2F35A583847F38535B7718" alt="图片说明" title="图片标题"></p><p>Stage里面共计分成了252个task，每个读取128MB数据。</p><h3 id="foreach阶段"><a href="#foreach阶段" class="headerlink" title="foreach阶段"></a>foreach阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106365520_C40B30CD2EE17428DAC6E44372D32E64" alt="图片说明" title="图片标题"><br>依然并行全力计算<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106387308_25162C0F4A622B24CE97415EFFDE198A" alt="图片说明" title="图片标题"><br>每个执行节点上4个core都在并发运行。</p><h2 id="GZIP"><a href="#GZIP" class="headerlink" title="GZIP"></a>GZIP</h2><p>文件大小：10.6GB<br>运行时间：2.2h</p><p>###read阶段<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106404340_1FCD14A02942D89F2391B8A13BF3F1BF" alt="图片说明" title="图片标题"><br>只有单节点读取</p><p>同时该节点上也只有一个核心在运行<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106440480_1A2BEF1D5E0E50929495F9B550115FBA" alt="图片说明" title="图片标题"><br>foreach阶段</p><p>也是只有单节点、单core运行</p><h2 id="BZIP2"><a href="#BZIP2" class="headerlink" title="BZIP2"></a>BZIP2</h2><p>文件大小：7.7GB<br>运行时间：12min</p><h3 id="read阶段"><a href="#read阶段" class="headerlink" title="read阶段"></a>read阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106466729_53072EE252372595149335F751CD1394" alt="图片说明" title="图片标题"><br>与非压缩一致，并行进行</p><h3 id="foreach阶段-1"><a href="#foreach阶段-1" class="headerlink" title="foreach阶段"></a>foreach阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106481625_67EEB8DB9FD25C46B3486090E777B5D7" alt="图片说明" title="图片标题"><br>同样并发执行</p><h2 id="SNAPPY"><a href="#SNAPPY" class="headerlink" title="SNAPPY"></a>SNAPPY</h2><p>文件大小：16.5GB<br>运行时间：2.1h<br>这里直接采用的整文件压缩，所以文件不可分割。</p><h3 id="read阶段-1"><a href="#read阶段-1" class="headerlink" title="read阶段"></a>read阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106498372_E01A7A3A8C35CDBBFDFD4A58BA865021" alt="图片说明" title="图片标题"></p><p>单节点单核心读取，非并行。</p><h3 id="foreach阶段-2"><a href="#foreach阶段-2" class="headerlink" title="foreach阶段"></a>foreach阶段</h3><p>同上类似，单节点单核心运行<br>结果<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106539620_64157194D9DFAFAA77BE73D5A02CE37B" alt="图片说明" title="图片标题"><br>gzip和snappy无法采用并行计算，也就是说在spark平台上，这两种格式只能采用串行单进程执行，于本文开头表格对应，无法分割（splittable）的压缩格式只能顺序一个进程读取，而读取后多文件又在一个executor上，其他executor无文件导致无法并行的foreach。<br>bz2和非压缩格式支持分割，也就是说可以并行读取以及计算。<br>不可分割的压缩格式文件不可并行读取，完全无法发挥spark的并行计算优势，并且若压缩包过大，对单节点的物理性能要求较高。<br>建议<br>snappy采用分块压缩方式使其可以并行读取计算。<br>gzip格式最好提前进行分割成小文件或者换格式，因多个文件可以并行读取。另一个办法是read文件后调用repartition操作强制将读取多数据重新均匀分配到不同的executor上，但这个操作会导致大量单节点性能占用，因此该格式建议不在spark上使用。<br>bz2表现相同于非压缩，但解压操作需要耗费时间。<br><strong>非压缩性能表现最佳，但会占用过大HDFS存储。</strong><br>spark输出压缩文件<br>实际生产环境需要spark输出文件到HDFS，并且为了节省空间会使用压缩格式，以下介绍几种常用的压缩格式<br>文本文件压缩</p><h2 id="bzip2"><a href="#bzip2" class="headerlink" title="bzip2"></a>bzip2</h2><p>压缩率最高，压缩解压速度较慢，支持split。</p><pre><code>import org.apache.hadoop.io.compress.BZip2Codec//rdd.saveAsTextFile(&quot;codec/bzip2&quot;,classOf[BZip2Codec])</code></pre><h2 id="snappy"><a href="#snappy" class="headerlink" title="snappy"></a>snappy</h2><p>json文本压缩率 38.2%，压缩和解压缩时间短。</p><pre><code>import org.apache.hadoop.io.compress.SnappyCodec//rdd.saveAsTextFile(&quot;codec/snappy&quot;,classOf[SnappyCodec])</code></pre><h2 id="gzip"><a href="#gzip" class="headerlink" title="gzip"></a>gzip</h2><p>压缩率高，压缩和解压速度较快，不支持split，如果不对文件大小进行控制，下次分析可能可能会造成效率低下的问题。<br>json文本压缩率23.5%，适合使用率低，长期存储的文件。</p><pre><code>import org.apache.hadoop.io.compress.GzipCodec//rdd.saveAsTextFile(&quot;codec/gzip&quot;,classOf[GzipCodec])</code></pre><h2 id="parquet文件压缩"><a href="#parquet文件压缩" class="headerlink" title="parquet文件压缩"></a>parquet文件压缩</h2><p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！<br>转换 1 TB 数据将花费多长时间？<br>50 分钟，在一个 6 数据节点的 Spark v1.5.1 集群上可达到约 20 GB/分的吞吐量。使用的总内存约为 500GB。HDFS 上最终的 Parquet 文件的格式为：</p><pre><code>.../user/spark/data/parquet/catalog_page/part-r-00000-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet/user/spark/data/parquet/catalog_page/part-r-00001-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet...</code></pre><h3 id="存储节省"><a href="#存储节省" class="headerlink" title="存储节省"></a>存储节省</h3><p>以下 Linux 输出显示了 TEXT 和 PARQUET 在 HDFS 上的大小比较：</p><pre><code>% hadoop fs -du -h -s /user/spark/hadoopds1000g897.9 G /user/spark/hadoopds1000g% hadoop fs -du -h -s /user/spark/data/parquet231.4 G /user/spark/data/parquet</code></pre><p>1 TB 数据的存储节省了将近 75%！<br>parquet为文件提供了列式存储，查询时只会取出需要的字段和分区，对IO性能的提升非常大，同时占用空间较小，即使是parquet的uncompressed存储方式也比普通的文本要小的多。<br>spark中通过对parquet文件进行存储，spark2.0后默认使用snappy压缩，1.6.3及以前版本默认使用的gzip压缩方式。</p><p>dataset.write().parquet(“path”);</p><p>可以通过<br>spark.sql.parquet.compression.codec</p><p>参数或是在代码中进行修改配置压缩方式。<br>sparkConf.set(“spark.sql.parquet.compression.codec”,”gzip”)</p><p>parquet存储提供了<br>lzo gzip snappy uncompressed</p><p>参考文章<br><a href="https://zturn.cc/?p=24" target="_blank" rel="noopener">https://zturn.cc/?p=24</a><br><a href="https://blog.csdn.net/bajinsheng/article/details/100031359" target="_blank" rel="noopener">https://blog.csdn.net/bajinsheng/article/details/100031359</a><br><a href="https://www.ibm.com/developerworks/cn/analytics/blog/ba-parquet-for-spark-sql/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/analytics/blog/ba-parquet-for-spark-sql/index.html</a></p>]]></content>
    
    <summary type="html">
    
      HDFS上分布式文件存储，成为大数据平台首选存储平台。而Spark往往以HDFS文件为输入，为保持兼容性，Spark支持多种格式文件读取，大数据场景下，性能瓶颈往往是IO，而不是CPU算力，所以对文件的压缩处理成为了很必要的手段。Spark为提供兼容性，同时支持多种压缩包直接读取，方便于用户使用，不用提前对压缩格式处理，但各种压缩格式各有优缺点，若不注意将导致Spark的能力无法发挥出来。故对Spark计算压缩文件做一个分析。
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
      <category term="性能调优" scheme="https://keysluomo.github.io/tags/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>Spark 概述</title>
    <link href="https://keysluomo.github.io/2020/06/12/Spark-%E6%A6%82%E8%BF%B0/"/>
    <id>https://keysluomo.github.io/2020/06/12/Spark-概述/</id>
    <published>2020-06-12T05:12:22.000Z</published>
    <updated>2020-06-14T02:41:46.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据处理框架"><a href="#大数据处理框架" class="headerlink" title="大数据处理框架"></a>大数据处理框架</h1><p>集群环境对于编程来说带来了很多挑战，首先就是并行化：这就要求我们以并行化的方式重写应用程序，以便我们可以利用更大范围节点的计算能力。集群环境的第二个挑战就是对单点失败的处理，节点宕机以及个别节点计算缓慢在集群环境中非常普遍，这会极大地影响程序的性能。最后一个挑战是集群在大多数情况下都会被多个用户分享，那么动态地进行计算资源的分配，也会干扰程序的执行。因此，针对集群环境出现了大量的大数据编程框架。首先我们要提到的就是Google的MapReduce，它给我们展示了一个简单通用和自动容错的批处理计算模型。<br>但是对于其他类型的计算，比如交互式和流式计算，MapReduce并不适合，这也导致了大量的不同于MapReduce的专有的数据处理模型的出现，比如Storm、Impala和GraphLab。随着新模型的不断出现，似乎对于大数据处理而言，我们应对不同类型的作业需要一系列不同的处理框架才能很好地完成。但是这些专有系统也有一些不足。</p><ul><li>重复工作：许多专有系统在解决同样的问题，比如分布式作业以及容错。举例来说，一个分布式的SQL引擎或者一个机器学习系统都需要实现并行聚合。这些问题在每个专有系统中会重复地被解决。</li><li>组合问题：在不同的系统之间进行组合计算是一件费力又不讨好的事情。对于特定的大数据应用程序而言，中间数据集是非常大的，而且移动的成本也非常高昂。在目前的环境中，我们需要将数据复制到稳定的存储系统中（比如HDFS），以便在不同的计算引擎中进行分享。然而，这样的复制可能比真正的计算所花费的代价要大，所以以流水线的形式将多个系统组合起来效率并不高。</li><li>适用范围的局限性：如果一个应用不适合一个专有的计算系统，那么使用者只能换一个系统，或者重写一个新的计算系统。</li><li>资源分配：在不同的计算引擎之间进行资源的动态共享是比较困难的，因为大多数的计算引擎都会假设它们在程序运行结束之前拥有相同的机器节点的资源。</li><li>管理问题：对于多个专有系统，需要花费更多的精力和时间来管理和部署。<br>尤其是对于终端使用者而言，他们需要学习多种API和系统模型。  </li></ul><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p>针对MapReduce及各种专有系统中出现的不足，伯克利大学推出了全新的统一大数据处理框架Spark，创新性地提出了RDD概念（一种新的抽象的弹性数据集），在某种程度上Spark是对MapReduce模型的一种扩展。要在MapReduce上实现其不擅长的计算工作（比如迭代式、交互式和流式），看上去是一件非常困难的事情，其实主要的原因是MapReduce缺乏一种特性，即在并行计算的各个阶段进行有效的数据共享，这种共享就是RDD的本质。利用这种有效的数据共享和类似MapReduce的操作接口，上述的各种专有类型计算都能够有效地表达，而且能够获得与专有系统同等的性能。<br>特别值得一提的是，从前对于集群处理的容错方式，比如MapReduce和Dryad，是将计算构建成为一个有向无环图的任务集。而这只能允许它们有效地重新计算部分DAG。在单独的计算之间（在迭代的计算步骤之间），除了复制文件，这些模型没有提供其他的存储抽象，这就显著地增加了在网络之间复制文件的代价。RDD能够适应当前大部分的数据并行算法和编程模型。  </p><h1 id="RDD表达能力"><a href="#RDD表达能力" class="headerlink" title="RDD表达能力"></a>RDD表达能力</h1><p>可以使用RDD实现很多现有的集群编程模型以及一些以前的模型不支持的新应用。在这些模型中，RDD能够取得和专有系统同样的性能，还能提供包括容错处理、滞后节点（straggler node）处理等这些专有系统缺乏的特性。这里会重点讨论如下四类模型。</p><ul><li>迭代算法：这是目前专有系统实现的非常普遍的一种应用场景，比如迭代算法可以用于图处理和机器学习。RDD能够很好地实现这些模型，包括Pregel、HaLoop和GraphLab等模型。</li><li>关系型查询：对于MapReduce来说非常重要的需求就是运行SQL查询，包括长期运行、数小时的批处理作业和交互式的查询。然而对于MapReduce而言，对比并行数据库进行交互式查询，有其内在的缺点，比如由于其容错的模型而导致速度很慢。利用RDD模型，可以通过实现许多通用的数据库引擎特性，从而获得非常好的性能。</li><li>MapReduce 批处理：RDD提供的接口是MapReduce的超集，所以RDD可以有效地运行利用MapReduce实现的应用程序，另外RDD还适合更加抽象的基于DAG的应用程序，比如DryadLINQ。</li><li>流式处理：目前的流式系统也只提供了有限的容错处理，需要消耗系统非常大的拷贝代价或者非常长的容错时间。特别是在目前的系统中，基本都是基于连续计算的模型，常驻的有状态的操作会处理到达的每一条记录。为了恢复失败的节点，它们需要为每一个操作复制两份操作，或者是将上游的数据进行代价非常大的操作重放。利用RDD实现一种新的模型——离散数据流（D-Stream），可以克服上面的这些问题。D-Stream将流式计算当作一系列的短小而确定的批处理操作，而不是常驻的有状态的操作，将两个离散流之间的状态保存在RDD中。离散流模型能够允许通过RDD的继承关系图（lineage）进行并行性的恢复而不需要进行数据拷贝。  </li></ul><h1 id="Spark子系统"><a href="#Spark子系统" class="headerlink" title="Spark子系统"></a>Spark子系统</h1><p>如果按照目前流行的大数据处理场景来划分，可以将大数据处理分为如下三种情况。</p><ul><li>复杂的批量数据处理（batch data processing），通常的时间跨度为数十分钟到数小时。</li><li>基于历史数据的交互式查询（interactive query），通常的时间跨度为数十秒到数分钟。</li><li>基于实时数据流的数据处理（streaming data processing），通常的时间跨度为数百毫秒到数秒。<br>由于RDD具有丰富的表达能力，所以伯克利在Spark Core的基础之上衍生出了能够同时处理上述三种情形的统一大数据处理平台，如图1-1所示。<br><div align="center"><img src="/2020/06/12/Spark-%E6%A6%82%E8%BF%B0/spark.png" alt="图片说明"></div></li><li>Spark Core：基于RDD提供了丰富的操作接口，利用DAG进行统一的任务规划，使得Spark能够更加灵活地处理类似MapReduce的批处理作业。</li><li>Shark/Spark SQL:兼容Hive的接口HQL，提供了比Hive高出10~100倍的查询速度的分布式SQL引擎。</li><li>Spark Streaming：将流式计算分解成一系列的短小的批处理作业，利用Spark轻量级和低延时的调度框架，可以很好的支持流式处理。目前已经支持的数据输入源包括Kafka、Flume、Twitter、TCP sockets。</li><li>GraphX：基于Spark的图计算框架，兼容Pregel和GraphLab接口，增强了图构建以及图转换功能。</li><li>MLlib:Spark Core 天然地非常适合于迭代式运算，MLlib就是构建在Spark上的机器学习算法库。目前已经可以支持常用的分类算法、聚类算法、推荐算法等。<br>Spark生态系统的目标就是将批处理、交互式处理、流式处理融合到同一个软件栈中。对于最终的用户或者是开发者而言，Spark生态系统有如下特性。</li><li>Spark生态系统兼容Hadoop生态系统。这个特性对于最终用户至关重要，虽然Spark 通用引擎在一定程度上是用来取代MapReduce系统的，但是Spark能够完美兼容Hadoop生态中的HDFS和YARN等其他组件，使得现有的Hadoop用户能够非常容易地迁移到Spark系统中。图1-2显示了Spark与Hadoop生态的兼容性。<br><div align="center"><img src="/2020/06/12/Spark-%E6%A6%82%E8%BF%B0/1-2.png" alt></div></li><li>Spark生态系统学习成本很低。要实现一个相对完整的端到端解决方案，以前需要部署维护多个专有系统，现在只需要一个Spark系统。另外，如果开发者对Spark Core的原理有比较深入的理解，对构架在Spark Core之上的其他组件的运用将会非常容易。图1-3对比了Spark生态和其他大数据专有系统的代码量。在图1-3中的Spark一项中，批处理对应Spark Core，交互式处理对应Shark/Spark SQL，流计算对应Spark Streaming，而图计算对应GraphX。<br><div align="center"><img src="/2020/06/12/Spark-%E6%A6%82%E8%BF%B0/1-3.png" alt></div></li><li>Spark 性能表现优异。由于Spark利用DAG进行调度执行规划，所以在多任务计算以及迭代计算中能够大量减少磁盘I/O的时间。另外，对于每一项任务启动一个线程，而不是启动一个进程，大大缩短了任务启动时间。</li><li>Spark有强大的社区支持。Spark近一年多来保持非常迅猛的发展势头，被誉为大数据处理的未来。Spark的创始团队成立了Databricks公司，全力支持Spark 生态的发展。目前Hadoop商业版本发行公司中，已经有Cloudera、Hortonworks、MapR等公司相继宣布支持Spark软件栈。图1-4显示了Spark不同版本发布时对社区做出贡献的贡献者数量变化情况。<br><div align="center"><img src="/2020/06/12/Spark-%E6%A6%82%E8%BF%B0/1-4.png" alt></div></li><li>Spark 支持多种语言编程接口。Spark生态本身是使用Scala语言编写的，但是考虑到其流行性，因此Spark从一开始就支持Java和Python接口，方便Spark 程序开发者自由选择。</li></ul>]]></content>
    
    <summary type="html">
    
      预览描述
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://keysluomo.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 状态管理与容错机制</title>
    <link href="https://keysluomo.github.io/2020/06/12/Apache-Flink-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
    <id>https://keysluomo.github.io/2020/06/12/Apache-Flink-状态管理与容错机制/</id>
    <published>2020-06-12T04:56:07.000Z</published>
    <updated>2020-06-12T12:14:33.389Z</updated>
    
    <content type="html"><![CDATA[<h1 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h1><h3 id="作者：孙梦瑶"><a href="#作者：孙梦瑶" class="headerlink" title="作者：孙梦瑶"></a>作者：孙梦瑶</h3><h3 id="整理：韩非"><a href="#整理：韩非" class="headerlink" title="整理：韩非"></a>整理：韩非</h3><h3 id="校对：邱从贤（山智）"><a href="#校对：邱从贤（山智）" class="headerlink" title="校对：邱从贤（山智）"></a>校对：邱从贤（山智）</h3><p><a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>本文主要分享内容如下：</p><p>状态管理的基本概念；</p><p>状态的类型与使用示例；</p><p>容错机制与故障恢复。</p><h1 id="一-状态管理的基本概念"><a href="#一-状态管理的基本概念" class="headerlink" title="一. 状态管理的基本概念"></a>一. 状态管理的基本概念</h1><h2 id="1-什么是状态"><a href="#1-什么是状态" class="headerlink" title="1.什么是状态"></a>1.什么是状态</h2><p>  <img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571142947113_E857A8299D24A087FF0E55FCF75A5E09" alt="图片说明" title="图片标题">  </p><p>首先举一个无状态计算的例子：消费延迟计算。假设现在有一个消息队列，消息队列中有一个生产者持续往消费队列写入消息，多个消费者分别从消息队列中读取消息。从图上可以看出，生产者已经写入 16 条消息，Offset 停留在 15 ；有 3 个消费者，有的消费快，而有的消费慢。消费快的已经消费了 13 条数据，消费者慢的才消费了 7、8 条数据。</p><p>如何实时统计每个消费者落后多少条数据，如图给出了输入输出的示例。可以了解到输入的时间点有一个时间戳，生产者将消息写到了某个时间点的位置，每个消费者同一时间点分别读到了什么位置。刚才也提到了生产者写入了 15 条，消费者分别读取了 10、7、12 条。那么问题来了，怎么将生产者、消费者的进度转换为右侧示意图信息呢？</p><p>consumer 0 落后了 5 条，consumer 1 落后了 8 条，consumer 2 落后了 3 条，根据 Flink 的原理，此处需进行 Map 操作。Map 首先把消息读取进来，然后分别相减，即可知道每个 consumer 分别落后了几条。Map 一直往下发，则会得出最终结果。</p><p>大家会发现，在这种模式的计算中，无论这条输入进来多少次，输出的结果都是一样的，因为单条输入中已经包含了所需的所有信息。消费落后等于生产者减去消费者。生产者的消费在单条数据中可以得到，消费者的数据也可以在单条数据中得到，所以相同输入可以得到相同输出，这就是一个无状态的计算。</p><p>相应的什么是有状态的计算？<br><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571142977408_909F1111218DA0F992FEE083105341CF" alt="图片说明" title="图片标题"><br>以访问日志统计量的例子进行说明，比如当前拿到一个 Nginx 访问日志，一条日志表示一个请求，记录该请求从哪里来，访问的哪个地址，需要实时统计每个地址总共被访问了多少次，也即每个 API 被调用了多少次。可以看到下面简化的输入和输出，输入第一条是在某个时间点请求 GET 了 /api/a；第二条日志记录了某个时间点 Post /api/b ;第三条是在某个时间点 GET了一个 /api/a，总共有 3 个 Nginx 日志。从这 3 条 Nginx 日志可以看出，第一条进来输出 /api/a 被访问了一次，第二条进来输出 /api/b 被访问了一次，紧接着又进来一条访问 api/a，所以 api/a 被访问了 2 次。不同的是，两条 /api/a 的 Nginx 日志进来的数据是一样的，但输出的时候结果可能不同，第一次输出 count=1 ，第二次输出 count=2，说明相同输入可能得到不同输出。输出的结果取决于当前请求的 API 地址之前累计被访问过多少次。第一条过来累计是 0 次，count = 1，第二条过来 API 的访问已经有一次了，所以 /api/a 访问累计次数 count=2。单条数据其实仅包含当前这次访问的信息，而不包含所有的信息。要得到这个结果，还需要依赖 API 累计访问的量，即状态。</p><p>这个计算模式是将数据输入算子中，用来进行各种复杂的计算并输出数据。这个过程中算子会去访问之前存储在里面的状态。另外一方面，它还会把现在的数据对状态的影响实时更新，如果输入 200 条数据，最后输出就是 200 条结果。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143001685_B490A739A55C2916AA8993039A8E9D17" alt="图片说明" title="图片标题"><br>什么场景会用到状态呢？下面列举了常见的 4 种：</p><p>去重：比如上游的系统数据可能会有重复，落到下游系统时希望把重复的数据都去掉。去重需要先了解哪些数据来过，哪些数据还没有来，也就是把所有的主键都记录下来，当一条数据到来后，能够看到在主键当中是否存在。</p><p>窗口计算：比如统计每分钟 Nginx 日志 API 被访问了多少次。窗口是一分钟计算一次，在窗口触发前，如 08:00 ~ 08:01 这个窗口，前59秒的数据来了需要先放入内存，即需要把这个窗口之内的数据先保留下来，等到 8:01 时一分钟后，再将整个窗口内触发的数据输出。未触发的窗口数据也是一种状态。</p><p>机器学习/深度学习：如训练的模型以及当前模型的参数也是一种状态，机器学习可能每次都用有一个数据集，需要在数据集上进行学习，对模型进行一个反馈。</p><p>访问历史数据：比如与昨天的数据进行对比，需要访问一些历史数据。如果每次从外部去读，对资源的消耗可能比较大，所以也希望把这些历史数据也放入状态中做对比。</p><h2 id="2-为什么要管理状态"><a href="#2-为什么要管理状态" class="headerlink" title="2. 为什么要管理状态"></a>2. 为什么要管理状态</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143038696_E074DE090177F98DB5E17EA0EEA487FD" alt="图片说明" title="图片标题"><br>管理状态最直接的方式就是将数据都放到内存中，这也是很常见的做法。比如在做 WordCount 时，Word 作为输入，Count 作为输出。在计算的过程中把输入不断累加到 Count。</p><p>但对于流式作业有以下要求：</p><p>7*24小时运行，高可靠；</p><p>数据不丢不重，恰好计算一次；</p><p>数据实时产出，不延迟；</p><p>基于以上要求，内存的管理就会出现一些问题。由于内存的容量是有限制的。如果要做 24 小时的窗口计算，将 24 小时的数据都放到内存，可能会出现内存不足；另外，作业是 7*24，需要保障高可用，机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，保证运行的作业不受影响；此外，考虑横向扩展，假如网站的访问量不高，统计每个 API 访问次数的程序可以用单线程去运行，但如果网站访问量突然增加，单节点无法处理全部访问数据，此时需要增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点也问题之一。因此，将数据都放到内存中，并不是最合适的一种状态管理方式。</p><h2 id="3-理想的状态管理"><a href="#3-理想的状态管理" class="headerlink" title="3. 理想的状态管理"></a>3. 理想的状态管理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143056691_3C14E7779B855AD529EC7BDD67713F1A" alt="图片说明" title="图片标题"><br>最理想的状态管理需要满足易用、高效、可靠三点需求：</p><p>易用，Flink 提供了丰富的数据结构、多样的状态组织形式以及简洁的扩展接口，让状态管理更加易用；<br>高效，实时作业一般需要更低的延迟，一旦出现故障，恢复速度也需要更快；当处理能力不够时，可以横向扩展，同时在处理备份时，不影响作业本身处理性能；<br>可靠，Flink 提供了状态持久化，包括不丢不重的语义以及具备自动的容错能力，比如 HA，当节点挂掉后会自动拉起，不需要人工介入。</p><h1 id="二-Flink-状态的类型与使用示例"><a href="#二-Flink-状态的类型与使用示例" class="headerlink" title="二. Flink 状态的类型与使用示例"></a>二. Flink 状态的类型与使用示例</h1><h2 id="1-Managed-State-amp-Raw-State"><a href="#1-Managed-State-amp-Raw-State" class="headerlink" title="1. Managed State &amp; Raw State"></a>1. Managed State &amp; Raw State</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143093887_CDD5C3A073E8EDF8BCE5DBC3F331915E" alt="图片说明" title="图片标题"><br>Managed State 是 Flink 自动管理的 State，而 Raw State 是原生态 State，两者的区别如下：</p><p>从状态管理方式的方式来说，Managed State 由 Flink Runtime 管理，自动存储，自动恢复，在内存管理上有优化；而 Raw State 需要用户自己管理，需要自己序列化，Flink 不知道 State 中存入的数据是什么结构，只有用户自己知道，需要最终序列化为可存储的数据结构。</p><p>从状态数据结构来说，Managed State 支持已知的数据结构，如 Value、List、Map 等。而 Raw State只支持字节数组 ，所有状态都要转换为二进制字节数组才可以。</p><p>从推荐使用场景来说，Managed State 大多数情况下均可使用，而 Raw State 是当 Managed State 不够用时，比如需要自定义 Operator 时，推荐使用 Raw State。</p><h2 id="2-Keyed-State-amp-Operator-State"><a href="#2-Keyed-State-amp-Operator-State" class="headerlink" title="2. Keyed State &amp; Operator State"></a>2. Keyed State &amp; Operator State</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143125792_7A7126166D67C3FDDB90CEFFD7505ED8" alt="图片说明" title="图片标题"><br>Managed State 分为两种，一种是 Keyed State；另外一种是 Operator State。在Flink Stream模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream 。</p><p>每个 Key 对应一个 State，即一个 Operator 实例处理多个 Key，访问相应的多个 State，并由此就衍生了 Keyed State。Keyed State 只能用在 KeyedStream 的算子中，即在整个程序中没有 keyBy 的过程就没有办法使用 KeyedStream。</p><p>相比较而言，Operator State 可以用于所有算子，相对于数据源有一个更好的匹配方式，常用于 Source，例如 FlinkKafkaConsumer。相比 Keyed State，一个 Operator 实例对应一个 State，随着并发的改变，Keyed State 中，State 随着 Key 在实例间迁移，比如原来有 1 个并发，对应的 API 请求过来，/api/a 和 /api/b 都存放在这个实例当中；如果请求量变大，需要扩容，就会把 /api/a 的状态和 /api/b 的状态分别放在不同的节点。由于 Operator State 没有 Key，并发改变时需要选择状态如何重新分配。其中内置了 2 种分配方式：一种是均匀分配，另外一种是将所有 State 合并为全量 State 再分发给每个实例。</p><p>在访问上，Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个Rich Function。Operator State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。在数据结构上，Keyed State 支持的数据结构，比如 ValueState、ListState、ReducingState、AggregatingState 和 MapState；而 Operator State 支持的数据结构相对较少，如 ListState。</p><h2 id="3-Keyed-State-使用示例"><a href="#3-Keyed-State-使用示例" class="headerlink" title="3. Keyed State 使用示例"></a>3. Keyed State 使用示例</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143165677_9823447C87064C14565BD91D2B3A3434" alt="图片说明" title="图片标题"><br>Keyed State 有很多种，如图为几种 Keyed State 之间的关系。首先 State 的子类中一级子类有 ValueState、MapState、AppendingState。AppendingState 又有一个子类 MergingState。MergingState 又分为 3 个子类分别是ListState、ReducingState、AggregatingState。这个继承关系使它们的访问方式、数据结构也存在差异。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143194008_77861EEA917B2DB9735F3A5C9AC49413" alt="图片说明" title="图片标题"><br>几种 Keyed State 的差异具体体现在：</p><p>ValueState 存储单个值，比如 Wordcount，用 Word 当 Key，State 就是它的 Count。这里面的单个值可能是数值或者字符串，作为单个值，访问接口可能有两种，get 和 set。在 State 上体现的是 update(T) / T value()。</p><p>MapState 的状态数据类型是 Map，在 State 上有 put、remove等。需要注意的是在 MapState 中的 key 和 Keyed state 中的 key 不是同一个。</p><p>ListState 状态数据类型是 List，访问接口如 add、update 等。</p><p>ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值，原因在于其中的 add 方法不是把当前的元素追加到列表中，而是把当前元素直接更新进了 Reducing 的结果中。</p><p>AggregatingState 的区别是在访问接口，ReducingState 中 add（T）和 T get() 进去和出来的元素都是同一个类型，但在 AggregatingState 输入的 IN，输出的是 OUT。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143217922_688BF6B699D86F2D4A0129F59E6CC1F5" alt="图片说明" title="图片标题"><br>下面以 ValueState 为例，来阐述一下具体如何使用，以状态机的案例来讲解 。</p><p>源代码地址：<a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java" target="_blank" rel="noopener">https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java</a></p><p>感兴趣的同学可直接查看完整源代码，在此截取部分。如图为 Flink 作业的主方法与主函数中的内容，前面的输入、后面的输出以及一些个性化的配置项都已去掉，仅保留了主干。</p><p>首先 events 是一个 DataStream，通过 env.addSource 加载数据进来，接下来有一个 DataStream 叫 alerts，先 keyby 一个 sourceAddress，然后在 flatMap 一个StateMachineMapper。StateMachineMapper 就是一个状态机，状态机指有不同的状态与状态间有不同的转换关系的结合，以买东西的过程简单举例。首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货。已付款，待发货的状态再来一个事件发货，订单状态将会变为配送中，配送中的状态再来一个事件签收，则该订单的状态就变为已签收。在整个过程中，随时都可以来一个事件，取消订单，无论哪个状态，一旦触发了取消订单事件最终就会将状态转移到已取消，至此状态就结束了。</p><p>Flink 写状态机是如何实现的？首先这是一个 RichFlatMapFunction，要用 Keyed State getRuntimeContext，getRuntimeContext 的过程中需要 RichFunction，所以需要在 open 方法中获取 currentState ，然后 getState，currentState 保存的是当前状态机上的状态。</p><p>如果刚下订单，那么 currentState 就是待付款状态，初始化后，currentState 就代表订单完成。订单来了后，就会走 flatMap 这个方法，在 flatMap 方法中，首先定义一个 State，从 currentState 取出，即 Value，Value 取值后先判断值是否为空，如果 sourceAddress state 是空，则说明没有被使用过，那么此状态应该为刚创建订单的初始状态，即待付款。然后赋值 state = State.Initial，注意此处的 State 是本地的变量，而不是 Flink 中管理的状态，将它的值从状态中取出。接下来在本地又会来一个变量，然后 transition，将事件对它的影响加上，刚才待付款的订单收到付款成功的事件，就会变成已付款，待发货，然后 nextState 即可算出。此外，还需要判断 State 是否合法，比如一个已签收的订单，又来一个状态叫取消订单，会发现已签收订单不能被取消，此时这个状态就会下发，订单状态为非法状态。</p><p>如果不是非法的状态，还要看该状态是否已经无法转换，比如这个状态变为已取消时，就不会在有其他的状态再发生了，此时就会从 state 中 clear。clear 是所有的 Flink 管理 keyed state 都有的公共方法，意味着将信息删除，如果既不是一个非法状态也不是一个结束状态，后面可能还会有更多的转换，此时需要将订单的当前状态 update ，这样就完成了 ValueState 的初始化、取值、更新以及清零，在整个过程中状态机的作用就是将非法的状态进行下发，方便下游进行处理。其他的状态也是类似的使用方式。</p><h1 id="三-容错机制与故障恢复"><a href="#三-容错机制与故障恢复" class="headerlink" title="三. 容错机制与故障恢复"></a>三. 容错机制与故障恢复</h1><h2 id="1-状态如何保存及恢复"><a href="#1-状态如何保存及恢复" class="headerlink" title="1. 状态如何保存及恢复"></a>1. 状态如何保存及恢复</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143252142_BE28DFB6669E19340E15658720E17D19" alt="图片说明" title="图片标题"><br>Flink 状态保存主要依靠 Checkpoint 机制，Checkpoint 会定时制作分布式快照，对程序中的状态进行备份。分布式快照是如何实现的可以参考【第二课时】的内容，这里就不再阐述分布式快照具体是如何实现的。分布式快照 Checkpoint 完成后，当作业发生故障了如何去恢复？假如作业分布跑在 3 台机器上，其中一台挂了。这个时候需要把进程或者线程移到 active 的 2 台机器上，此时还需要将整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态，然后从该点开始继续处理。</p><p>如果要从 Checkpoint 恢复，必要条件是数据源需要支持数据重新发送。Checkpoint恢复后， Flink 提供两种一致性语义，一种是恰好一次，一种是至少一次。在做 Checkpoint时，可根据 Barries 对齐来判断是恰好一次还是至少一次，如果对齐，则为恰好一次，否则没有对齐即为至少一次。如果只有一个上游，也就是说 Barries 是不需要对齐的的；如果只有一个 Checkpoint 在做，不管什么时候从 Checkpoint 恢复，都会恢复到刚才的状态；如果有多个上游，假如一个上游的 Barries 到了，另一个 Barries 还没有来，如果这个时候对状态进行快照，那么从这个快照恢复的时候其中一个上游的数据可能会有重复。</p><p>Checkpoint 通过代码的实现方法如下：</p><p>首先从作业的运行环境 env.enableCheckpointing 传入 1000，意思是做 2 个 Checkpoint 的事件间隔为 1 秒。Checkpoint 做的越频繁，恢复时追数据就会相对减少，同时 Checkpoint 相应的也会有一些 IO 消耗。</p><p>接下来是设置 Checkpoint 的 model，即设置了 Exactly_Once 语义，表示需要 Barrier 对齐，这样可以保证消息不会丢失也不会重复。</p><p>setMinPauseBetweenCheckpoints 是 2 个 Checkpoint 之间最少是要等 500ms，也就是刚做完一个 Checkpoint。比如某个 Checkpoint 做了700ms，按照原则过 300ms 应该是做下一个 Checkpoint，因为设置了 1000ms 做一次 Checkpoint 的，但是中间的等待时间比较短，不足 500ms 了，需要多等 200ms，因此以这样的方式防止 Checkpoint 太过于频繁而导致业务处理的速度下降。</p><p>setCheckpointTimeout 表示做 Checkpoint 多久超时，如果 Checkpoint 在 1min 之内尚未完成，说明 Checkpoint 超时失败。</p><p>setMaxConcurrentCheckpoints 表示同时有多少个 Checkpoint 在做快照，这个可以根据具体需求去做设置。</p><p>enableExternalizedCheckpoints 表示下 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。</p><p>上面讲过，除了故障恢复之外，还需要可以手动去调整并发重新分配这些状态。手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在，那么作业如何恢复数据？</p><p>一方面 Flink 在 Cancel 时允许在外部介质保留 Checkpoint ；另一方面，Flink 还有另外一个机制是 SavePoint。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143266754_72F6543E219A34B25A05F127BF7A64E8" alt="图片说明" title="图片标题"><br>Savepoint 与 Checkpoint 类似，同样是把状态存储到外部介质。当作业失败时，可以从外部恢复。Savepoint 与 Checkpoint 有什么区别呢？</p><p>从触发管理方式来讲，Checkpoint 由 Flink 自动触发并管理，而 Savepoint 由用户手动触发并人肉管理；</p><p>从用途来讲，Checkpoint 在 Task 发生异常时快速恢复，例如网络抖动或超时异常，而 Savepoint 有计划地进行备份，使作业能停止后再恢复，例如修改代码、调整并发；</p><p>最后从特点来讲，Checkpoint 比较轻量级，作业出现问题会自动从故障中恢复，在作业停止后默认清除；而 Savepoint 比较持久，以标准格式存储，允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复。</p><h2 id="2-可选的状态存储方式"><a href="#2-可选的状态存储方式" class="headerlink" title="2. 可选的状态存储方式"></a>2. 可选的状态存储方式</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143282020_64E753626240D5EA470570F497051FEB" alt="图片说明" title="图片标题"><br>Checkpoint 的存储，第一种是内存存储，即 MemoryStateBackend，构造方法是设置最大的StateSize，选择是否做异步快照，这种存储状态本身存储在 TaskManager 节点也就是执行节点内存中的，因为内存有容量限制，所以单个 State maxStateSize 默认 5 M，且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M。Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。推荐使用的场景为：本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143322242_5498DC3E67A0518E3A1BA95DE5F26257" alt="图片说明" title="图片标题"><br>另一种就是在文件系统上的 FsStateBackend ，构建方法是需要传一个文件路径和是否异步快照。State 依然在 TaskManager 内存中，但不会像 MemoryStateBackend 有 5 M 的设置上限，Checkpoint 存储在外部文件系统（本地或 HDFS），打破了总大小 Jobmanager 内存的限制。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。推荐使用的场景、常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启HA的作业。</p><p>还有一种存储为 RocksDBStateBackend ，RocksDB 是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key最大 2G，总大小不超过配置的文件系统容量即可。推荐使用的场景为：超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。</p><h1 id="四-总结"><a href="#四-总结" class="headerlink" title="四. 总结"></a>四. 总结</h1><h2 id="1-为什么要使用状态？"><a href="#1-为什么要使用状态？" class="headerlink" title="1. 为什么要使用状态？"></a>1. 为什么要使用状态？</h2><p>前面提到有状态的作业要有有状态的逻辑，有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。所以需要通过状态来满足业务逻辑。</p><h2 id="2-为什么要管理状态？"><a href="#2-为什么要管理状态？" class="headerlink" title="2.为什么要管理状态？"></a>2.为什么要管理状态？</h2><p>使用了状态，为什么要管理状态？因为实时作业需要7*24不间断的运行，需要应对不可靠的因素而带来的影响。</p><h2 id="3-如何选择状态的类型和存储方式？"><a href="#3-如何选择状态的类型和存储方式？" class="headerlink" title="3.如何选择状态的类型和存储方式？"></a>3.如何选择状态的类型和存储方式？</h2><p>那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p>]]></content>
    
    <summary type="html">
    
      容错机制与故障恢复。
    
    </summary>
    
      <category term="Flink" scheme="https://keysluomo.github.io/categories/Flink/"/>
    
    
      <category term="状态管理与容错机制" scheme="https://keysluomo.github.io/tags/%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 简单的构建一个应用程序</title>
    <link href="https://keysluomo.github.io/2020/06/12/Apache-Flink-%E7%AE%80%E5%8D%95%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"/>
    <id>https://keysluomo.github.io/2020/06/12/Apache-Flink-简单的构建一个应用程序/</id>
    <published>2020-06-12T03:17:46.000Z</published>
    <updated>2020-06-12T04:34:29.086Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简单的构建一个ApacheFlink的应用程序"><a href="#简单的构建一个ApacheFlink的应用程序" class="headerlink" title="简单的构建一个ApacheFlink的应用程序"></a>简单的构建一个ApacheFlink的应用程序</h1><h2 id="开发环境的准备："><a href="#开发环境的准备：" class="headerlink" title="开发环境的准备："></a>开发环境的准备：</h2><p>Flink 可以运行在 Linux, Max OS X, 或者是 Windows 上。这里我是在Windows上运行的。在本地机器上需要有Java8.x和maven环境，另外我们推荐使用 ItelliJ IDEA 作为 Flink 应用程序的开发 IDE。<br>首先在我们的pom.xml文件中添加Flink相关的依赖。</p><p>工作目录：<br><img src="https://uploadfiles.nowcoder.com/images/20191012/9094293_1570843930821_A4D48C34269A213CE6F6CE74BF95681C" alt="图片说明" title="图片标题"> </p><h1 id="编写Flink程序"><a href="#编写Flink程序" class="headerlink" title="编写Flink程序"></a>编写Flink程序</h1><p>创建 SocketWindowWordCount.java 文件：</p><pre><code>package FlinkDemo;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class SocketWindowWordCount {    public static void main(String[] args) throws Exception {        // 创建 execution environment        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // 通过连接 socket 获取输入数据，这里连接到本地9000端口，如果9000端口已被占用，请换一个端口        DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\n&quot;);        // 解析数据，按 word 分组，开窗，聚合        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = text                .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() {                    @Override                    public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {                        for (String word : value.split(&quot;\\s&quot;)) {                            out.collect(Tuple2.of(word, 1));                        }                    }                })                .keyBy(0)                .timeWindow(Time.seconds(5))                .sum(1);        // 将结果打印到控制台，注意这里使用的是单线程打印，而非多线程        windowCounts.print().setParallelism(1);        env.execute(&quot;Socket Window WordCount&quot;);    }}</code></pre><h1 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h1><p>要运行示例程序，首先我们在终端启动 netcat 获得输入流：<br>nc -lk 9000<br>然后直接运行SocketWindowWordCount的 main 方法。</p><p>只需要在 netcat 控制台输入单词，就能在 SocketWindowWordCount 的输出控制台看到每个单词的词频统计。如果想看到大于1的计数，请在5秒内反复键入相同的单词。<br>如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191012/9094293_1570862262668_C587F81A2896284000DDEC3E5471605F" alt="图片说明" title="图片标题"> </p>]]></content>
    
    <summary type="html">
    
      从零开始，构建第一个Apache Flink应用程序
    
    </summary>
    
      <category term="Flink" scheme="https://keysluomo.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://keysluomo.github.io/tags/Flink/"/>
    
  </entry>
  
</feed>
