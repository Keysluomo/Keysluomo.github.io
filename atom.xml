<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>落墨</title>
  
  <subtitle>凡心所向，素履以往</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://keysluomo.github.io/"/>
  <updated>2020-07-01T13:16:22.803Z</updated>
  <id>https://keysluomo.github.io/</id>
  
  <author>
    <name>落墨</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka producer</title>
    <link href="https://keysluomo.github.io/2020/06/30/Kafka-producer/"/>
    <id>https://keysluomo.github.io/2020/06/30/Kafka-producer/</id>
    <published>2020-06-30T01:13:19.000Z</published>
    <updated>2020-07-01T13:16:22.803Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>不管是把Kafka 作为消息队列、消息、总线还是数据存储平台来使用，总是需要有一个可以<br>往Kafka 写入数据的生产者和一个可以从Kafka 读取数据的消费者，或者一个兼具两种角<br>色的应用程序。</p><h1 id="生产者概述"><a href="#生产者概述" class="headerlink" title="生产者概述"></a>生产者概述</h1><p>一个应用程序在很多情况下需要往Kafka 写入消息： 记录用户的活动（用于审计和分析）、<br>记录度量指标、保存日志、消息、记录智能家电的信息、与其他应用程序进行异步通信、缓<br>冲即将写入到数据库的数据，等等。</p><p>在保存网站的点击信息场景里，允许丢失少量的消息或出现少量的消息重复，延迟可以高一些，只要不影响用户体验就行。换句话说，只要用户点击链接后可以马上加载页面，那么我们并不介意消息要在几秒钟之后才能到达Kafka 服务器。吞吐量则取决于网站用户使用网站的频度。<br>不同的使用场景对生产者API 的使用和配置会有直接的影响。尽管生产者API 使用起来很简单， 但消息的发送过程还是有点复杂。如图所示：</p><p><img src="/2020/06/30/Kafka-producer/1.png" alt></p><p>我们从创建一个ProducerRecord 对象开始， Producer Record 对象需要包含目标主题和要发<br>送的内容。我们还可以指定键或分区。在发送ProducerRecord 对象时，生产者要先把键和<br>值对象序列化成字节数组，这样它们才能够在网络上传输。<br>接下来，数据被传给分区器。如果之前在ProducerRcord 对象里指定了分区，那么分区器<br>就不会再做任何事情，直接把指定的分区返回。如果没有指定分区，那么分区器会根据<br>Producer Record 对象的键来选择一个分区。选好分区以后，生产者就知道该往哪个主题和<br>分区发送这条记录了。紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消<br>息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的<br>broker 上。<br>服务器在收到这些消息时会返回一个响应。如果消息成功写入Kafka ，就返回一个<br>RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入<br>失败， 则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还<br>是失败，就返回错误信息。</p><h1 id="Kafka的配置属性"><a href="#Kafka的配置属性" class="headerlink" title="Kafka的配置属性"></a>Kafka的配置属性</h1><p>Kafka生产者有3个<strong>必选属性</strong>：bootstrap.servers,key.serializer,value.serializer。</p><ul><li>bootstrap.servers：该属性指定broker的地址清单，地址的格式为host:port.清单里不需要包含所有的broker地址，生产者会从给定的broker 里查找到其他broker的信息。不过建议至少要提供两个broker 的信息， 一且其中一个宕机，生产者仍然能够连接到集群上。</li><li>key.serializer:broker ：希望接收到的消息的键和值都是字节数组。生产者接口允许使用参数化类型，因此可以把java 对象作为键和值发送给broker 。这样的代码具有良好的可读性，不过生产者需要知道如何把这些java 对象转换成字节数组。key. serializer必须被设置为一<br>个实现了org.apache.kafka.common.seialization.Serialize 接口的类，生产者会使用这个类把键对象序列化成字节数组。Kafka 客户端默认提供了ByteArraySeializer（这个只做很少的事情）、StringSerializer 和IntegerSerializer，因此，如果你只使用常见的几种java 对象类型，那么就没必要实现自己的序列化器。要注意， key.serializer 是必须设置的，就算你打算只发送值内容。</li><li>value.serializer：与key.serializer一样， value.serializer指定的类会将值序列化。如果键和值都是字符串，可以使用与key.serializer 一样的序列化器。如果键是整数类型而值是字符串，那么需要使用不同的序列化器。    </li><li>acks<br>acks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的。<br>这个参数对消息丢失的可能性有重要影响。主参数有如下选项。<br>• 如果<strong>acks=0</strong> ， 生产者在成功写入消息之前不会等待任何来自服务器的响应。也就是说，<br>如果当中出现了问题， 导致服务器没有收到消息，那么生产者就无从得知，消息也就丢<br>失了。不过，因为生产者不需要等待服务器的响应，所以它可以以网络能够支持的最大<br>速度发送消息，从而达到很高的吞吐量。<br>如果<strong>acks=1</strong> ，只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功<br>响应。如果消息无法到达首领节点（比如首领节点崩溃，新的首领还没有被选举出来），<br>生产者会收到一个错误响应，为了避免数据丢失，生产者会重发消息。不过，如果一个<br>没有收到消息的节点成为新首领，消息还是会丢失。这个时候的吞吐量取决于使用的是同步发送还是异步发送。如果让发送客户端等待服务器的响应（通过调用Future对象的get（）方法），显然会增加延迟（在网络上传输一个来回的延迟）。如果客户端使用回<br>调，延迟问题就可以得到缓解，不过吞吐量还是会受发送中消息数量的限制（比如，生<br>产者在收到服务器响应之前可以发送多少个消息）。<br>如果<strong>acks=all</strong> ，只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自<br>服务器的成功响应。这种模式是最安全的，它可以保证不止一个服务器收到消息，就算<br>有服务器发生崩溃，整个集群仍然可以运行。不过，它的延迟比acks=1时更高，因为我们要等待不只一个服务器节点接收消息。</li><li>buffer.me mory<br>该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果<br>应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。这个时候，<br>send （）方法调用要么被阻塞，要么抛出异常，取决于如何设置block.on.buffer. full 参数<br>（在0. 9.0.0 版本里被替换成了max.block.ms，表示在抛出异常之前可以阻塞一段时间）。</li><li><p>compression.type<br>默认情况下，消息发送时不会被压缩。该参数可以设置为snappy 、gzip 或lz 4 ，它指定了<br>消息被发送给broker 之前使用哪一种压缩算也进行压缩。snappy 压缩算法由Google发明，<br>它占用较少的CPU ，却能提供较好的性能和相当可观的压缩比，如果比较关注性能和网<br>络带宽，可以使用这种算法。gzip 压缩算法一般会占用较多的CPU ，但会提供更高的压缩<br>比，所以如果网络带宽比较有限，可以使用这种算法。使用压缩可以降低网络传输开销和<br>存储开销，而这往往是向Kafka 发送消息的瓶颈所在。</p></li><li><p>retries<br>生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领）。在这种情况<br>下， retries 参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会<br>放弃重试并返回错误。默认情况下，生产者会在每次重试之间等待l00 ms ，不过可以通过<br>retry.backoff.ms 参数来改变这个时间间隔。建议在设置重试次数和重试时间间隔之前，<br>先测试一下恢复一个崩溃节点需要多少时间（比如所有分区选举出首领需要多长时间），<br>让总的重试时间比Kafka 集群从崩溃中恢复的时间长，否则生产者会过早地放弃重试。不<br>过有些错误不是临时性错误，没办法通过重试来解决（比如“消息太大”错误）。一般情<br>况下，因为生产者会自动进行重试，所以就没必要在代码逻辑里处理那些可重试的错误。<br>你只需要处理那些不可重试的错误或重试次数超出上限的情况。</p></li><li><p>batch.size<br>当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指<br>定了一个批次可以使用的内存大小，按照字节数计算（而不是消息个数）。当批次被填满，<br>批次里的所有消息会被发送出去。不过生产者井不一定都会等到批次被填满才发送，半满<br>的批次，甚至只包含一个消息的批次也有可能被发送。所以就算把批次大小设置得很大，<br>也不会造成延迟，只是会占用更多的内存而已。但如果设置得太小，因为生产者需要更频<br>繁地发送消息，会增加一些额外的开销。</p></li><li><p>linger.ms<br>该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。KafkaProduce 会在批次填满或linger.ms达到上限时把批次发送出去。默认情况下，只要有可用的线程， 生产者就会把消息发送出去，就算批次里只有一个消息。把linger.ms 设置成比0 大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次。虽然这样会增加延迟，但也会提升吞吐量（因为一次性发送更多的消息，每个消息的开销就变小了） 。</p></li><li>client.id<br>该参数可以是任意的字符串，服务器会用它来识别消息的来源，还可以用在日志和配额指<br>标里。</li><li>max.in.flight.requests.per.connection<br>该参数指定了生产者在收到服务器晌应之前可以发送多少个消息。它的值越高，就会占用<br>越多的内存，不过也会提升吞吐量。把它设为1 可以保证消息是按照发送的顺序写入服务<br>器的，即使发生了重试。</li><li>timeout.ms 、request.timeout.ms 和metadata. fetch. timeout. ms<br>request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间，metada.fetch.timeout.ms 指定了生产者在获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间。如果等待响应超时，那么生产者要么重试发送数据，要么返回一个错误（抛出异常或执行回调）。timeout.ms指定了broker 等待同步副本返回消息确认的时间，与asks 的配置相匹配一一如果在指定时间内没有收到同步副本的确认，那么broker 就会返回一个错误。</li><li>max.block.ms<br>该参数指定了在调用send （） 方法或使用partitionsFor（）方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到max.block.ms时，生产者会抛出超时异常。</li><li>max.request.size<br>该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值，也可以指<br>单个请求里所有消息总的大小。例如，假设这个值为1MB ，那么可以发送的单个最大消<br>息为1MB ，或者生产者可以在单个请求里发送一个批次，该批次包含了1000 个消息，每<br>个消息大小为1 KB。另外， broker 对可接收的消息最大值也有自己的限制（message.max.bytes )，所以两边的配置最好可以匹配，避免生产者发送的消息被broker 拒绝。</li><li>receive.buffer. bytes 和send . buffer.bytes<br>这两个参数分别指定了TCP socket 接收和发送数据包的缓冲区大小。如果它们被设为－ 1 ,<br>就使用操作系统的默认值。如果生产者或消费者与broker 处于不同的数据中心，那么可以<br>适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。</li></ul><p>下面的代码片段演示如何创建一个生产者，这里只指定了<strong>必要的属性</strong>，其他使用默认设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Properties pro = <span class="keyword">new</span> Properties();</span><br><span class="line">pro.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"broker1:9092,broker2:9092"</span>)</span><br><span class="line">pro.put(<span class="string">"key.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">pro.put(<span class="string">"value.serialization.StringSerializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">producer = <span class="keyword">new</span> KafkaProducer&lt;String,String&gt;(pro);</span><br></pre></td></tr></table></figure><p>实例完生产者对象后，接下来就可以开始发送消息了。发送消息有以下三种方式。</p><ul><li>发送并忘记（ fire- and-forget )<br>我们把消息发送给服务器，但井不关心它是否正常到达。大多数情况下，消息会正常到<br>达，因为Kafka 是高可用的，而且生产者会自动尝试重发。不过，使用这种方式有时候<br>也会丢失一些消息。</li><li>同步发送<br>我们使用send （） 方法发送消息， 它会返回一个Future 对象，调用get （） 方法进行等待，就可以知道悄息是否发送成功。</li><li>异步发送<br>我们调用send （） 方怯，并指定一个回调函数， 服务器在返回响应时调用该函数。在下面的几个例子中， 我们会介绍如何使用上述几种方式来发送消息，以及如何处理可能发生的异常情况。 </li></ul><h2 id="发送消息的三种方式实例"><a href="#发送消息的三种方式实例" class="headerlink" title="发送消息的三种方式实例"></a>发送消息的三种方式实例</h2><ul><li>并发并忘记</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.kafka.client;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line">public class KafkaProducerDemo &#123;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author:落墨</span><br><span class="line"> * @CreateTime:2020&#x2F;7&#x2F;1</span><br><span class="line"> * @Description:KafkaProducer</span><br><span class="line"> *&#x2F;</span><br><span class="line">    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line">        Properties kafkaPropertie &#x3D; new Properties();</span><br><span class="line">        &#x2F;&#x2F;配置broker地址，配置多个容错</span><br><span class="line">        kafkaPropertie.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node1:9093,node1:9094&quot;);</span><br><span class="line">        &#x2F;&#x2F;配置key-value允许使用参数化类型</span><br><span class="line">        kafkaPropertie.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        kafkaPropertie.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaProducer kafkaProducer &#x3D; new KafkaProducer(kafkaPropertie);</span><br><span class="line"></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;String, String&gt;(&quot;testTopic&quot;,&quot;key1&quot;,&quot;hello world&quot;);</span><br><span class="line"></span><br><span class="line">        kafkaProducer.send(record);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>同步发送消息</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kafka.client;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@CreateTime</span>:2020/7/1</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:KafkaProducer</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties kafkaPropertie = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//配置broker地址，配置多个容错</span></span><br><span class="line">        kafkaPropertie.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"node1:9092,node1:9093,node1:9094"</span>);</span><br><span class="line">        <span class="comment">//配置key-value允许使用参数化类型</span></span><br><span class="line">        kafkaPropertie.put(<span class="string">"key.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        kafkaPropertie.put(<span class="string">"value.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer kafkaProducer = <span class="keyword">new</span> KafkaProducer(kafkaPropertie);</span><br><span class="line">        <span class="comment">//创建消息对象，第一个为参数topic,第二个参数为key,第三个参数为value</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"testTopic"</span>,<span class="string">"key1"</span>,<span class="string">"hello world"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//同步发送方式,get方法返回结果</span></span><br><span class="line">        RecordMetadata metadata = (RecordMetadata) kafkaProducer.send(record).get();</span><br><span class="line">        System.out.println(<span class="string">"broker返回消息发送信息"</span> + metadata);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>异步发送消息</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">package com.kafka.client;</span><br><span class="line">import org.apache.kafka.clients.producer.Callback;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author:落墨</span><br><span class="line"> * @CreateTime:2020&#x2F;7&#x2F;1</span><br><span class="line"> * @Description:KafkaProducer</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class KafkaProducerDemo &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties kafkaPropertie &#x3D; new Properties();</span><br><span class="line">        &#x2F;&#x2F;配置broker地址，配置多个容错</span><br><span class="line">        kafkaPropertie.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node1:9093,node1:9094&quot;);</span><br><span class="line">        &#x2F;&#x2F;配置key-value允许使用参数化类型</span><br><span class="line">        kafkaPropertie.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        kafkaPropertie.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaProducer kafkaProducer &#x3D; new KafkaProducer(kafkaPropertie);</span><br><span class="line">        &#x2F;&#x2F;创建消息对象，第一个为参数topic,第二个参数为key,第三个参数为value</span><br><span class="line">        final ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;String, String&gt;(&quot;testTopic&quot;,&quot;key1&quot;,&quot;hello world&quot;);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;异步发送消息。异常时打印异常信息或发送结果</span><br><span class="line">        kafkaProducer.send(record, new Callback() &#123;</span><br><span class="line">            public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123;</span><br><span class="line">                if (e !&#x3D; null) &#123;</span><br><span class="line">                    System.out.println(e.getMessage());</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    System.out.println(&quot;接收到返回结果：&quot; + recordMetadata);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        &#x2F;&#x2F;异步发送消息时必须要flush,否则发送不成功，不会执行回调函数</span><br><span class="line">        kafkaProducer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Producer-性能调优"><a href="#Producer-性能调优" class="headerlink" title="Producer 性能调优"></a>Producer 性能调优</h1><h2 id="1-一段Kafka生产端的实例代码"><a href="#1-一段Kafka生产端的实例代码" class="headerlink" title="1.一段Kafka生产端的实例代码"></a>1.一段Kafka生产端的实例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Properties props &#x3D; new Properties();</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 67108864);</span><br><span class="line">props.put(&quot;batch.size&quot;, 131072);</span><br><span class="line">props.put(&quot;linger.ms&quot;, 100);</span><br><span class="line">props.put(&quot;max.request.size&quot;, 10485760);</span><br><span class="line">props.put(&quot;acks&quot;, &quot;1&quot;);</span><br><span class="line">props.put(&quot;retries&quot;, 10);</span><br><span class="line">props.put(&quot;retry.backoff.ms&quot;, 500);</span><br><span class="line">KafkaProducer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;String, String&gt;(props);</span><br></pre></td></tr></table></figure><h2 id="2-内存缓冲的大小"><a href="#2-内存缓冲的大小" class="headerlink" title="2.内存缓冲的大小"></a>2.内存缓冲的大小</h2><p>首先我们看看“buffer.memory”这个参数是什么意思？</p><p>Kafka的客户端发送数据到服务器，一般都是要经过缓冲的，也就是说，你通过KafkaProducer发送出去的消息都是先进入到客户端本地的内存缓冲里，然后把很多消息收集成一个一个的Batch，再发送到Broker上去的。</p><p><img src="/2020/06/30/Kafka-producer/2.png" alt></p><p>所以这个“buffer.memory”的本质就是用来约束KafkaProducer能够使用的内存缓冲的大小的，他的默认值是32MB。</p><p>那么既然了解了这个含义，大家想一下，在生产项目里，这个参数应该怎么来设置呢？</p><p>你可以先想一下，如果这个内存缓冲设置的过小的话，可能会导致一个什么问题？</p><p>首先要明确一点，那就是在内存缓冲里大量的消息会缓冲在里面，形成一个一个的Batch，每个Batch里包含多条消息。</p><p>然后KafkaProducer有一个Sender线程会把多个Batch打包成一个Request发送到Kafka服务器上去。</p><p><img src="/2020/06/30/Kafka-producer/3.png" alt></p><p>那么如果要是内存设置的太小，可能<strong>导致一个问题</strong>：消息快速的写入内存缓冲里面，但是Sender线程来不及把Request发送到Kafka服务器。</p><p>这样是不是会造成内存缓冲很快就被写满？一旦被写满，就会阻塞用户线程，不让继续往Kafka写消息了。</p><p>所以对于“buffer.memory”这个参数应该结合自己的实际情况来进行压测，你需要测算一下在生产环境，你的用户线程会以每秒多少消息的频率来写入内存缓冲。</p><p>比如说每秒300条消息，那么你就需要压测一下，假设内存缓冲就32MB，每秒写300条消息到内存缓冲，是否会经常把内存缓冲写满？经过这样的压测，你可以调试出来一个合理的内存大小。</p><h2 id="3-多少数据打包为一个Batch合适？"><a href="#3-多少数据打包为一个Batch合适？" class="headerlink" title="3.多少数据打包为一个Batch合适？"></a>3.多少数据打包为一个Batch合适？</h2><p>接着你需要思考第二个问题，就是你的“batch.size”应该如何设置？这个东西是决定了你的每个Batch要存放多少数据就可以发送出去了。</p><p>比如说你要是给一个Batch设置成是16KB的大小，那么里面凑够16KB的数据就可以发送了。</p><p>这个参数的默认值是16KB，一般可以尝试把这个参数调节大一些，然后利用自己的生产环境发消息的负载来测试一下。</p><p>比如说发送消息的频率就是每秒300条，那么如果比如“batch.size”调节到了32KB，或者64KB，是否可以提升发送消息的整体吞吐量。</p><p>因为理论上来说，提升batch的大小，可以允许更多的数据缓冲在里面，那么一次Request发送出去的数据量就更多了，这样吞吐量可能会有所提升。</p><p>但是这个东西也不能无限的大，过于大了之后，要是数据老是缓冲在Batch里迟迟不发送出去，那么岂不是你发送消息的延迟就会很高。</p><p>比如说，一条消息进入了Batch，但是要等待5秒钟Batch才凑满了64KB，才能发送出去。那这条消息的延迟就是5秒钟。</p><p>所以需要在这里按照生产环境的发消息的速率，调节不同的Batch大小自己测试一下最终出去的吞吐量以及消息的 延迟，设置一个最合理的参数。</p><h2 id="4-要是一个Batch迟迟无法凑满怎么办？"><a href="#4-要是一个Batch迟迟无法凑满怎么办？" class="headerlink" title="4.要是一个Batch迟迟无法凑满怎么办？"></a>4.要是一个Batch迟迟无法凑满怎么办？</h2><p>要是一个Batch迟迟无法凑满，此时就需要引入另外一个参数了，“linger.ms”</p><p>他的含义就是说一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去了。</p><p>给大家举个例子，比如说batch.size是16kb，但是现在某个低峰时间段，发送消息很慢。</p><p>这就导致可能Batch被创建之后，陆陆续续有消息进来，但是迟迟无法凑够16KB，难道此时就一直等着吗？</p><p>当然不是，假设你现在设置“linger.ms”是50ms，那么只要这个Batch从创建开始到现在已经过了50ms了，哪怕他还没满16KB，也要发送他出去了。</p><p>所以“linger.ms”决定了你的消息一旦写入一个Batch，最多等待这么多时间，他一定会跟着Batch一起发送出去。</p><p>避免一个Batch迟迟凑不满，导致消息一直积压在内存里发送不出去的情况。<strong>这是一个很关键的参数。</strong></p><p>这个参数一般要非常慎重的来设置，要配合batch.size一起来设置。</p><p>举个例子，首先假设你的Batch是32KB，那么你得估算一下，正常情况下，一般多久会凑够一个Batch，比如正常来说可能20ms就会凑够一个Batch。</p><p>那么你的linger.ms就可以设置为25ms，也就是说，正常来说，大部分的Batch在20ms内都会凑满，但是你的linger.ms可以保证，哪怕遇到低峰时期，20ms凑不满一个Batch，还是会在25ms之后强制Batch发送出去。</p><p>如果要是你把linger.ms设置的太小了，比如说默认就是0ms，或者你设置个5ms，那可能导致你的Batch虽然设置了32KB，但是经常是还没凑够32KB的数据，5ms之后就直接强制Batch发送出去，这样也不太好其实，会导致你的Batch形同虚设，一直凑不满数据。</p><h2 id="5-最大请求大小"><a href="#5-最大请求大小" class="headerlink" title="5.最大请求大小"></a>5.最大请求大小</h2><p>“max.request.size”这个参数决定了每次发送给Kafka服务器请求的最大大小，同时也会限制你一条消息的最大大小也不能超过这个参数设置的值，这个其实可以根据你自己的消息的大小来灵活的调整。</p><p>给大家举个例子，你们公司发送的消息都是那种大的报文消息，每条消息都是很多的数据，一条消息可能都要20KB。</p><p>此时你的batch.size是不是就需要调节大一些？比如设置个512KB？然后你的buffer.memory是不是要给的大一些？比如设置个128MB？</p><p>只有这样，才能让你在大消息的场景下，还能使用Batch打包多条消息的机制。但是此时“max.request.size”是不是也得同步增加？</p><p>因为可能你的一个请求是很大的，默认他是1MB，你是不是可以适当调大一些，比如调节到5MB？</p><h2 id="6-重试机制"><a href="#6-重试机制" class="headerlink" title="6.重试机制"></a>6.重试机制</h2><p>“retries”和“retries.backoff.ms”决定了重试机制，也就是如果一个请求失败了可以重试几次，每次重试的间隔是多少毫秒。</p><p>这个大家适当设置几次重试的机会，给一定的重试间隔即可，比如给100ms的重试间隔。</p><h2 id="7-持久化机制"><a href="#7-持久化机制" class="headerlink" title="7.持久化机制"></a>7.持久化机制</h2><p>“acks”参数决定了发送出去的消息要采用什么样的持久化策略，这个涉及到了很多其他的概念，大家可以参考之前专门为“acks”写过的一篇文章：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247485069&amp;idx=1&amp;sn=abd8ddc19fbb49c11c8ad9e1ffdff147&amp;chksm=fba6ee8eccd1679874d5bd19109ed8cb3daadf59dabe67c906f8caed0714ba2dda16f5a032ce&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《简历写了会Kafka，面试官90%会让你讲讲acks参数对消息持久化的影响》</a>。</p><p><strong>参考文章</strong></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247485210&amp;idx=1&amp;sn=abb9930b99c054d5bd56519fa5896b42&amp;chksm=fba6ef19ccd1660f2ac0f4127e6823ab02a4607be56a55c91901a2e4fed831f49145d2d8e7c4&amp;mpshare=1&amp;scene=24&amp;srcid=#rd" target="_blank" rel="noopener">Kafka参数调优实战</a></p>]]></content>
    
    <summary type="html">
    
      无论你是使用Kafka作为队列，消息总线还是数据存储平台，你都会用到生产者，用于发送数据到Kafka。本文将介绍Kafka的producer。
    
    </summary>
    
      <category term="Kafka" scheme="https://keysluomo.github.io/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="https://keysluomo.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Flink DataStream API编程</title>
    <link href="https://keysluomo.github.io/2020/06/21/Flink-DataStream-API%E7%BC%96%E7%A8%8B/"/>
    <id>https://keysluomo.github.io/2020/06/21/Flink-DataStream-API编程/</id>
    <published>2020-06-21T07:06:45.000Z</published>
    <updated>2020-06-24T06:43:05.963Z</updated>
    
    <content type="html"><![CDATA[<h1 id="流处理基本概念"><a href="#流处理基本概念" class="headerlink" title="流处理基本概念"></a>流处理基本概念</h1><p>对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是对立统一的，它们的关系有点类似于对于 Java 中的 ArrayList 中的元素，是直接看作一个有限数据集并用下标去访问，还是用迭代器去访问。</p><p>流处理系统本身有很多自己的特点。一般来说，由于需要支持无限数据集的处理，流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中</p><p><img src="/2020/06/21/Flink-DataStream-API%E7%BC%96%E7%A8%8B/1.png" alt></p><center>图2. 一个 DAG 计算逻辑图与实际的物理时模型。<br>逻辑图中的每个算子在物理图中可能有多个并发。</center><p>对于实际的分布式流处理引擎，它们的实际运行时物理模型要更复杂一些，这是由于每个算子都可能有多个实例。如图 2 所示，作为 Source 的 A 算子有两个实例，中间算子 C 也有两个实例。在逻辑模型中，A 和 B 是 C 的上游节点，而在对应的物理逻辑中，C 的所有实例和 A、B 的所有实例之间可能都存在数据交换。在物理模型中，我们会根据计算逻辑，采用系统自动优化或人为指定的方式将计算工作分布到不同的实例中。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。</p><h1 id="DataStream编程概述"><a href="#DataStream编程概述" class="headerlink" title="DataStream编程概述"></a>DataStream编程概述</h1><p>Flink中的DataStream程序是常规程序，可对数据流实施转换（例如，过滤，更新状态，定义窗口，聚合）。最初从各种来源（例如，消息队列，套接字流，文件）创建数据流。结果通过接收器返回，接收器可以例如将数据写入文件或标准输出（例如命令行终端）。Flink程序可在各种上下文中运行，独立运行或嵌入其他程序中。执行可以在本地JVM或许多计算机的群集中进行。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/16</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:JavaDataStreamSourceApp</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaDataStreamSourceApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//socketFunction(env);</span></span><br><span class="line">        DataStreamSource&lt;String&gt;data = env.socketTextStream(<span class="string">"local"</span>,<span class="number">9999</span>);</span><br><span class="line">        DataStream&lt;word&gt; result= data.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, word&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;word&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span>(String value:s.split(<span class="string">","</span>)) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> word(value,<span class="number">1L</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="string">"name"</span>).timeWindow(Time.seconds(<span class="number">5</span>),Time.seconds(<span class="number">1</span>)).reduce(<span class="keyword">new</span> ReduceFunction&lt;word&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> word <span class="title">reduce</span><span class="params">(word word, word t1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> word(word.name,word.count+t1.count);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        result.print();</span><br><span class="line">        env.execute(<span class="string">"JavaDataStreamSourceApp"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span>  <span class="class"><span class="keyword">class</span> <span class="title">word</span></span>&#123;</span><br><span class="line">            <span class="keyword">private</span> String name;</span><br><span class="line">            <span class="keyword">private</span> Long count;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> name;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">word</span><span class="params">()</span></span>&#123;&#125; <span class="comment">//需要不包含参数的空参构造器</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">word</span><span class="params">(String name, Long count)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.name = name;</span><br><span class="line">            <span class="keyword">this</span>.count = count;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.name = name;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> Long <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> count;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"word&#123;"</span> +</span><br><span class="line">                    <span class="string">"name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                    <span class="string">", count="</span> + count +</span><br><span class="line">                    <span class="string">'&#125;'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCount</span><span class="params">(Long count)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.count = count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line">  * @Author:落墨</span><br><span class="line">  * @CreateTime:2020&#x2F;6&#x2F;16</span><br><span class="line">  * @Description:DataStreamSourceApp</span><br><span class="line">  *&#x2F;</span><br><span class="line">object Wordcount &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val data &#x3D; env.socketTextStream(&quot;localhost&quot;,9999)</span><br><span class="line">    import  org.apache.flink.api.scala._</span><br><span class="line">    data.flatMap(_.split(&quot;,&quot;)).map(x &#x3D;&gt;(x,1)).keyBy(0)</span><br><span class="line">      .timeWindow(Time.seconds(5),Time.seconds(1))</span><br><span class="line">      .sum(1)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(&quot;WordCount&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了实现流式 Word Count，我们首先要先获得一个 StreamExecutionEnvironment 对象。它是我们构建图过程中的上下文对象。基于这个对象，我们可以添加一些算子。对于流处理程度，我们一般需要首先创建一个数据源去接入数据。在这个例子中，我们使用了 Environment 对象中内置的读取文件的数据源。这一步之后，我们拿到的是一个 DataStream 对象，它可以看作一个无限的数据集，可以在该集合上进行一序列的操作。例如，在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。</p><p>最后，我们需要调用 env.execute 方法来开始程序的执行。需要强调的是，前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。</p><p>基于流式 Word Count 的例子可以看出，基于 Flink 的 DataStream API 来编写流处理程序一般需要三步：通过 Source 接入数据、进行一系统列的处理以及将数据写出。最后，不要忘记显式调用 Execute 方式，否则前面编写的逻辑并不会真正执行。</p><h2 id="Flink中使用数据源"><a href="#Flink中使用数据源" class="headerlink" title="Flink中使用数据源"></a>Flink中使用数据源</h2><p>source是程序从中读取其输入的位置。我们可以使用StreamExecutionEnvironment.addSource(sourceFunction)将一个source附加到程序中。Flink提供了许多预先实现的sourceFunction，但是我们可以通过实现SourceFunction 非并行源，实现ParallelSourceFunction接口或扩展RichParallelSourceFunction 并行源来编写自己的自定义源。</p><h3 id="基于文件："><a href="#基于文件：" class="headerlink" title="基于文件："></a>基于文件：</h3><ul><li><p><code>readTextFile(path)</code>- <code>TextInputFormat</code>逐行读取文本文件，即符合规范的文件，并将其作为字符串返回。</p></li><li><p><code>readFile(fileInputFormat, path)</code> -根据指定的文件输入格式读取（一次）文件。</p></li><li><p><code>readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)</code>-这是前两个内部调用的方法。它<code>path</code>根据给定的读取文件<code>fileInputFormat</code>。根据提供的内容<code>watchType</code>，此源可以定期（每<code>interval</code>ms）监视路径中的新数据（<code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>），或者处理一次当前路径中的数据并退出（<code>FileProcessingMode.PROCESS_ONCE</code>）。使用<code>pathFilter</code>，用户可以进一步从文件中排除文件。       </p></li></ul><h3 id="基于套接字："><a href="#基于套接字：" class="headerlink" title="基于套接字："></a>基于套接字：</h3><ul><li><code>socketTextStream</code>-从套接字读取。元素可以由定界符分隔。</li></ul><h3 id="基于集合："><a href="#基于集合：" class="headerlink" title="基于集合："></a>基于集合：</h3><ul><li><p>fromCollection(Collection)-从Java Java.util.Collection创建数据流。集合中的所有元素必须具有相同的类型。</p></li><li><p>fromCollection(Iterator, Class)-从迭代器创建数据流。该类指定迭代器返回的元素的数据类型。</p></li><li><p>fromElements(T …)-从给定的对象序列创建数据流。所有对象必须具有相同的类型。</p></li><li><p>fromParallelCollection(SplittableIterator, Class)-从迭代器并行创建数据流。该类指定迭代器返回的元素的数据类型。</p></li><li><p>generateSequence(from, to) -并行生成给定间隔中的数字序列。</p></li></ul><h3 id="基于连接器："><a href="#基于连接器：" class="headerlink" title="基于连接器："></a>基于连接器：</h3><ul><li>addSource-附加新的源功能。例如，要阅读Apache Kafka，可以使用 addSource(new FlinkKafkaConsumer08&lt;&gt;(…))。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">implementing the SourceFunction <span class="keyword">for</span> non-parallel sources</span><br><span class="line">implementing the ParallelSourceFunction <span class="class"><span class="keyword">interface</span></span></span><br><span class="line"><span class="class"><span class="title">extending</span> <span class="title">the</span> <span class="title">RichParallelSourceFunction</span> <span class="title">for</span> <span class="title">parallel</span> <span class="title">sources</span>.</span></span><br></pre></td></tr></table></figure><h3 id="自定义-SourceFunction"><a href="#自定义-SourceFunction" class="headerlink" title="自定义 SourceFunction"></a>自定义 SourceFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.SourceFunction</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:SourceFunction</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">class CustomNonParallelSourceFunction extends SourceFunction[Long]&#123;</span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">1L</span></span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="keyword">true</span></span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">(sourceContext: SourceFunction.SourceContext[Long])</span>: Unit </span>= &#123;</span><br><span class="line">   <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">     sourceContext.collect(count)</span><br><span class="line">     count += <span class="number">1</span></span><br><span class="line">     Thread.sleep(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">override def <span class="title">cancel</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    isRunning = <span class="keyword">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义ParallelSourceFunction"><a href="#自定义ParallelSourceFunction" class="headerlink" title="自定义ParallelSourceFunction"></a>自定义ParallelSourceFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;ParallelSourceFunction, SourceFunction&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:ParallelSourceFunction</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">class CustomParallelSourceFunction extends  ParallelSourceFunction[Long]&#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="keyword">true</span></span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">1L</span></span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">(sourceContext: SourceFunction.SourceContext[Long])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">      sourceContext.collect(count)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">      Thread.sleep(<span class="number">10000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">override def <span class="title">cancel</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    isRunning = <span class="keyword">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义-RichParallelSourceFunction"><a href="#自定义-RichParallelSourceFunction" class="headerlink" title="自定义 RichParallelSourceFunction"></a>自定义 RichParallelSourceFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;RichParallelSourceFunction, SourceFunction&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:RichParallelSourceFunction</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomRichParallelSourceFunction</span> <span class="keyword">extends</span></span></span><br><span class="line">  RichParallelSourceFunction[Long] &#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="keyword">true</span></span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">1L</span></span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">(sourceContext: SourceFunction.SourceContext[Long])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">      sourceContext.collect(count)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line">      Thread.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">override def <span class="title">cancel</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    isRunning = <span class="keyword">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用自定义数据源"><a href="#使用自定义数据源" class="headerlink" title="使用自定义数据源"></a>使用自定义数据源</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:使用自定义数据源</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object DataStreamSourceApp &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//socketFunction(env)</span></span><br><span class="line">    <span class="comment">//nonParallelSourceFunction(env)</span></span><br><span class="line">    <span class="comment">//ParallelSourceFunction(env)</span></span><br><span class="line">    RichParallelSourceFunction(env)</span><br><span class="line">    env.execute(<span class="string">"DataStreamSourceApp"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">socketFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">      val data =  environment.socketTextStream(<span class="string">"localhost"</span>,<span class="number">9999</span>)</span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">nonParallelSourceFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">   val data = environment.addSource(<span class="keyword">new</span> CustomNonParallelSourceFunction).setParallelism(<span class="number">1</span>)<span class="comment">//因为实现的不是并行的SourceFunction，所以参数不能设置为2</span></span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">ParallelSourceFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">    val data = environment.addSource(<span class="keyword">new</span> CustomParallelSourceFunction).setParallelism(<span class="number">1</span>)<span class="comment">//因为实现的是并行的sourceFunction所以可以设置大于1的并行参数</span></span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">RichParallelSourceFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">    val data = environment.addSource(<span class="keyword">new</span> CustomRichParallelSourceFunction).setParallelism(<span class="number">1</span>)<span class="comment">//因为实现的是并行的sourceFunction所以可以设置大于1的并行参数</span></span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Flink中的DataStream程序是常规程序，可对数据流实施转换（例如，过滤，更新状态，定义窗口，聚合）。
    
    </summary>
    
      <category term="Flink" scheme="https://keysluomo.github.io/categories/Flink/"/>
    
    
      <category term="DataStream API" scheme="https://keysluomo.github.io/tags/DataStream-API/"/>
    
  </entry>
  
  <entry>
    <title>Flink Table API&amp;SQL编程</title>
    <link href="https://keysluomo.github.io/2020/06/21/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B/"/>
    <id>https://keysluomo.github.io/2020/06/21/Flink-Table-API-SQL编程/</id>
    <published>2020-06-21T05:01:35.000Z</published>
    <updated>2020-06-24T06:42:01.135Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Flink关系型API"><a href="#什么是Flink关系型API" class="headerlink" title="什么是Flink关系型API"></a>什么是Flink关系型API</h1><p>Flink提供了三层API。每个API在简介型和表达性之间提供了不同的权衡，并且针对不同的用例</p><p><img src="/2020/06/21/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B/1.png" alt></p><p>Flink具有两个关系API，即Table API和SQL。这两个API都是用于批处理和流处理的统一API，即，对无界的实时流或有界的记录流以相同的语义执行查询，并产生相同的结果。Table API和SQL利用Apache Calcite进行解析，验证和查询优化。它们可以与DataStream和DataSet API无缝集成。无论输入是批处理输入（DataSet）还是流输入（DataStream），在两个接口中指定的查询具有相同的语义并指定相同的结果。</p><p><strong>请注意，Table API和SQL尚未完成功能，正在积极开发中。</strong> <strong>[Table API，SQL]和[stream，batch]输入的每种组合都不支持所有操作。</strong></p><p>Flink的关系API旨在简化数据分析，数据管道和ETL应用程序的定义。</p><p>以下示例显示了SQL查询，以会话化点击流并计算每个会话的点击次数。这与DataStream API示例中的用例相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT userId, COUNT(*)</span><br><span class="line">FROM clicks</span><br><span class="line"><span class="function">GROUP BY <span class="title">SESSION</span><span class="params">(clicktime, INTERVAL <span class="string">'30'</span> MINUTE)</span>, userId</span></span><br></pre></td></tr></table></figure><h1 id="Table-API-amp-SQL特点"><a href="#Table-API-amp-SQL特点" class="headerlink" title="Table API &amp; SQL特点"></a>Table API &amp; SQL特点</h1><ul><li>第一，Table API &amp; SQL 是一种声明式的 API。用户只需关心做什么，不用关心怎么做，比如图中的 WordCount 例子，只需要关心按什么维度聚合，做哪种类型的聚合，不需要关心底层的实现。</li><li><p>第二，高性能。Table API &amp; SQL 底层会有优化器对 query 进行优化。举个例子，假如 WordCount 的例子里写了两个 count 操作，优化器会识别并避免重复的计算，计算的时候只保留一个 count 操作，输出的时候再把相同的值输出两遍即可，以达到更好的性能。</p></li><li><p>第三，流批统一。上图例子可以发现，API 并没有区分流和批，同一套 query 可以流批复用，对业务开发来说，避免开发两套代码。</p></li><li><p>第四，标准稳定。Table API &amp; SQL 遵循 SQL 标准，不易变动。API 比较稳定的好处是不用考虑 API 兼容性问题。</p></li><li><p>第五，易理解。语义明确，所见即所得。</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Table API:</span><br><span class="line">tab.groupBy(<span class="string">"word"</span>)</span><br><span class="line">    .select(<span class="string">"word,count(1) as count"</span>)</span><br><span class="line">SQL:</span><br><span class="line">select word,count(*) AS cnt </span><br><span class="line">from MyTable</span><br><span class="line">group by word</span><br></pre></td></tr></table></figure><h1 id="Table-API-amp-SQL开发"><a href="#Table-API-amp-SQL开发" class="headerlink" title="Table API&amp;SQL开发"></a>Table API&amp;SQL开发</h1><h2 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h2><ul><li>Scala版</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/16</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:TableSQLAPI</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object TableSQLAPI&#123;</span><br><span class="line">    <span class="function">def <span class="title">main</span><span class="params">(args:Array[String])</span>:unit </span>= &#123;</span><br><span class="line">        val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">        val tableEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line">            val filePath = <span class="string">"file:///user/sales.csv"</span></span><br><span class="line">            <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">            <span class="comment">//已经拿到DataSet</span></span><br><span class="line">            val csv = env.readCsvFile[SalesLog](filePath,ignoreFirstLine = <span class="keyword">true</span>)</span><br><span class="line">            <span class="comment">//DataSet ==&gt; Table</span></span><br><span class="line">            val salesTable = tablelEnv.fromDataSet(csv)</span><br><span class="line">            <span class="comment">//Table ==&gt; table</span></span><br><span class="line">            tableEnv.registerTable(<span class="string">"sales"</span>,salesTable)</span><br><span class="line">            <span class="comment">//sql</span></span><br><span class="line">            val resultTable = tableEnv.sqlQuery(<span class="string">"select customerId,sum(amountPaid) money from sales group by customerId"</span>)</span><br><span class="line">            tableEnv.toDataset[Row](resultTable).print()</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">case</span> class <span class="title">SalesLog</span><span class="params">(transactionId:String,customerId:String,itemId:String,</span></span></span><br><span class="line"><span class="function"><span class="params">                               amountPaid:Double)</span>        </span></span><br><span class="line"><span class="function">            </span></span><br><span class="line"><span class="function">    &#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>Java版</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/16</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:JavaTableSQLAPI</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaTableSQLAPI</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEvironment();</span><br><span class="line">        BatchTableEnvironment tableEnv = BatchTableEnvironment.getTableEnvironment(env);</span><br><span class="line">         val filePath = <span class="string">"file:///user/sales.csv"</span></span><br><span class="line">            <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">        DataSet&lt;Sales&gt; csv = env.readCsvFile(filePath).ignoreFirstLine().pojoType(Sales<span class="class">.<span class="keyword">class</span>,<span class="title">transactionId</span>,<span class="title">customerId</span>,<span class="title">itemId</span>,<span class="title">amountPaid</span>)</span>;</span><br><span class="line">        Table sales = tableEnv.fromDataSet(csv);</span><br><span class="line">        tableEnv.registerTable(<span class="string">"sales"</span>,sales);</span><br><span class="line">        Table resultTable = tableEnv.sqlQuery(<span class="string">"select customerId,sum(amountPaid) money from sales group by customerId"</span>);</span><br><span class="line">        DataSet&lt;ROw&gt; result = tableEnv.toDataSet(resultTable,Row<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        result.print();</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Sales</span></span>&#123;</span><br><span class="line">        <span class="keyword">public</span> String transactionId;</span><br><span class="line">        <span class="keyword">public</span> String customerId;</span><br><span class="line">        <span class="keyword">public</span> String itemId;</span><br><span class="line">        <span class="keyword">public</span> Double amountPaid;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Flink具有两个关系API，即Table API和SQL。这两个API都是用于批处理和流处理的统一API，即，对无界的实时流或有界的记录流以相同的语义执行查询，并产生相同的结果。
    
    </summary>
    
      <category term="Flink" scheme="https://keysluomo.github.io/categories/Flink/"/>
    
    
      <category term="Table API &amp; SQL" scheme="https://keysluomo.github.io/tags/Table-API-SQL/"/>
    
  </entry>
  
  <entry>
    <title>Scala</title>
    <link href="https://keysluomo.github.io/2020/04/27/Scala/"/>
    <id>https://keysluomo.github.io/2020/04/27/Scala/</id>
    <published>2020-04-27T03:00:31.000Z</published>
    <updated>2020-06-25T16:32:19.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言<br>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。<br>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。Scala 运行在Java虚拟机上，并兼容现有的Java程序。Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库。</p><h1 id="Scala特性："><a href="#Scala特性：" class="headerlink" title="Scala特性："></a>Scala特性：</h1><p><strong>面向对象特性：</strong><br>Scala是一种纯面向对象的语言，每个值都是对象。每个方法都是调用。举例来说，如果你执行 <code>1+2</code>，则对于 Scala 而言，实际是在调用 Int 类里定义的名为 <code>+</code> 的方法。<br><strong>函数式编程：</strong><br>Scala也是一种函数式语言，奇函数也能成值来使用。Scala提供了轻量级的语法用以定义匿名函数，支持高级函数，并支持柯里化。Scala的case class及内置的模式匹配相当于函数式编程语言中常用的代数类型。</p><p>我们通过函数式编程来实现数组翻倍的例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val nums = Array(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">val doubleNums = nums.map(_*<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>利用Array的map函数，将所有元素2，并生成了一个新的双倍数组。map函数所做的事情是把遍历整个数组的过程，归纳并抽离出来，让我们专注于描述我们想要的是什么”<strong>_*2*</strong>“。我们传入map的是一个纯函数；它不具有任何副作用(不会改变外部状态)，它只是接收一个数字，返回*2后的值。</p><p>正如<strong><em>封装、继承\</em></strong>和<strong><em>多态\</em></strong>是面向对象编程的三大特性。函数式编程也有自己的语言特性<strong>数据不可变、函数是第一公民、引用透明</strong>和<strong>尾递归</strong></p><p><strong>数据不可变（immutable data）</strong>：变量只赋值一次，如果想改变其值就创建一个新的。</p><p><strong>函数是第一公民（first class method）</strong>:函数可以像普通变量一样去使用。函数可以像变量一样被创建，修改，并当成变量一样传递，返回或是在函数中嵌套函数。</p><p><strong>引用透明(referential transparency)</strong>： 指的是函数的运行不依赖于外部变量或“状态”，只依赖于输入的参数，任何时候只要参数相同，调用函数所得到的返回值总是相同的。天然适应并发编程，因为调用函数的结果具有一致性，所以根本不需要加锁，也就不存在死锁的问题。</p><p><strong>尾递归（tail call optimization）</strong>:函数调用要压栈保存现场，递归层次过深的话，压栈过多会产生性能问题。所以引入尾递归优化，每次递归时都会重用栈，提升性能。</p><h1 id="Scala安装"><a href="#Scala安装" class="headerlink" title="Scala安装"></a>Scala安装</h1><h2 id="Windows-上安装Scala"><a href="#Windows-上安装Scala" class="headerlink" title="Windows 上安装Scala"></a>Windows 上安装Scala</h2><p>Scala 语言可以运行在Window、Linux、Unix、 Mac OS X等系统上。</p><p>Scala是基于java之上，大量使用java的类库和变量，<strong>使用 Scala 之前必须先安装 Java（&gt;1.5版本）。</strong></p><p>这里就不介绍Java环境配置，如果还未安装可以参考相关的博客</p><p>接下来，我们从<a href="http://www.scala-lang.org/downloads" target="_blank" rel="noopener">Scala官网</a></p><p><img src="/2020/04/27/Scala/1.png" alt></p><p>下载后，双击 msi 文件，一步步安装即可，安装过程你可以使用默认的安装目录。</p><p>安装好scala后，系统会自动提示，单击 finish，完成安装。</p><p>右击我的电脑，单击”属性”，进入如图所示页面。下面开始配置环境变量，右击【我的电脑】–【属性】–【高级系统设置】–【环境变量】，如图：</p><p><img src="/2020/04/27/Scala/2.png" alt></p><p>设置 SCALA_HOME 变量：单击新建，在变量名栏输入：<strong>SCALA_HOME</strong>: 变量值一栏输入：E:\02_software\scala-2.11.11 也就是 Scala 的安装目录，根据个人情况有所不同</p><p><img src="/2020/04/27/Scala/3.png" alt></p><p>设置 Path 变量：找到系统变量下的”Path”如图，单击编辑。在”变量值”一栏的最前面添加如下的路径： %SCALA_HOME%\bin;%SCALA_HOME%\jre\bin;</p><p><strong>注意：</strong>后面的分号 <strong>；</strong> 不要漏掉。</p><p><img src="/2020/04/27/Scala/4.png" alt></p><p>设置 Classpath 变量：找到找到系统变量下的”Classpath”如图，单击编辑，如没有，则单击”新建”:</p><ul><li>“变量名”：ClassPath</li><li>“变量值”：.;%SCALA_HOME%\bin;%SCALA_HOME%\lib\dt.jar;%SCALA_HOME%\lib\tools.jar.;</li></ul><p><strong>注意：</strong>“变量值”最前面的 .; 不要漏掉。最后单击确定即可。</p><p><img src="/2020/04/27/Scala/5.png" alt></p><p>检查环境变量是否设置好了：调出”cmd”检查。单击 【开始】，在输入框中输入cmd，然后”回车”，输入 scala，然后回车，如环境变量设置ok，你应该能看到这些信息。</p><p><img src="/2020/04/27/Scala/6.png" alt></p>]]></content>
    
    <summary type="html">
    
      Scala即可伸缩的语言（Scalable Language），是一种多范式的编程语言，类似于java的编程，设计初衷是要集成面向对象编程和函数式编程的各种特性。
    
    </summary>
    
      <category term="Scala" scheme="https://keysluomo.github.io/categories/Scala/"/>
    
    
      <category term="编程语言" scheme="https://keysluomo.github.io/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>Flume监控之Ganglia</title>
    <link href="https://keysluomo.github.io/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/"/>
    <id>https://keysluomo.github.io/2020/04/06/Flume监控之Ganglia/</id>
    <published>2020-04-06T10:39:22.000Z</published>
    <updated>2020-06-25T16:32:53.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。今天我们来看看如何用Ganglia来监控我们的flume集群数据。</p><h1 id="Ganglia的安装与部署"><a href="#Ganglia的安装与部署" class="headerlink" title="Ganglia的安装与部署"></a>Ganglia的安装与部署</h1><h2 id="1-安装httpd服务与php"><a href="#1-安装httpd服务与php" class="headerlink" title="1.安装httpd服务与php"></a>1.安装httpd服务与php</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# sudo yum -y install httpd php</span><br></pre></td></tr></table></figure><h2 id="2-安装其他依赖"><a href="#2-安装其他依赖" class="headerlink" title="2.安装其他依赖"></a>2.安装其他依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[root@bigdata107 flume]# sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure><p>##3.安装ganglia<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# rpm -Uvh http:&#x2F;&#x2F;dl.fedoraproject.org&#x2F;pub&#x2F;epel&#x2F;6&#x2F;x86_64&#x2F;epel-release-6-8.noarch.rpm</span><br><span class="line"></span><br><span class="line">[root@bigdata107 flume]# yum -y install ganglia-gmetad</span><br><span class="line">[root@bigdata107 flume]# yum -y install ganglia-web</span><br><span class="line">[root@bigdata107 flume]# yum install -y ganglia-gmond</span><br></pre></td></tr></table></figure></p><h2 id="4-修改配置文件"><a href="#4-修改配置文件" class="headerlink" title="4.修改配置文件"></a>4.修改配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;ganglia.conf</span><br></pre></td></tr></table></figure><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/1.png" alt="图片说明"> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim &#x2F;etc&#x2F;ganglia&#x2F;gmetad.conf</span><br></pre></td></tr></table></figure><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/2.png" alt="图片说明"> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim &#x2F;etc&#x2F;ganglia&#x2F;gmond.conf</span><br></pre></td></tr></table></figure><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/3.png" alt="图片说明"> </p><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/4.png" alt="图片说明"> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]#  vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br></pre></td></tr></table></figure><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/5.png" alt="图片说明"> </p><h2 id="5-selinux本次生效关闭必须重启-如果此时不想重启-可以临时生效"><a href="#5-selinux本次生效关闭必须重启-如果此时不想重启-可以临时生效" class="headerlink" title="5.selinux本次生效关闭必须重启,如果此时不想重启,可以临时生效"></a>5.selinux本次生效关闭必须重启,如果此时不想重启,可以临时生效</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]#  sudo setenforce 0</span><br></pre></td></tr></table></figure><h1 id="启动Ganglia"><a href="#启动Ganglia" class="headerlink" title="启动Ganglia"></a>启动Ganglia</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# service httpd start</span><br><span class="line">正在启动 httpd：</span><br><span class="line">[root@bigdata107 flume]# service gmetad start</span><br><span class="line">Starting GANGLIA gmetad:  [确定]</span><br><span class="line">[root@bigdata107 flume]# service gmond start</span><br><span class="line">Starting GANGLIA gmond:  [确定]</span><br></pre></td></tr></table></figure><h2 id="1-打开web-UI"><a href="#1-打开web-UI" class="headerlink" title="1.打开web UI"></a>1.打开web UI</h2><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/6.png" alt=" "> </p><h2 id="2-通过Ganglia监控Flume"><a href="#2-通过Ganglia监控Flume" class="headerlink" title="2.通过Ganglia监控Flume"></a>2.通过Ganglia监控Flume</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim flume-env.sh</span><br></pre></td></tr></table></figure><p>添加如下内容：<br>export JAVA_OPTS=&quot;-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.216.107:8649 -Xms100m -Xmx200m&quot;</p><h2 id="3-启动flume任务"><a href="#3-启动flume任务" class="headerlink" title="3.启动flume任务"></a>3.启动flume任务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# bin&#x2F;flume-ng agent </span><br><span class="line">--conf conf&#x2F; </span><br><span class="line">--name agent1 </span><br><span class="line">--conf-file job&#x2F;flume_telnet_logger.conf </span><br><span class="line">-Dflume.root.logger&#x3D;&#x3D;INFO,console </span><br><span class="line">-Dflume.monitoring.type&#x3D;ganglia </span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.216.107:8649</span><br></pre></td></tr></table></figure><h2 id="4-发送数据查看Ganglia监测图"><a href="#4-发送数据查看Ganglia监测图" class="headerlink" title="4.发送数据查看Ganglia监测图"></a>4.发送数据查看Ganglia监测图</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 flume]# telnet bigdata107 44444</span><br></pre></td></tr></table></figure><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/7.png" alt="图片说明"></p><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/8.png" alt="图片说明"> </p>]]></content>
    
    <summary type="html">
    
      Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。今天我们来看看如何用Ganglia来监控我们的flume集群数据。
    
    </summary>
    
      <category term="Flume" scheme="https://keysluomo.github.io/categories/Flume/"/>
    
    
      <category term="集群监控" scheme="https://keysluomo.github.io/tags/%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>HDFS NameNode的工作机制</title>
    <link href="https://keysluomo.github.io/2020/04/02/HDFS-NameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/"/>
    <id>https://keysluomo.github.io/2020/04/02/HDFS-NameNode的工作机制/</id>
    <published>2020-04-02T10:47:21.000Z</published>
    <updated>2020-06-25T16:34:16.972Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h2><p><img src="/2020/04/02/HDFS-NameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/1.png" alt=" "> </p><h3 id="1-第一阶段：-namenode-启动"><a href="#1-第一阶段：-namenode-启动" class="headerlink" title="1 第一阶段： namenode 启动"></a>1 第一阶段： namenode 启动</h3><p>1）第一次启动 namenode 格式化后， 创建 fsimage 和 edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>2） 客户端对元数据进行增删改的请求。<br>3） namenode 记录操作日志，更新滚动日志。<br>4） namenode 在内存中对数据进行增删改查。</p><h3 id="2-第二阶段：-Secondary-NameNode-工作"><a href="#2-第二阶段：-Secondary-NameNode-工作" class="headerlink" title="2 第二阶段： Secondary NameNode 工作"></a>2 第二阶段： Secondary NameNode 工作</h3><p>1） Secondary NameNode 询问 namenode 是否需要 checkpoint。 直接带回 namenode 是否检查结果。<br>2） Secondary NameNode 请求执行 checkpoint。<br>3） namenode 滚动正在写的 edits 日志。<br>4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。<br>5） Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。<br>6） 生成新的镜像文件 fsimage.chkpoint。<br>7） 拷贝 fsimage.chkpoint 到 namenode。<br>8） namenode 将 fsimage.chkpoint 重新命名成 fsimage。</p>]]></content>
    
    <summary type="html">
    
      NameNode的工作机制
    
    </summary>
    
      <category term="Hadoop" scheme="https://keysluomo.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://keysluomo.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Java线程死锁</title>
    <link href="https://keysluomo.github.io/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/"/>
    <id>https://keysluomo.github.io/2020/03/28/Java线程死锁/</id>
    <published>2020-03-28T10:28:39.000Z</published>
    <updated>2020-06-25T16:33:23.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java线程死锁"><a href="#Java线程死锁" class="headerlink" title="Java线程死锁"></a>Java线程死锁</h1><blockquote><br>死锁是一种特定的程序状态，在实体之间，由于循环依赖导致彼此一直处于等待之中，没有任何个体可以继续前进。死锁不仅仅是在线程之间会发生，存在资源独占的进程之间同样也 可能出现死锁。通常来说，我们大多是聚焦在多线程场景中的死锁，指两个或多个线程之 间，由于互相持有对方需要的锁，而永久处于阻塞的状态。<br><br></blockquote><p><strong>Java线程死锁</strong>是一个经典的多线程问题，因为不同的线程都在等待那些根本不可能被释放的锁，从而导致所有的工作都无法完成。如图所示</p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁.png" alt="图片说明"> </p><hr><h2 id="如何去定位Java线程死锁呢？"><a href="#如何去定位Java线程死锁呢？" class="headerlink" title="如何去定位Java线程死锁呢？"></a>如何去定位Java线程死锁呢？</h2><p>定位死锁最常见的方式就是利用 jstack 等工具获取线程栈，然后定位互相之间的依赖关系，进而找到死锁。如果是比较明显的死锁，往往 jstack 等就能直接定位，类似 JConsole 甚至可以在图形界面进行有限的死锁检测。<br>既然了解了用什么工具去定位线程死锁，那我们模拟一个Java线程死锁的情况，实战定位线程死锁<br><strong>死锁代码：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * @Author:luomo</span><br><span class="line"> * @CreateTime: 2020&#x2F;3&#x2F;28</span><br><span class="line"> * @Description:模拟DeadLock</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class deadLock implements Runnable&#123;</span><br><span class="line">    public static  Object obj1&#x3D;new Object();</span><br><span class="line">    public static  Object obj2&#x3D;new Object();</span><br><span class="line">    private int flag;</span><br><span class="line">    deadLock(int flag)&#123;</span><br><span class="line">        this.flag&#x3D;flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">        if(flag&#x3D;&#x3D;0)&#123;</span><br><span class="line">            synchronized (obj1)&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;成功获取锁1&quot;);</span><br><span class="line">                try &#123;</span><br><span class="line">                    Thread.currentThread().sleep(1000);</span><br><span class="line">                &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;尝试获取锁2&quot;);</span><br><span class="line">                synchronized (obj2)&#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            synchronized (obj2)&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);</span><br><span class="line">                try &#123;</span><br><span class="line">                    Thread.currentThread().sleep(1000);</span><br><span class="line">                &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;尝试获取锁1&quot;);</span><br><span class="line">                synchronized (obj1)&#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">   deadLock d1&#x3D;new deadLock(0);</span><br><span class="line">   deadLock d2&#x3D;new deadLock(1);</span><br><span class="line">   Thread thread1&#x3D;new Thread(d1);</span><br><span class="line">   Thread thread2&#x3D;new Thread(d2);</span><br><span class="line">   thread1.start();</span><br><span class="line">   thread2.start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>代码运行：</strong></p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁1.png" alt=" "> </p><p>从运行结果我们看到Thread1和Thread0同时都在争用对方已经占有的锁，进而产生死锁。</p><hr><h2 id="如何定位死锁"><a href="#如何定位死锁" class="headerlink" title="如何定位死锁"></a>如何定位死锁</h2><p>如果程序发生了死锁，我们如何去定位死锁？我们可以通过JConsole工具来发现死锁。<br>打开cmd：输入 JConsole 回车<br>我们可以看到一个可视化的工具，找到死锁进程点击连接</p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁2.png" alt="图片说明"> </p><p>我们可以看到有检查死锁的选项</p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁3.png" alt="图片说明"> </p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁4.png" alt="图片说明"> </p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁5.png" alt="图片说明"> </p><p>通过上图我们可以发现产生死锁的线程，从而定位到发生死锁的代码。</p><p>当然我们还可以使用Jstack + pid的方式来定位问题</p><p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁6.png" alt="图片说明"> </p>]]></content>
    
    <summary type="html">
    
      死锁是一种特定的程序状态，在实体之间，由于循环依赖导致彼此一直处于等待之中，没有任何个体可以继续前进。死锁不仅仅是在线程之间会发生，存在资源独占的进程之间同样也 可能出现死锁。通常来说，我们大多是聚焦在多线程场景中的死锁，指两个或多个线程之 间，由于互相持有对方需要的锁，而永久处于阻塞的状态。
    
    </summary>
    
      <category term="Java" scheme="https://keysluomo.github.io/categories/Java/"/>
    
    
      <category term="死锁" scheme="https://keysluomo.github.io/tags/%E6%AD%BB%E9%94%81/"/>
    
  </entry>
  
  <entry>
    <title>Zeppelin 安装</title>
    <link href="https://keysluomo.github.io/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/"/>
    <id>https://keysluomo.github.io/2019/12/21/Zeppelin-安装/</id>
    <published>2019-12-20T16:39:36.000Z</published>
    <updated>2020-06-25T16:33:50.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Apache Zeppelin 是一个可以进行大数据可视化分析的交互式开发系统，可以承担数据接入、数据发现、数据分析、数据可视化、数据协作等任务，其前端提供丰富的可视化图形库，不限于SparkSQL，后端支持HBase、Flink 等大数据系统以插件扩展的方式，并支持Spark、Python、JDBC、Markdown、Shell 等各种常用Interpreter，这使得开发者可以方便地使用SQL 在 Zeppelin 中做数据开发。</p><h3 id="安装环境："><a href="#安装环境：" class="headerlink" title="安装环境："></a>安装环境：</h3><p>Centos6.8、jdk1.8、zeppelin-0.8.1-bin-all</p><h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><p>打开下面链接进行下载:<br><a href="http://zeppelin.apache.org/download.html" target="_blank" rel="noopener">http://zeppelin.apache.org/download.html</a></p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/1.png" alt="图片说明"> </p><p>选择zeppelin-0.8.2-bin-all下载并将其解压缩到您选择的目录中，即可开始使用。</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/2.png" alt="图片说明"> </p><p>执行 bin/zeppelin-daemon.sh start<br>Zeppelin成功启动后，使用Web浏览器转到http：// localhost：8080。如图：</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/3.png" alt="图片说明"> </p><p>安装成功。</p><h3 id="如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决："><a href="#如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决：" class="headerlink" title="如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决："></a>如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决：</h3><p>1.防火墙是否关闭<br>使用以下命令永久关闭防火墙<br>chkconfig iptables off<br>2.查看是否是端口被占用<br>lsof -i:端口号 //查看是否被占用<br>如果被占用可以尝试修改默认端口 zeppelin-site.xml.template</p><p>vi zeppelin-site.xml.template</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/4.png" alt="图片说明"> </p><h2 id="这里展示一下Zeppelin连接数据库的一个demo"><a href="#这里展示一下Zeppelin连接数据库的一个demo" class="headerlink" title="这里展示一下Zeppelin连接数据库的一个demo"></a>这里展示一下Zeppelin连接数据库的一个demo</h2><p>1.首先进入Zeppelin的首页</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/5.png" alt="图片说明"> </p><p>2.点击 anonymous -&gt; Interpreter 输入jdbc回车</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/6.png" alt="图片说明"> </p><p>3.点击 edit 编辑红框选中选项</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/7.png" alt="图片说明"> </p><p>然后添加你的mysql驱动位置</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/8.png" alt="图片说明"> </p><p>4.创建一个新的node，点击 create new note：输入NoteName和Default Interpreter</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/9.png" alt="图片说明"> </p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/10.png" alt="图片说明"> </p><p>5.在框中输入SQL语句，点击执行</p><p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/11.png" alt="图片说明"> </p><p>我们还可以根据自己的需要进行选择图表的展示方式。</p>]]></content>
    
    <summary type="html">
    
      Apache Zeppelin 是一个可以进行大数据可视化分析的交互式开发系统，可以承担数据接入、数据发现、数据分析、数据可视化、数据协作等任务，其前端提供丰富的可视化图形库，不限于SparkSQL，后端支持HBase、Flink 等大数据系统以插件扩展的方式，并支持Spark、Python、JDBC、Markdown、Shell 等各种常用Interpreter，这使得开发者可以方便地使用SQL 在 Zeppelin 中做数据开发。
    
    </summary>
    
      <category term="Zeppelin" scheme="https://keysluomo.github.io/categories/Zeppelin/"/>
    
    
      <category term="Zeppelin 安装" scheme="https://keysluomo.github.io/tags/Zeppelin-%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Kafka实现高吞吐之零拷贝</title>
    <link href="https://keysluomo.github.io/2019/12/09/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/"/>
    <id>https://keysluomo.github.io/2019/12/09/Kafka实现高吞吐之零拷贝/</id>
    <published>2019-12-09T10:04:27.000Z</published>
    <updated>2020-06-25T16:30:03.867Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Kafka是一个非常优秀的消息开源系统，作为分布式的消息队列之所以能够实现高吞吐，其中的一个原因就是sendFile 的零拷贝</p><h2 id="关于零拷贝"><a href="#关于零拷贝" class="headerlink" title="关于零拷贝"></a>关于零拷贝</h2><p>&quot;零拷贝&quot;中的&quot;拷贝&quot;是操作系统在I/O操作中,将数据从一个内存区域复制到另外一个内存区域. 而&quot;零&quot;并不是指0次复制, 更多的是指在用户态和内核态之前的复制是0次.</p><h2 id="CPU-COPY"><a href="#CPU-COPY" class="headerlink" title="CPU COPY"></a>CPU COPY</h2><p>通过计算机的组成原理我们知道, 内存的读写操作是需要CPU的协调数据总线,地址总线和控制总线来完成的<br>因此在&quot;拷贝&quot;发生的时候,往往需要CPU暂停现有的处理逻辑,来协助内存的读写.这种我们称为CPU COPY，CPU COPY不但占用了CPU资源,还占用了总线的带宽.</p><h2 id="DMA-COPY"><a href="#DMA-COPY" class="headerlink" title="DMA COPY"></a>DMA COPY</h2><p>DMA(DIRECT MEMORY ACCESS)是现代计算机的重要功能. 它的一个重要 的特点就是, 当需要与外设进行数据交换时, CPU只需要初始化这个动作便可以继续执行其他指令,剩下的数据传输的动作完全由DMA来完成，可以看到DMA COPY是可以避免大量的CPU中断的</p><h2 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h2><p>本文中的上下文切换时指由用户态切换到内核态, 以及由内核态切换到用户态<br>存在多次拷贝的原因？</p><ul><li><p>操作系统为了保护系统不被应用程序有意或无意地破坏,为操作系统设置了用户态和内核态两种状态.用户态想要获取系统资源(例如访问硬盘), 必须通过系统调用进入到内核态, 由内核态获取到系统资源,再切换回用户态返回应用程序.</p></li><li><p>出于&quot;readahead cache&quot;和异步写入等等性能优化的需要, 操作系统在内核态中也增加了一个&quot;内核缓冲区&quot;(kernel buffer). 读取数据时并不是直接把数据读取到应用程序的buffer, 而先读取到kernel buffer, 再由kernel buffer复制到应用程序的buffer. 因此,数据在被应用程序使用之前,可能需要被多次拷贝  </p></li></ul><p>所有<strong>涉及到数据传输</strong>的场景, 无非就一种:从硬盘上读取文件数据, 发送到网络上去。<br>这个场景我们简化为一个模型:</p><p> File.read(fileDesc, buf, len);<br> Socket.send(socket, buf, len);<br>为了方便描述,上面这两行代码, 我们给它起个名字: read-send模型</p><p>操作系统在实现这个read-send模型时,需要有以下步骤:</p><ol><li>应用程序开始读文件的操作</li><li>应用程序发起系统调用, 从用户态切换到内核态(第一次上下文切换)</li><li>内核态中把数据从硬盘文件读取到内核中间缓冲区(kernel buf)</li><li>数据从内核中间缓冲区(kernel buf)复制到(用户态)应用程序缓冲区(app buf),从内核态切换回到用户态(第二次上下文切换)</li><li>应用程序开始发送数据到网络上</li><li>应用程序发起系统调用,从用户态切换到内核态(第三次上下文切换)</li><li>内核中把数据从应用程序(app buf)的缓冲区复制到socket的缓冲区(socket)</li><li>内核中再把数据从socket的缓冲区(socket buf)发送的网卡的缓冲区(NIC buf)上</li><li>从内核态切换回到用户态(第四次上下文切换)</li></ol><p><img src="/2019/12/09/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/kafka.png" alt="图片说明"> </p><p>由上图可以很清晰地看到, 一次read-send涉及到了四次拷贝:</p><ol><li>硬盘拷贝到内核缓冲区(DMA COPY)</li><li>内核缓冲区拷贝到应用程序缓冲区(CPU COPY)</li><li>应用程序缓冲区拷贝到socket缓冲区(CPU COPY)</li><li>socket buf拷贝到网卡的buf(DMA COPY)  </li></ol><p>其中涉及到2次cpu中断, 还有4次的上下文切换，很明显,第2次和第3次的的copy只是把数据复制到app buffer又原封不动的复制回来, 为此带来了两次的cpu copy和两次上下文切换, 是完全没有必要的，linux的零拷贝技术就是为了优化掉这两次不必要的拷贝</p><h2 id="sendFile"><a href="#sendFile" class="headerlink" title="sendFile"></a>sendFile</h2><p>linux内核2.1开始引入一个叫sendFile系统调用,这个系统调用可以在内核态内把数据从内核缓冲区直接复制到套接字(SOCKET)缓冲区内, 从而可以减少上下文的切换和不必要数据的复制<br>有了sendFile这个系统调用后, 我们read-send模型就可以简化为:</p><ol><li>应用程序开始读文件的操作</li><li>应用程序发起系统调用, 从用户态切换到内核态(第一次上下文切换)</li><li>内核态中把数据从硬盘文件读取到内核中间缓冲区</li><li>通过sendFile,在内核态中把数据从内核缓冲区复制到socket的缓冲区</li><li>内核中再把数据从socket的缓冲区发送的网卡的buf上</li><li>从内核态切换到用户态(第二次上下文切换)  </li></ol><p><img src="/2019/12/09/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/kafka1.png" alt="图片说明"><br>如图所示：<br>涉及到数据拷贝变成:</p><ol><li>硬盘拷贝到内核缓冲区(DMA COPY)</li><li>内核缓冲区拷贝到socket缓冲区(CPU COPY)</li><li>socket缓冲区拷贝到网卡的buf(DMA COPY)</li></ol><p>可以看到,一次read-send模型中, 利用sendFile系统调用后, 可以将4次数据拷贝减少到3次, 4次上下文切换减少到2次, 2次CPU中断减少到1次，<br>相对传统I/O, 这种零拷贝技术通过减少两次上下文切换, 1次cpu copy, <strong>可以将I/O性能提高50%以上(网络数据, 未亲测)</strong>，开始的术语中说到, 所谓的<strong>零拷贝的&quot;零&quot;</strong>, <strong>是指用户态和内核态之间的拷贝次数为0</strong>, 从这个定义上来说, 现在的这个零拷贝技术已经是真正的&quot;零&quot;了</p>]]></content>
    
    <summary type="html">
    
      Kafka是一个非常优秀的消息开源系统，作为分布式的消息队列之所以能够实现高吞吐，其中的一个原因就是sendFile 的零拷贝
    
    </summary>
    
      <category term="Kafka" scheme="https://keysluomo.github.io/categories/Kafka/"/>
    
    
      <category term="零拷贝" scheme="https://keysluomo.github.io/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/"/>
    
  </entry>
  
  <entry>
    <title>Java虚拟机运行时数据区域</title>
    <link href="https://keysluomo.github.io/2019/12/02/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/"/>
    <id>https://keysluomo.github.io/2019/12/02/Java虚拟机运行时数据区域/</id>
    <published>2019-12-02T10:57:52.000Z</published>
    <updated>2020-06-25T16:34:49.734Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>&emsp;&emsp;对于从事C、C++程序开发人员来说，在内存管理领域，他们既是拥有最高权力的“皇帝”又是从事最基础工作的“劳动人民”-既拥有每一个对象的“所有权”，又担负着每一个对象生命开始到终结的维护责任。<br>对于Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不在需要为每一个new操作去写配对的delete/free代码，不容易出现内存泄漏和内存溢出问题，由虚拟机管理内存这一切看起来都很美好。然而一旦出现内存泄漏和溢出的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将异常艰难。Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的<br>用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而<br>建立和销毁。</p><h2 id="运行时数据区域"><a href="#运行时数据区域" class="headerlink" title="运行时数据区域"></a>运行时数据区域</h2><p>&emsp;&emsp;Java 虚拟机在执行java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间。<br><img src="/2019/12/02/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/JVM.png" alt="图片说明"> </p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>&emsp;&emsp;程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环跳转、异常处理、线程恢复等基础功能都需要依赖这个技术器来完成。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>&emsp;&emsp;与程序计数器一样，Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的时Java方法执行的内存模型：每个方法在执行的相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个放法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈出栈的过程。  </p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>&emsp;&emsp;本地方法栈与虚拟机栈所发挥的作用是非常相似的，他们之间的区别不过是虚拟机栈为虚拟机执行方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native法服务。在虚拟机规范中对被地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutofMemoryError异常。</p><h3 id="Java堆"><a href="#Java堆" class="headerlink" title="Java堆"></a>Java堆</h3><p>&emsp;&emsp;对于大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有对象实例以及数组都要在堆上分配,但是随着JIT<br>编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换[2]优化技术将会导致一些微妙的变化发生，所有的<br>对象都分配在堆上也渐渐变得不是那么“绝对”了。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p> &emsp;&emsp;   方法区与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名Non-Heap（非堆），目的应该是与Java堆区分开来</p>]]></content>
    
    <summary type="html">
    
      对于从事C、C++程序开发人员来说，在内存管理领域，他们既是拥有最高权力的“皇帝”又是从事最基础工作的“劳动人民”-既拥有每一个对象的“所有权”，又担负着每一个对象生命开始到终结的维护责任。对于Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不在需要为每一个new操作去写配对的delete/free代码，不容易出现内存泄漏和内存溢出问题，由虚拟机管理内存这一切看起来都很美好。然而一旦出现内存泄漏和溢出的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将异常艰难。
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="运行时数据区域" scheme="https://keysluomo.github.io/tags/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>VMwareWorkstation pro无法在Windows上运行的问题</title>
    <link href="https://keysluomo.github.io/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://keysluomo.github.io/2019/11/23/VMwareWorkstation-pro无法在Windows上运行的问题/</id>
    <published>2019-11-23T03:03:57.000Z</published>
    <updated>2020-06-25T16:30:37.819Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于Windows系统更新问题，导致电脑上的VMwareWorkstation pro无法使用，之前有卸过Windows组件，但是系统自动更新之后又不能打开，所有只能乖乖地将我的VMwareWorkstation更新到15版本。</p><h3 id="首先根据系统的提示将最新的版本下载至我们的电脑中"><a href="#首先根据系统的提示将最新的版本下载至我们的电脑中" class="headerlink" title="首先根据系统的提示将最新的版本下载至我们的电脑中"></a>首先根据系统的提示将最新的版本下载至我们的电脑中</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/1.png" alt="图片说明"> </p><h3 id="由于我的电脑是Windows系统所以选择Windows版本"><a href="#由于我的电脑是Windows系统所以选择Windows版本" class="headerlink" title="由于我的电脑是Windows系统所以选择Windows版本"></a>由于我的电脑是Windows系统所以选择Windows版本</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/2.png" alt="图片说明"> </p><h3 id="当我们打开运行安装时发现"><a href="#当我们打开运行安装时发现" class="headerlink" title="当我们打开运行安装时发现"></a>当我们打开运行安装时发现</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/3.png" alt="图片说明"> </p><h3 id="于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载"><a href="#于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载" class="headerlink" title="于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载"></a>于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/4.png" alt="图片说明"> </p><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/5.png" alt="图片说明"> </p><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/6.png" alt="图片说明"> </p><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/7.png" alt="图片说明"> </p><h2 id="最后升级到我们的VMwareWorkstation-pro版本即可，大功告成-虚拟机又能打开了。（不过界面…）"><a href="#最后升级到我们的VMwareWorkstation-pro版本即可，大功告成-虚拟机又能打开了。（不过界面…）" class="headerlink" title="最后升级到我们的VMwareWorkstation pro版本即可，大功告成! 虚拟机又能打开了。（不过界面…）"></a>最后升级到我们的VMwareWorkstation pro版本即可，大功告成! 虚拟机又能打开了。（不过界面…）</h2><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/8.png" alt="图片说明"> </p>]]></content>
    
    <summary type="html">
    
      VMwareWorkstation无法使用问题
    
    </summary>
    
      <category term="Tools" scheme="https://keysluomo.github.io/categories/Tools/"/>
    
    
      <category term="VMware" scheme="https://keysluomo.github.io/tags/VMware/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper分布式集群部署</title>
    <link href="https://keysluomo.github.io/2019/11/17/Zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://keysluomo.github.io/2019/11/17/Zookeeper分布式集群部署/</id>
    <published>2019-11-17T10:32:08.000Z</published>
    <updated>2020-06-25T16:31:01.136Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote><p>ZooKeeper是一个开放源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。  </p><p>ZooKeeper是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。ZooKeeper可以保证如下分布式一致性特性。<br>今天我们来看看如何分布式安装</p></blockquote><h1 id="Zookeeper分布式集群部署"><a href="#Zookeeper分布式集群部署" class="headerlink" title="Zookeeper分布式集群部署"></a>Zookeeper分布式集群部署</h1><h2 id="下载Zookeeper"><a href="#下载Zookeeper" class="headerlink" title="下载Zookeeper"></a>下载Zookeeper</h2><p>这里可以参考官网，下载需要的版本：<br><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">https://zookeeper.apache.org/</a>  </p><h2 id="解压Zookeeper到我们指定的目录"><a href="#解压Zookeeper到我们指定的目录" class="headerlink" title="解压Zookeeper到我们指定的目录"></a>解压Zookeeper到我们指定的目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 software]#  tar -zxvf zookeeper-3.4.10 -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p><strong>1.修改zoo_sample.cfg zoo.cfg 为zoo.cfg</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 software]#  mv zoo_sample.cfg zoo.cfg zoo.cfg</span><br></pre></td></tr></table></figure><p><strong>2.修改配置文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 software]# vi zoo.cfg</span><br></pre></td></tr></table></figure><blockquote><p>#数据目录需要提前创建  </p><p>dataDir=/opt/module/zookeeper-3.4.5-cdh5.10.0/zkData  </p><p>#server.每个节点服务编号=服务器ip地址：集群通信端口：选举端口<br>server.1=bigdata107 :2888:3888<br>server.2=bigdata108 :2888:3888<br>server.3=bigdata109 :2888:3888</p></blockquote><p><strong>3.分发到各个节点</strong>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 software]#   scp -r zookeeper-3.4.10&#x2F; bigdata108:&#x2F;opt&#x2F;module&#x2F;</span><br><span class="line">[root@bigdata107 software]#  scp -r zookeeper-3.4.10&#x2F; bigdata109:&#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p><strong>4.创建相关目录</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata107 zookeeper-3.4.10]# mkdir zkData</span><br></pre></td></tr></table></figure></p><p><strong>5.在各个节点zkData目录下，创建myid</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#bigdata107 节点</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# touch myid</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# vi myid</span><br><span class="line">1</span><br><span class="line">#bigdata108 节点</span><br><span class="line">[root@bigdata108 zookeeper-3.4.10]# touch myid</span><br><span class="line">[root@bigdata108 zookeeper-3.4.10]# vi myid</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">#bigdata109 节点</span><br><span class="line">[root@bigdata109 zookeeper-3.4.10]# touch myid</span><br><span class="line">[root@bigdata109 zookeeper-3.4.10]# vi myid</span><br><span class="line">3</span><br></pre></td></tr></table></figure></p><p><strong>6.启动Zookeeper服务</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#启动Zookeeper服务</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh start</span><br><span class="line">#查看各个节点服务状态 </span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh status</span><br><span class="line">#关闭各个节点服务</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh stop</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      ZooKeeper是一个开放源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。
    
    </summary>
    
      <category term="Zookeeper" scheme="https://keysluomo.github.io/categories/Zookeeper/"/>
    
    
      <category term="集群部署" scheme="https://keysluomo.github.io/tags/%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>VisualVM:多合一故障处理工具</title>
    <link href="https://keysluomo.github.io/2019/11/10/VisualVM-%E5%A4%9A%E5%90%88%E4%B8%80%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7/"/>
    <id>https://keysluomo.github.io/2019/11/10/VisualVM-多合一故障处理工具/</id>
    <published>2019-11-09T16:47:40.000Z</published>
    <updated>2020-06-25T16:31:28.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>VisualVM (All-in-One Java Troubleshooting Tool)是到目前为止随JDK发布的功能最强大的运行监视和故障处理程序，并且可以预见在未来一段时间内都是官方助力发展的虚拟机故障处理工具。官方在VisualVM的软件说明中写上了“All in One”的描述字样，预示着它除了运行监视，故障处理 外，还提供了很多其他方面的功能。如性能分析，VisualVM的性能分析功能甚至比起JProfiler、YourKit等专业收费的Profiling工具都不会逊色多少，而且VisualVM的还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，因此它对应用程序的实际性能的影响很小，使用它可以直接应用在生产环境中。这个是JProfiler、YourKit等工具无法与之媲美的。</p><h2 id="Visual兼容范围与插件安装"><a href="#Visual兼容范围与插件安装" class="headerlink" title="Visual兼容范围与插件安装"></a>Visual兼容范围与插件安装</h2><p>VisualVM基于NetBeans平台开发，因此它一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM可以做到：</p><ul><li>显示虚拟机进程以及进程的配置、环境信息。</li><li>监视应用程序的CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。</li><li>dump以及分析堆转储快照。</li><li>方法级的程序运行性能分析，找出被调用最多、运行时间最长的方法。</li><li>离线程序快照：收集程序的运行时配置、线程dump、内存dump等信息建立一个快照，可以将快照发送开发者处进行Bug反馈。</li><li>其他plugins的无限的可能性。。<br>主要特性的兼容性见表：</li></ul><p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573376421667_0603DDC2187E3F359DFF1C4A07BF61DF" alt="图片说明" title="图片标题"> </p><p>首先我们在jdk1.8/bin目录下启动jvisualvm,我们先不着急找应用程序进行监测，因为现在VisualVM还没有加载任何插件，虽然基本的监视、线程面板的功能主程序都以默认插件的形式提供了但是不给VisualVM装任何扩展插件，就相当于放弃了它最精华的功能，和没有安装任何应用软件操作系统差不多。<br>插件我们进行手工安装：<br>打开“工具 -&gt; 插件 -&gt; 可用插件 “选中“Visual GC”默认进行安装<br>安装成功后：</p><p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573377093671_1269F967A1AE25FDC458A14CB4CB4B37" alt="图片说明" title="图片标题"> </p><p>大家可以根据自己的工作需要和兴趣选择合适的插件，然后点击安装即可。<br>安装完插件，选择一个需要监视的程序就进入程序的主界面了，如图所示。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573377517433_B76A4E6CB4F70B4F3FD192B97C655C83" alt="图片说明" title="图片标题"> </p><p>VisualVM中“概述”、“监视”、“线程”、“MBeans”的功能与前面介绍的JConsole差别不大，下面介绍几个特色的功能、插件。</p><h2 id="2-生成、浏览堆转储快照"><a href="#2-生成、浏览堆转储快照" class="headerlink" title="2.生成、浏览堆转储快照"></a>2.生成、浏览堆转储快照</h2><p>在VisualVM中生成dump文件有两种方式，可以执行下列任一操作：</p><ul><li>在“应用程序”窗口中右键”单击应用程序节点，然后选择“堆Dump”。</li><li>在“应用程序”窗口中双击应用程序节点以打开应用程序标签，然后在“监视”标签中单击“堆Dump”。<br>生成dump文件之后，应用程序页签将在该堆的应用程序下增加一个以[heapdump]开头的子节点，并且在主页签中打开了该转储快照，如图。如果需要把dump文件保存或发送出去，要在heapdump节点上右键选择“另存为”菜单，否则当VisualVM关闭时，生成的dump文件会被当做临时文件删除掉。要打开一个已经存在的dump文件，通过菜单“装入”功能，选择硬盘上的dump文件即可。</li></ul><p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573378591042_FAB281188970DB420E1323E98D38BBAA" alt="图片说明" title="图片标题"> </p><h2 id="分析程序性能"><a href="#分析程序性能" class="headerlink" title="分析程序性能"></a>分析程序性能</h2><p>在Profiler页签中，VisualVM提供了程序运行期间方法级的CPU执行时间分析以及内存分析，做Profiling分析肯定会对程序运行性能有较大的影响，所以一般不在生产环境中使用这项功能。<br>要开始分析，先选择“CPU”和“内存”按钮中的一个，然后切换到应用程序中对程序进行操作，VisualVM会记录到这段时间中应用程序执行过的方法。如果是CPU分析，将会统计每个方法的执行次数、执行耗时；如果是内存分析，则会统计每个方法关联的对象数以及这些对象所占的空间。分析结束后，点击“停止”按钮结束监控过程，如图：</p><p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573379709800_6C3B2CC29B95904A4B0C47224C44304A" alt="图片说明" title="图片标题"> </p><h2 id="4-BTrace动态日志跟踪"><a href="#4-BTrace动态日志跟踪" class="headerlink" title="4.BTrace动态日志跟踪"></a>4.BTrace动态日志跟踪</h2><p>BTrace是一个很“有趣”的VisualVM插件，本身也是可以独立运行的程序，它的作用是在不停止目标程序运行的前提下，通过HotSpot虚拟机的HotSwap技术动态缴入原本并不存在的调试代码。这项功能对实际生产中的程序很有意义：经常遇到程序出现问题，但排查错误的一些必要信息，譬如方法参数、返回值等，在开发时并没有打印到日止之中，以至于不得不停掉服务，通过调试增量来加入日志代码以解决问题。当遇到生产环境服务无法随便停止是，缺一两句日志导致排错进行不下去是一件非常郁闷的事情。<br>在VisualVM中安装了BTrace插件后，在应用程序面板中右键点击要调试的程序，会出现“Trace Application。。。”菜单，点击将进入BTrace面板。这个面板里面看起来就像一个简单的Java程序开发环境，里面还有一段Java代码。如图：</p><p><img src="https://uploadfiles.nowcoder.com/images/20191111/9094293_1573475795296_506A122025C95B87465048F8325621A5" alt="图片说明" title="图片标题"> </p><p>这里准备了一个Demo来演示BTrace的功能：产生 两个1000以内的随机整数，输出两个数字相加的结果代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import java.io.BufferedReader;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.io.InputStreamReader;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author: luomo</span><br><span class="line"> * @CreateTime: 2019&#x2F;11&#x2F;11</span><br><span class="line"> * @Description: 对BTrace插件简单的使用</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class BTraceTest &#123;</span><br><span class="line">    public int add(int a,int b)&#123;</span><br><span class="line">        return a+b;</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args)throws IOException &#123;</span><br><span class="line">        BTraceTest test&#x3D;new BTraceTest();</span><br><span class="line">        BufferedReader reader&#x3D;new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        for(int i&#x3D;0;i&lt;10;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            reader.readLine();</span><br><span class="line">            int a&#x3D;(int) Math.round(Math.random()*1000);</span><br><span class="line">            int b&#x3D;(int) Math.round(Math.random()*1000);</span><br><span class="line">            System.out.println(test.add(a,b));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序运行后，在VisualVM中打开该程序的监视，在BTrace页签填充TracingScript内容，输入的调试代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;* BTrace Script Template *&#x2F;</span><br><span class="line">import com.sun.btrace.annotations.*;</span><br><span class="line">import static com.sun.btrace.BTraceUtils.*;</span><br><span class="line"></span><br><span class="line">@BTrace</span><br><span class="line">public class TracingScript &#123;</span><br><span class="line">&#x2F;* put your code here *&#x2F;</span><br><span class="line">    @OnMethod(</span><br><span class="line">        clazz&#x3D;&quot;BTraceTest&quot;,</span><br><span class="line">        method&#x3D;&quot;add&quot;,</span><br><span class="line">        location&#x3D;@Location(Kind.RETURN)</span><br><span class="line">)</span><br><span class="line">public static void func(@Self BTraceTest instance,int a,int b,@Return int result)&#123;</span><br><span class="line">println(&quot;调用堆栈：&quot;);</span><br><span class="line">jstack();</span><br><span class="line">println(strcat(&quot;方法参数A：&quot;,str(a)));</span><br><span class="line">println(strcat(&quot;方法参数B:&quot;,str(b)));</span><br><span class="line">println(strcat(&quot;方法结果：&quot;,str(result)));</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击“Start”按钮后稍等片刻，编译完成后，可见Output面板上出现“BTrace code sucessfuly deployed”的字样。程序运行的时候在Output面板将会输出如图的调试信息。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191111/9094293_1573477699767_FA5FB94755C7F76228C24ADC9E5CF1A9" alt="图片说明" title="图片标题"> </p><p>BTrace的用法还有许多，打印调用堆栈、参数、返回值只是最基本的应用，在他的网站上有使用BTrace进行性能监视、定位连接泄漏和内存泄漏、解决多线程竞争问题等例子。</p>]]></content>
    
    <summary type="html">
    
      VisualVM (All-in-One Java Troubleshooting Tool)是到目前为止随JDK发布的功能最强大的运行监视和故障处理程序，并且可以预见在未来一段时间内都是官方助力发展的虚拟机故障处理工具。官方在VisualVM的软件说明中写上了“All in One”的描述字样，预示着它除了运行监视，故障处理 外，还提供了很多其他方面的功能。如性能分析，VisualVM的性能分析功能甚至比起JProfiler、YourKit等专业收费的Profiling工具都不会逊色多少，而且VisualVM的还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，因此它对应用程序的实际性能的影响很小，使用它可以直接应用在生产环境中。这个是JProfiler、YourKit等工具无法与之媲美的。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>JConsole:Java监视与管理控制台</title>
    <link href="https://keysluomo.github.io/2019/11/09/JConsole-Java%E7%9B%91%E8%A7%86%E4%B8%8E%E7%AE%A1%E7%90%86%E6%8E%A7%E5%88%B6%E5%8F%B0/"/>
    <id>https://keysluomo.github.io/2019/11/09/JConsole-Java监视与管理控制台/</id>
    <published>2019-11-08T16:45:19.000Z</published>
    <updated>2020-06-25T16:31:46.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>JConsole（Java Monitoring and Management Console）是一种基于JMX的可视化监视、管理工具。<br><strong>1.启动JConsole</strong><br>通过JDK/bin目录下的“jconsole.exe”启动JConsole后，将自动搜索出本机运行的所有虚拟机进程，不需要用户自己再使用jps来查询了，如图双击选择其中一个进程即可开始监控了，也可以使用下面的“远程进程”功能来连接远程服务器，对远程虚拟机进行监控。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573274563736_6654F5DFA4EC7329FE86D88517B8FA93" alt="图片说明" title="图片标题"> </p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573275239073_438AC748ECDB4217D868DFDE95ECFD09" alt=" " title="图片标题"> </p><p>从图中可以看出机器运行了几个本地虚拟机进程，其中OOMTest是我准备的“反面教材”代码双击它进入JConsole主界面，包括“概述”、“内存”、“线程”、“类”、“VM概要”、“MBean”六个页签。如图所示</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573278160273_562106A0F5A1D726229907F196EB6D40" alt="图片说明" title="图片标题"> </p><p>“概述”页签显示的是整个虚拟机主要运行数据的概览，其中包括“堆内存使用情况”、“类”、“CPU使用情况”四种信息的曲线图，这些曲线图是后面“内存”、“线程”、“类”页签的信息汇总。</p><p><strong>2.内存监控</strong><br>“内存”页签相当于可视化的jsat命令，用于监视收集器管理的虚拟机内存（Java堆和永久代）的变化趋势。我们通过运行代码“OOMTest”来体验一下监视功能，运行时设置的虚拟机参数为：-Xms100m -Xmx100m -XX:+UseSerialGC,这段代码的作用是以64KB/50毫秒的速度往Java堆中填充数据，一共填充1000次，使用JConsole的“内存”页签进行监视，观察曲线和柱状图指示图的变化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author:luomo</span><br><span class="line"> * @CreateTime:2019&#x2F;11&#x2F;9</span><br><span class="line"> * @Description:OOMTest</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class OOMTest &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 内存占位符对象，一个OOMObject大约占64KB</span><br><span class="line">     *&#x2F;</span><br><span class="line">    static class OOMobject&#123;</span><br><span class="line">        public byte[] placeholder&#x3D;new byte[64*1024];</span><br><span class="line">    &#125;</span><br><span class="line">    public static void fillHeap(int num) throws InterruptedException&#123;</span><br><span class="line">        List&lt;OOMobject&gt; list &#x3D;new ArrayList&lt;OOMobject&gt;();</span><br><span class="line">        for(int i&#x3D;0;i&lt;num;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            &#x2F;&#x2F;稍作延时，令监视曲线的变化更加明显</span><br><span class="line">            Thread.sleep(50);</span><br><span class="line">            list.add(new OOMobject());</span><br><span class="line">        &#125;</span><br><span class="line">        System.gc();</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args)throws Exception&#123;</span><br><span class="line">        fillHeap(1000);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序运行后，在“内存”页签中可以看到内存池Eden区的运行趋势呈现折线状，如图。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282506435_42550CBEF3FCED46D7B42799FAD4566A" alt="图片说明" title="图片标题"> </p><p>监视范围扩大至整个堆后，会发现曲线是一条向上增长的平滑曲线。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282418309_B397F1590C7ED6E011AB02405287D0D0" alt="图片说明" title="图片标题"> </p><p>并且从柱状图可以看出，在1000次循环执行结束，运行了System.gc()后，虽然整个新生代Eden和Survivor区都基本被清空了，但是代表老年代的柱状图仍然保持峰值状态，说明被填充进堆中的数据在System.gc（）方法执行之后仍然存活。</p><p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282569188_69314B5631685BFF2D1BB4774E598582" alt="图片说明" title="图片标题"> </p><p><strong>这里有两个小问题供读者思考：</strong><br>1.虚拟机启动参数只限制了Java堆为100MB，没有指定-Xmn参数，能否从监控图中估计出新生代有多大？<br>2.为何执行了System.gc（）之后，图中代表老年代的柱状图仍然显示峰值状态，代码需要如何调整才能让System.gc（）回收掉填充到堆中的对象？<br><strong>问题一：</strong>我们可以从图中知道Eden空间大小，因为没有设置-XX：SurvivorRadio参数，所以Eden与Survivor空间比例默认值为8:1，整个新生代空间这样我们可以用Eden空间大小/占新生代空间的比例，得出新生代空间的大小。<br><strong>问题二：</strong>执行完System.gc（）之后，空间未能回收是因为List<OOMObject>list对象仍然存活，fillHeap（）方法仍然没有退出，因此list对象在System.gc（）执行时仍然处于作用域之内。如果把System.gc移动到fillHeap()方法外调用就可以回收掉全部内存。 </OOMObject></p>]]></content>
    
    <summary type="html">
    
      JConsole（Java Monitoring and Management Console）是一种基于JMX的可视化监视、管理工具。
    
    </summary>
    
    
      <category term="JVM" scheme="https://keysluomo.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>垃圾收集器</title>
    <link href="https://keysluomo.github.io/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>https://keysluomo.github.io/2019/11/08/垃圾收集器/</id>
    <published>2019-11-08T10:51:43.000Z</published>
    <updated>2020-06-25T16:27:37.599Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如果说收集算法是内存回收的方法论，那么垃圾收集就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。这里讨论的收集器基于JDK1.7Update14之后的HotSpot虚拟机。</p><h2 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h2><p>Serial收集器是最基本、发展历史最悠久的收集器，曾经是虚拟机新生代收集的唯一选择。我们从名字就可知道，这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。<br>如图示意了Serial、Serial Old收集器的运行过程。<br><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/1.png" alt="图片说明"> </p><h2 id="ParNew"><a href="#ParNew" class="headerlink" title="ParNew"></a>ParNew</h2><p>ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数（例如：-XX：SurvivorRatio、——XX：PretenureSizeThreshold、-XX：HandlePromotionFailure等）、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。<br>ParNew/Serial Old示意图：</p><p><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/2.png" alt="ParNew/Serial Old示意图"> </p><h2 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h2><p>Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去和ParNew都一样，那他有什么特别之处呢?<br>Parallen Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能的缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量。所谓吞吐量<br>就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间），虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。<br>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度就能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要是和在后台运算而不需要太多交互的任务。</p><h2 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h2><p>Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备预案，在并发手机发生Concurrent Mode Failure时使用。<br>如图是Serial、Serial Old收集器的运行过程。</p><p><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/3.png" alt="图片说明"></p><h2 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h2><p>Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器是在JDk1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择。由于老年代Serial Old收集器在服务端应用性能上的拖累，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew和CMS的组合给力。<br>Parallel Old收集器的工作过程如图：</p><p><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/4.png" alt="图片说明"> </p><h2 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h2><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常复合这类应用的需求。<br>从名字可以看出，CMS收集器是基于“标记清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤，<br>包括：</p><ul><li>初始标记</li><li>并发标记</li><li>重新标记</li><li>并发清除<br>其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能<br>直接关联到的对象，速度很快，并发标记阶段就是进行GC RootsTracing的过程，而重新标记阶段则是为了修正并<br>发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初<br>始标记阶段稍长一些，但远比并发标记的时间短。<br><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/5.png" alt="图片说明"> <h2 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h2>G1（Garbage-First）收集器是当今收集器技术发展的最前沿成果之一。G1是一款面向服务端应用的垃圾收集器。HotSpot开发团队赋予它的使命是（在比较长期的）未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。</li><li>并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。</li><li>分代收集：与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更<br>好的收集效果。</li><li>空间整合：与CMS的“标记—清理”算法不同，G1从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但无论如何，这两种算法都意味着G1运作期间不会产生<br>内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</li><li>可预测的停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。<br>在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤：</li><li>初始标记（Initial Marking）</li><li>并发标记（Concurrent Marking）</li><li>最终标记（Final Marking）</li><li>筛选回收（Live Data Counting and Evacuation）<br><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/6.png" alt="图片说明"> </li></ul>]]></content>
    
    <summary type="html">
    
      如果说收集算法是内存回收的方法论，那么垃圾收集就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。这里讨论的收集器基于JDK1.7Update14之后的HotSpot虚拟机。
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="垃圾收集器" scheme="https://keysluomo.github.io/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>HotSpot虚拟机对象探秘</title>
    <link href="https://keysluomo.github.io/2019/11/07/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/"/>
    <id>https://keysluomo.github.io/2019/11/07/HotSpot虚拟机对象探秘/</id>
    <published>2019-11-06T16:42:22.000Z</published>
    <updated>2020-06-25T16:35:24.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>我们以常用的虚拟机HotSpot和常用的内存区域Java堆为例，深入了解HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。</p><h2 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h2><p>虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有先执行相应的类加载过程。<br>    在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同把一块确定大小的内存从Java堆中划分出来。假设堆中内存是绝对完整的，所有的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，所有用过的内存都放在一边，中间放着一个值指针作为分界点的指示器，这种分配方式成为“指针碰撞”。如图：<br><img src="/2019/11/07/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/1.png" alt="图片说明"> </p><p>如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个实例，并更新列表上的记录，这种分配方式称为“空闲列表”。如图：<br><img src="/2019/11/07/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/2.png" alt="图片说明"> </p><p>选择哪种分配方式是由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩功能决定。因此，在使用serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。</p><h2 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h2><p>在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（InstanceData）和对齐填充（Padding）。<br>HotSpot虚拟机的对象头包括两部分信息：</p><p><strong>第一部分</strong>：用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark Word”。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身<br><strong>第二部分</strong>:实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。<br><strong>第三部分</strong>：对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或者2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。</p><h2 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h2><p>建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。目前主流的访问方式有使用<strong>句柄</strong>和<strong>直接指针</strong>两种。</p><p>如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息，</p><p>如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference中存储的直接就是对象地址，</p>]]></content>
    
    <summary type="html">
    
      我们以常用的虚拟机HotSpot和常用的内存区域Java堆为例，深入了解HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。
    
    </summary>
    
    
      <category term="JVM" scheme="https://keysluomo.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>垃圾收集算法</title>
    <link href="https://keysluomo.github.io/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/"/>
    <id>https://keysluomo.github.io/2019/11/06/垃圾收集算法/</id>
    <published>2019-11-06T12:00:00.000Z</published>
    <updated>2020-06-25T16:28:20.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="标记清除算法"><a href="#标记清除算法" class="headerlink" title="标记清除算法"></a>标记清除算法</h1><p><img src="/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/1.png" alt="图片说明"><br>最基础的收集算法是“标记清除”算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经介绍过了。之所以说他是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。<br>主要不足：</p><ul><li>效率问题，标记和清除两个过程的效率都不高；</li><li>空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能导致以后再程序过程中需要分配较大的对象时，无法找到连续内存而不得不提前触发另一次垃圾收集动作</li></ul><h1 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h1><p><img src="/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/2.png" alt="图片说明"><br>为了解决效率问题，一种称为复制的收集算法出现了，他将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完了，就将还存活着的对象复制到另一块上面，然后再把已经使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂的情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。<br>主要不足：</p><ul><li>将内存缩小为了原来的一半，在对象存活率较高时就要进行较多的复制操作，效率将会变低。</li></ul><h1 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h1><p><img src="/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/3.png" alt="图片说明"><br>根据老年代的特点，有人提出了另外一种“标记-整理”算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。<br>主要不足：</p><ul><li>它对内存变动更频繁，需要整理所有存活对象的引用地址，在效率上比复制算法要差很多。</li></ul><h1 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h1><p>当前商业虚拟机的垃圾收集器都采用“分代收集”算法，这种算法并没有什么新的思想，知识根据对象存活周期的不同将内存划分为几块。分代收集算法分代收集算法（Generational Collection）严格来说并不是一种思想或理论，而是融合上述3种基础的算法思想，而产生的针对不同情况所采用不同算法的一套组合拳。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。</p>]]></content>
    
    <summary type="html">
    
      由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本文只是介绍几种算法的思想
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="垃圾收集算法" scheme="https://keysluomo.github.io/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>如何判断对象是否死亡?</title>
    <link href="https://keysluomo.github.io/2019/11/04/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/"/>
    <id>https://keysluomo.github.io/2019/11/04/如何判断对象是否死亡/</id>
    <published>2019-11-04T11:30:34.000Z</published>
    <updated>2020-06-25T16:28:44.262Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”，判断对象的生死存活都有那些算法？</p><h2 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h2><p>实现原理：给对象中添加一个引用计数器，每当一个地方引用它是，计数器值就加1；任何时刻计数器为0的对象就是不可能再被使用的。<br>但是主流的Java虚拟机里面没有选用引用计数算法来管理内存，其中主要的原因是它很难解决对象之间相互引用的问题。<br>如下一个简单的例子：请看测试代码：对象objA和objB都有字段instance，赋值令 objA.instance=objB及objB.instance=objA，实际上这两个对象已经不可能再被访问了，但是他们因为互相引用这对方，导致引用计数都不为0，于是引用计数算法无法通过GC收集器回收他们。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class ReferenceCountingGC &#123;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * @Author: luomo</span><br><span class="line">     * @CreateTime: 2019&#x2F;11&#x2F;4</span><br><span class="line">     * @Description: 引用计数算法</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public Object instance &#x3D; null;</span><br><span class="line">    private  static final int _1MB&#x3D;1024*1024;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     *  这个成员属性的唯一意义就是占点内存，以便能在GC日志中看清楚是否被回收</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private  byte[] bigSize &#x3D;new byte[2 * _1MB];</span><br><span class="line">    public static void testGC()&#123;</span><br><span class="line">        ReferenceCountingGC objA &#x3D;new  ReferenceCountingGC();</span><br><span class="line">        ReferenceCountingGC objB &#x3D;new ReferenceCountingGC();</span><br><span class="line">        objA.instance&#x3D;objB;</span><br><span class="line">        objB.instance&#x3D;objA;</span><br><span class="line">        objA &#x3D;null;</span><br><span class="line">        objB &#x3D;null;</span><br><span class="line">        System.gc();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    public  static void main(String[] args)&#123;</span><br><span class="line">        testGC();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序运行结果如下：<br><img src="/2019/11/04/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/1.png" alt=" "><br>从运行结果中可以清楚看出虚拟机并没有因为互相引用就不回收他们，这也从侧面说明虚拟机并不是通过引用计数算法来判断对象是否存活的。</p><h2 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h2><p>实现原理：在主流的语言的主流实现中，比如Java、C#、甚至是古老的Lisp都是使用的可达性分析算法来判断对象是否存活的。</p><p>这个算法的核心思路就是通过一些列的“GC Roots”对象作为起始点，从这些对象开始往下搜索，搜索所经过的路径称之为“引用链”。</p><p>当一个对象到GC Roots没有任何引用链相连的时候，证明此对象是可以被回收的。如下图所示：<br><img src="/2019/11/04/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/2.png" alt="图片说明"><br>在Java中，可作为GC Roots对象的列表：</p><ul><li>Java虚拟机栈中的引用对象。</li><li>本地方法栈中JNI（既一般说的Native方法）引用的对象。</li><li>方法区中类静态常量的引用对象。</li><li>方法区中常量的引用对象。</li></ul>]]></content>
    
    <summary type="html">
    
      在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”，判断对象的生死存活都有那些算法？
    
    </summary>
    
      <category term="JVM" scheme="https://keysluomo.github.io/categories/JVM/"/>
    
    
      <category term="JVM" scheme="https://keysluomo.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Spark Demo(Serializable)</title>
    <link href="https://keysluomo.github.io/2019/10/29/Spark-Demo-Serializable/"/>
    <id>https://keysluomo.github.io/2019/10/29/Spark-Demo-Serializable/</id>
    <published>2019-10-29T04:44:46.000Z</published>
    <updated>2020-06-25T16:29:14.334Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本文中将介绍spark中Task执行序列化的开发问题</p><h1 id="开发环境准备"><a href="#开发环境准备" class="headerlink" title="开发环境准备"></a>开发环境准备</h1><p>本实验Spark运行在Windows上，为了开发Spark应用程序，在本地机器上需要有Jdk1.8和Maven环境。<br>确保我们的环境配置正常，我们可以使用快捷键 Win+R 输入cmd：<br>环境如下：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572341256892_8D8973B8667179A2319B041328F690BD" alt="图片说明" title="图片标题"><br>程序开发工具我们使用IDEA，创建maven项目，添加pom依赖</p><h1 id="编写Spark程序"><a href="#编写Spark程序" class="headerlink" title="编写Spark程序"></a>编写Spark程序</h1><h2 id="目录结构如下"><a href="#目录结构如下" class="headerlink" title="目录结构如下:"></a>目录结构如下:</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572341490332_07386CADB58D4E8A80763A83F9F6B34D" alt="图片说明" title="图片标题"> </p><h2 id="创建Serializable-scala"><a href="#创建Serializable-scala" class="headerlink" title="创建Serializable.scala:"></a>创建Serializable.scala:</h2><p>首先我们需要了解RDD中的函数传递：<br>在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要主要的是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。<br>如果我们对我们自定义的类不进行序列化：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> SparkDemo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>: luomo</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>: 2019/10/29</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>: Serializable from Driver to Executor</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object Serializable &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val config:SparkConf =<span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Serializable"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(config)</span><br><span class="line"></span><br><span class="line">    val rdd:RDD[String] = sc.parallelize(Array(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"Flink"</span>))</span><br><span class="line"></span><br><span class="line">    val search = <span class="keyword">new</span> Search(<span class="string">"h"</span>)</span><br><span class="line"></span><br><span class="line">    val match1:RDD[String] =search.getMatch1(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  class Search(query:String)&#123;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">    <span class="function">def  <span class="title">isMatch</span><span class="params">(s:String)</span>:Boolean </span>=&#123;</span><br><span class="line">      s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatch1</span><span class="params">(rdd:RDD[String])</span> :RDD[String] </span>= &#123;</span><br><span class="line">      rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatche2</span><span class="params">(rdd: RDD[String])</span>: RDD[String] </span>=&#123;</span><br><span class="line">      rdd.filter(x=&gt; x.contains(query))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572346997711_619AA778B91D5D25CAB063D1C7D53FFE" alt="图片说明" title="图片标题"> </p><p>可见，对于自己定义的普通类，Spark是无法直接将其序列化的。<br>需要我们自定义的类继承java.io.Serializable</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> SparkDemo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>: luomo</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>: 2019/10/29</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>: Serializable from Driver to Executor</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object Serializable &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val config:SparkConf =<span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Serializable"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(config)</span><br><span class="line"></span><br><span class="line">    val rdd:RDD[String] = sc.parallelize(Array(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"Flink"</span>))</span><br><span class="line"></span><br><span class="line">    val search = <span class="keyword">new</span> Search(<span class="string">"h"</span>)</span><br><span class="line"></span><br><span class="line">    val match1:RDD[String] =search.getMatch1(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//自定义类</span></span><br><span class="line">  class Search(query:String) extends  java.io.Serializable &#123;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">    <span class="function">def  <span class="title">isMatch</span><span class="params">(s:String)</span>:Boolean </span>=&#123;</span><br><span class="line">      s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatch1</span><span class="params">(rdd:RDD[String])</span> :RDD[String] </span>= &#123;</span><br><span class="line">      rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatche2</span><span class="params">(rdd: RDD[String])</span>: RDD[String] </span>=&#123;</span><br><span class="line">      rdd.filter(x=&gt; x.contains(query))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h1><p>如图我们过滤出包含字符h的字符串：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572351464995_DFD27DCB713FEB02D8628E6C67FD4273" alt="图片说明" title="图片标题"> </p>]]></content>
    
    <summary type="html">
    
      在本文中将介绍spark中Task执行序列化的开发问题
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
      <category term="Serializable" scheme="https://keysluomo.github.io/tags/Serializable/"/>
    
  </entry>
  
  <entry>
    <title>Spark 概述</title>
    <link href="https://keysluomo.github.io/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/"/>
    <id>https://keysluomo.github.io/2019/10/16/Spark-概述/</id>
    <published>2019-10-16T05:12:22.000Z</published>
    <updated>2020-06-25T16:23:36.652Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据处理框架"><a href="#大数据处理框架" class="headerlink" title="大数据处理框架"></a>大数据处理框架</h1><p>集群环境对于编程来说带来了很多挑战，首先就是并行化：这就要求我们以并行化的方式重写应用程序，以便我们可以利用更大范围节点的计算能力。集群环境的第二个挑战就是对单点失败的处理，节点宕机以及个别节点计算缓慢在集群环境中非常普遍，这会极大地影响程序的性能。最后一个挑战是集群在大多数情况下都会被多个用户分享，那么动态地进行计算资源的分配，也会干扰程序的执行。因此，针对集群环境出现了大量的大数据编程框架。首先我们要提到的就是Google的MapReduce，它给我们展示了一个简单通用和自动容错的批处理计算模型。<br>但是对于其他类型的计算，比如交互式和流式计算，MapReduce并不适合，这也导致了大量的不同于MapReduce的专有的数据处理模型的出现，比如Storm、Impala和GraphLab。随着新模型的不断出现，似乎对于大数据处理而言，我们应对不同类型的作业需要一系列不同的处理框架才能很好地完成。但是这些专有系统也有一些不足。</p><ul><li>重复工作：许多专有系统在解决同样的问题，比如分布式作业以及容错。举例来说，一个分布式的SQL引擎或者一个机器学习系统都需要实现并行聚合。这些问题在每个专有系统中会重复地被解决。</li><li>组合问题：在不同的系统之间进行组合计算是一件费力又不讨好的事情。对于特定的大数据应用程序而言，中间数据集是非常大的，而且移动的成本也非常高昂。在目前的环境中，我们需要将数据复制到稳定的存储系统中（比如HDFS），以便在不同的计算引擎中进行分享。然而，这样的复制可能比真正的计算所花费的代价要大，所以以流水线的形式将多个系统组合起来效率并不高。</li><li>适用范围的局限性：如果一个应用不适合一个专有的计算系统，那么使用者只能换一个系统，或者重写一个新的计算系统。</li><li>资源分配：在不同的计算引擎之间进行资源的动态共享是比较困难的，因为大多数的计算引擎都会假设它们在程序运行结束之前拥有相同的机器节点的资源。</li><li>管理问题：对于多个专有系统，需要花费更多的精力和时间来管理和部署。<br>尤其是对于终端使用者而言，他们需要学习多种API和系统模型。  </li></ul><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p>针对MapReduce及各种专有系统中出现的不足，伯克利大学推出了全新的统一大数据处理框架Spark，创新性地提出了RDD概念（一种新的抽象的弹性数据集），在某种程度上Spark是对MapReduce模型的一种扩展。要在MapReduce上实现其不擅长的计算工作（比如迭代式、交互式和流式），看上去是一件非常困难的事情，其实主要的原因是MapReduce缺乏一种特性，即在并行计算的各个阶段进行有效的数据共享，这种共享就是RDD的本质。利用这种有效的数据共享和类似MapReduce的操作接口，上述的各种专有类型计算都能够有效地表达，而且能够获得与专有系统同等的性能。<br>特别值得一提的是，从前对于集群处理的容错方式，比如MapReduce和Dryad，是将计算构建成为一个有向无环图的任务集。而这只能允许它们有效地重新计算部分DAG。在单独的计算之间（在迭代的计算步骤之间），除了复制文件，这些模型没有提供其他的存储抽象，这就显著地增加了在网络之间复制文件的代价。RDD能够适应当前大部分的数据并行算法和编程模型。  </p><h1 id="RDD表达能力"><a href="#RDD表达能力" class="headerlink" title="RDD表达能力"></a>RDD表达能力</h1><p>可以使用RDD实现很多现有的集群编程模型以及一些以前的模型不支持的新应用。在这些模型中，RDD能够取得和专有系统同样的性能，还能提供包括容错处理、滞后节点（straggler node）处理等这些专有系统缺乏的特性。这里会重点讨论如下四类模型。</p><ul><li>迭代算法：这是目前专有系统实现的非常普遍的一种应用场景，比如迭代算法可以用于图处理和机器学习。RDD能够很好地实现这些模型，包括Pregel、HaLoop和GraphLab等模型。</li><li>关系型查询：对于MapReduce来说非常重要的需求就是运行SQL查询，包括长期运行、数小时的批处理作业和交互式的查询。然而对于MapReduce而言，对比并行数据库进行交互式查询，有其内在的缺点，比如由于其容错的模型而导致速度很慢。利用RDD模型，可以通过实现许多通用的数据库引擎特性，从而获得非常好的性能。</li><li>MapReduce 批处理：RDD提供的接口是MapReduce的超集，所以RDD可以有效地运行利用MapReduce实现的应用程序，另外RDD还适合更加抽象的基于DAG的应用程序，比如DryadLINQ。</li><li>流式处理：目前的流式系统也只提供了有限的容错处理，需要消耗系统非常大的拷贝代价或者非常长的容错时间。特别是在目前的系统中，基本都是基于连续计算的模型，常驻的有状态的操作会处理到达的每一条记录。为了恢复失败的节点，它们需要为每一个操作复制两份操作，或者是将上游的数据进行代价非常大的操作重放。利用RDD实现一种新的模型——离散数据流（D-Stream），可以克服上面的这些问题。D-Stream将流式计算当作一系列的短小而确定的批处理操作，而不是常驻的有状态的操作，将两个离散流之间的状态保存在RDD中。离散流模型能够允许通过RDD的继承关系图（lineage）进行并行性的恢复而不需要进行数据拷贝。  </li></ul><h1 id="Spark子系统"><a href="#Spark子系统" class="headerlink" title="Spark子系统"></a>Spark子系统</h1><p>如果按照目前流行的大数据处理场景来划分，可以将大数据处理分为如下三种情况。</p><ul><li>复杂的批量数据处理（batch data processing），通常的时间跨度为数十分钟到数小时。</li><li>基于历史数据的交互式查询（interactive query），通常的时间跨度为数十秒到数分钟。</li><li>基于实时数据流的数据处理（streaming data processing），通常的时间跨度为数百毫秒到数秒。<br>由于RDD具有丰富的表达能力，所以伯克利在Spark Core的基础之上衍生出了能够同时处理上述三种情形的统一大数据处理平台，如图1-1所示。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/spark.png" alt="图片说明"></div></li><li>Spark Core：基于RDD提供了丰富的操作接口，利用DAG进行统一的任务规划，使得Spark能够更加灵活地处理类似MapReduce的批处理作业。</li><li>Shark/Spark SQL:兼容Hive的接口HQL，提供了比Hive高出10~100倍的查询速度的分布式SQL引擎。</li><li>Spark Streaming：将流式计算分解成一系列的短小的批处理作业，利用Spark轻量级和低延时的调度框架，可以很好的支持流式处理。目前已经支持的数据输入源包括Kafka、Flume、Twitter、TCP sockets。</li><li>GraphX：基于Spark的图计算框架，兼容Pregel和GraphLab接口，增强了图构建以及图转换功能。</li><li>MLlib:Spark Core 天然地非常适合于迭代式运算，MLlib就是构建在Spark上的机器学习算法库。目前已经可以支持常用的分类算法、聚类算法、推荐算法等。<br>Spark生态系统的目标就是将批处理、交互式处理、流式处理融合到同一个软件栈中。对于最终的用户或者是开发者而言，Spark生态系统有如下特性。</li><li>Spark生态系统兼容Hadoop生态系统。这个特性对于最终用户至关重要，虽然Spark 通用引擎在一定程度上是用来取代MapReduce系统的，但是Spark能够完美兼容Hadoop生态中的HDFS和YARN等其他组件，使得现有的Hadoop用户能够非常容易地迁移到Spark系统中。图1-2显示了Spark与Hadoop生态的兼容性。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/1-2.png" alt></div></li><li>Spark生态系统学习成本很低。要实现一个相对完整的端到端解决方案，以前需要部署维护多个专有系统，现在只需要一个Spark系统。另外，如果开发者对Spark Core的原理有比较深入的理解，对构架在Spark Core之上的其他组件的运用将会非常容易。图1-3对比了Spark生态和其他大数据专有系统的代码量。在图1-3中的Spark一项中，批处理对应Spark Core，交互式处理对应Shark/Spark SQL，流计算对应Spark Streaming，而图计算对应GraphX。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/1-3.png" alt></div></li><li>Spark 性能表现优异。由于Spark利用DAG进行调度执行规划，所以在多任务计算以及迭代计算中能够大量减少磁盘I/O的时间。另外，对于每一项任务启动一个线程，而不是启动一个进程，大大缩短了任务启动时间。</li><li>Spark有强大的社区支持。Spark近一年多来保持非常迅猛的发展势头，被誉为大数据处理的未来。Spark的创始团队成立了Databricks公司，全力支持Spark 生态的发展。目前Hadoop商业版本发行公司中，已经有Cloudera、Hortonworks、MapR等公司相继宣布支持Spark软件栈。图1-4显示了Spark不同版本发布时对社区做出贡献的贡献者数量变化情况。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/1-4.png" alt></div></li><li>Spark 支持多种语言编程接口。Spark生态本身是使用Scala语言编写的，但是考虑到其流行性，因此Spark从一开始就支持Java和Python接口，方便Spark 程序开发者自由选择。</li></ul>]]></content>
    
    <summary type="html">
    
      大数据处理框架Spark
    
    </summary>
    
      <category term="Spark" scheme="https://keysluomo.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://keysluomo.github.io/tags/Spark/"/>
    
  </entry>
  
</feed>
