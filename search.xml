<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Apache Flink 状态管理与容错机制</title>
    <url>/2019/10/13/Apache-Flink-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h1 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h1><h3 id="作者：孙梦瑶"><a href="#作者：孙梦瑶" class="headerlink" title="作者：孙梦瑶"></a>作者：孙梦瑶</h3><h3 id="整理：韩非"><a href="#整理：韩非" class="headerlink" title="整理：韩非"></a>整理：韩非</h3><h3 id="校对：邱从贤（山智）"><a href="#校对：邱从贤（山智）" class="headerlink" title="校对：邱从贤（山智）"></a>校对：邱从贤（山智）</h3><p><a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>本文主要分享内容如下：</p>
<p>状态管理的基本概念；</p>
<p>状态的类型与使用示例；</p>
<p>容错机制与故障恢复。</p>
<h1 id="一-状态管理的基本概念"><a href="#一-状态管理的基本概念" class="headerlink" title="一. 状态管理的基本概念"></a>一. 状态管理的基本概念</h1><h2 id="1-什么是状态"><a href="#1-什么是状态" class="headerlink" title="1.什么是状态"></a>1.什么是状态</h2><p>  <img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571142947113_E857A8299D24A087FF0E55FCF75A5E09" alt="图片说明" title="图片标题">  </p>
<p>首先举一个无状态计算的例子：消费延迟计算。假设现在有一个消息队列，消息队列中有一个生产者持续往消费队列写入消息，多个消费者分别从消息队列中读取消息。从图上可以看出，生产者已经写入 16 条消息，Offset 停留在 15 ；有 3 个消费者，有的消费快，而有的消费慢。消费快的已经消费了 13 条数据，消费者慢的才消费了 7、8 条数据。</p>
<p>如何实时统计每个消费者落后多少条数据，如图给出了输入输出的示例。可以了解到输入的时间点有一个时间戳，生产者将消息写到了某个时间点的位置，每个消费者同一时间点分别读到了什么位置。刚才也提到了生产者写入了 15 条，消费者分别读取了 10、7、12 条。那么问题来了，怎么将生产者、消费者的进度转换为右侧示意图信息呢？</p>
<p>consumer 0 落后了 5 条，consumer 1 落后了 8 条，consumer 2 落后了 3 条，根据 Flink 的原理，此处需进行 Map 操作。Map 首先把消息读取进来，然后分别相减，即可知道每个 consumer 分别落后了几条。Map 一直往下发，则会得出最终结果。</p>
<p>大家会发现，在这种模式的计算中，无论这条输入进来多少次，输出的结果都是一样的，因为单条输入中已经包含了所需的所有信息。消费落后等于生产者减去消费者。生产者的消费在单条数据中可以得到，消费者的数据也可以在单条数据中得到，所以相同输入可以得到相同输出，这就是一个无状态的计算。</p>
<p>相应的什么是有状态的计算？<br><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571142977408_909F1111218DA0F992FEE083105341CF" alt="图片说明" title="图片标题"><br>以访问日志统计量的例子进行说明，比如当前拿到一个 Nginx 访问日志，一条日志表示一个请求，记录该请求从哪里来，访问的哪个地址，需要实时统计每个地址总共被访问了多少次，也即每个 API 被调用了多少次。可以看到下面简化的输入和输出，输入第一条是在某个时间点请求 GET 了 /api/a；第二条日志记录了某个时间点 Post /api/b ;第三条是在某个时间点 GET了一个 /api/a，总共有 3 个 Nginx 日志。从这 3 条 Nginx 日志可以看出，第一条进来输出 /api/a 被访问了一次，第二条进来输出 /api/b 被访问了一次，紧接着又进来一条访问 api/a，所以 api/a 被访问了 2 次。不同的是，两条 /api/a 的 Nginx 日志进来的数据是一样的，但输出的时候结果可能不同，第一次输出 count=1 ，第二次输出 count=2，说明相同输入可能得到不同输出。输出的结果取决于当前请求的 API 地址之前累计被访问过多少次。第一条过来累计是 0 次，count = 1，第二条过来 API 的访问已经有一次了，所以 /api/a 访问累计次数 count=2。单条数据其实仅包含当前这次访问的信息，而不包含所有的信息。要得到这个结果，还需要依赖 API 累计访问的量，即状态。</p>
<p>这个计算模式是将数据输入算子中，用来进行各种复杂的计算并输出数据。这个过程中算子会去访问之前存储在里面的状态。另外一方面，它还会把现在的数据对状态的影响实时更新，如果输入 200 条数据，最后输出就是 200 条结果。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143001685_B490A739A55C2916AA8993039A8E9D17" alt="图片说明" title="图片标题"><br>什么场景会用到状态呢？下面列举了常见的 4 种：</p>
<p>去重：比如上游的系统数据可能会有重复，落到下游系统时希望把重复的数据都去掉。去重需要先了解哪些数据来过，哪些数据还没有来，也就是把所有的主键都记录下来，当一条数据到来后，能够看到在主键当中是否存在。</p>
<p>窗口计算：比如统计每分钟 Nginx 日志 API 被访问了多少次。窗口是一分钟计算一次，在窗口触发前，如 08:00 ~ 08:01 这个窗口，前59秒的数据来了需要先放入内存，即需要把这个窗口之内的数据先保留下来，等到 8:01 时一分钟后，再将整个窗口内触发的数据输出。未触发的窗口数据也是一种状态。</p>
<p>机器学习/深度学习：如训练的模型以及当前模型的参数也是一种状态，机器学习可能每次都用有一个数据集，需要在数据集上进行学习，对模型进行一个反馈。</p>
<p>访问历史数据：比如与昨天的数据进行对比，需要访问一些历史数据。如果每次从外部去读，对资源的消耗可能比较大，所以也希望把这些历史数据也放入状态中做对比。</p>
<h2 id="2-为什么要管理状态"><a href="#2-为什么要管理状态" class="headerlink" title="2. 为什么要管理状态"></a>2. 为什么要管理状态</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143038696_E074DE090177F98DB5E17EA0EEA487FD" alt="图片说明" title="图片标题"><br>管理状态最直接的方式就是将数据都放到内存中，这也是很常见的做法。比如在做 WordCount 时，Word 作为输入，Count 作为输出。在计算的过程中把输入不断累加到 Count。</p>
<p>但对于流式作业有以下要求：</p>
<p>7*24小时运行，高可靠；</p>
<p>数据不丢不重，恰好计算一次；</p>
<p>数据实时产出，不延迟；</p>
<p>基于以上要求，内存的管理就会出现一些问题。由于内存的容量是有限制的。如果要做 24 小时的窗口计算，将 24 小时的数据都放到内存，可能会出现内存不足；另外，作业是 7*24，需要保障高可用，机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，保证运行的作业不受影响；此外，考虑横向扩展，假如网站的访问量不高，统计每个 API 访问次数的程序可以用单线程去运行，但如果网站访问量突然增加，单节点无法处理全部访问数据，此时需要增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点也问题之一。因此，将数据都放到内存中，并不是最合适的一种状态管理方式。</p>
<h2 id="3-理想的状态管理"><a href="#3-理想的状态管理" class="headerlink" title="3. 理想的状态管理"></a>3. 理想的状态管理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143056691_3C14E7779B855AD529EC7BDD67713F1A" alt="图片说明" title="图片标题"><br>最理想的状态管理需要满足易用、高效、可靠三点需求：</p>
<p>易用，Flink 提供了丰富的数据结构、多样的状态组织形式以及简洁的扩展接口，让状态管理更加易用；<br>高效，实时作业一般需要更低的延迟，一旦出现故障，恢复速度也需要更快；当处理能力不够时，可以横向扩展，同时在处理备份时，不影响作业本身处理性能；<br>可靠，Flink 提供了状态持久化，包括不丢不重的语义以及具备自动的容错能力，比如 HA，当节点挂掉后会自动拉起，不需要人工介入。</p>
<h1 id="二-Flink-状态的类型与使用示例"><a href="#二-Flink-状态的类型与使用示例" class="headerlink" title="二. Flink 状态的类型与使用示例"></a>二. Flink 状态的类型与使用示例</h1><h2 id="1-Managed-State-amp-Raw-State"><a href="#1-Managed-State-amp-Raw-State" class="headerlink" title="1. Managed State &amp; Raw State"></a>1. Managed State &amp; Raw State</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143093887_CDD5C3A073E8EDF8BCE5DBC3F331915E" alt="图片说明" title="图片标题"><br>Managed State 是 Flink 自动管理的 State，而 Raw State 是原生态 State，两者的区别如下：</p>
<p>从状态管理方式的方式来说，Managed State 由 Flink Runtime 管理，自动存储，自动恢复，在内存管理上有优化；而 Raw State 需要用户自己管理，需要自己序列化，Flink 不知道 State 中存入的数据是什么结构，只有用户自己知道，需要最终序列化为可存储的数据结构。</p>
<p>从状态数据结构来说，Managed State 支持已知的数据结构，如 Value、List、Map 等。而 Raw State只支持字节数组 ，所有状态都要转换为二进制字节数组才可以。</p>
<p>从推荐使用场景来说，Managed State 大多数情况下均可使用，而 Raw State 是当 Managed State 不够用时，比如需要自定义 Operator 时，推荐使用 Raw State。</p>
<h2 id="2-Keyed-State-amp-Operator-State"><a href="#2-Keyed-State-amp-Operator-State" class="headerlink" title="2. Keyed State &amp; Operator State"></a>2. Keyed State &amp; Operator State</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143125792_7A7126166D67C3FDDB90CEFFD7505ED8" alt="图片说明" title="图片标题"><br>Managed State 分为两种，一种是 Keyed State；另外一种是 Operator State。在Flink Stream模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream 。</p>
<p>每个 Key 对应一个 State，即一个 Operator 实例处理多个 Key，访问相应的多个 State，并由此就衍生了 Keyed State。Keyed State 只能用在 KeyedStream 的算子中，即在整个程序中没有 keyBy 的过程就没有办法使用 KeyedStream。</p>
<p>相比较而言，Operator State 可以用于所有算子，相对于数据源有一个更好的匹配方式，常用于 Source，例如 FlinkKafkaConsumer。相比 Keyed State，一个 Operator 实例对应一个 State，随着并发的改变，Keyed State 中，State 随着 Key 在实例间迁移，比如原来有 1 个并发，对应的 API 请求过来，/api/a 和 /api/b 都存放在这个实例当中；如果请求量变大，需要扩容，就会把 /api/a 的状态和 /api/b 的状态分别放在不同的节点。由于 Operator State 没有 Key，并发改变时需要选择状态如何重新分配。其中内置了 2 种分配方式：一种是均匀分配，另外一种是将所有 State 合并为全量 State 再分发给每个实例。</p>
<p>在访问上，Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个Rich Function。Operator State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。在数据结构上，Keyed State 支持的数据结构，比如 ValueState、ListState、ReducingState、AggregatingState 和 MapState；而 Operator State 支持的数据结构相对较少，如 ListState。</p>
<h2 id="3-Keyed-State-使用示例"><a href="#3-Keyed-State-使用示例" class="headerlink" title="3. Keyed State 使用示例"></a>3. Keyed State 使用示例</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143165677_9823447C87064C14565BD91D2B3A3434" alt="图片说明" title="图片标题"><br>Keyed State 有很多种，如图为几种 Keyed State 之间的关系。首先 State 的子类中一级子类有 ValueState、MapState、AppendingState。AppendingState 又有一个子类 MergingState。MergingState 又分为 3 个子类分别是ListState、ReducingState、AggregatingState。这个继承关系使它们的访问方式、数据结构也存在差异。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143194008_77861EEA917B2DB9735F3A5C9AC49413" alt="图片说明" title="图片标题"><br>几种 Keyed State 的差异具体体现在：</p>
<p>ValueState 存储单个值，比如 Wordcount，用 Word 当 Key，State 就是它的 Count。这里面的单个值可能是数值或者字符串，作为单个值，访问接口可能有两种，get 和 set。在 State 上体现的是 update(T) / T value()。</p>
<p>MapState 的状态数据类型是 Map，在 State 上有 put、remove等。需要注意的是在 MapState 中的 key 和 Keyed state 中的 key 不是同一个。</p>
<p>ListState 状态数据类型是 List，访问接口如 add、update 等。</p>
<p>ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值，原因在于其中的 add 方法不是把当前的元素追加到列表中，而是把当前元素直接更新进了 Reducing 的结果中。</p>
<p>AggregatingState 的区别是在访问接口，ReducingState 中 add（T）和 T get() 进去和出来的元素都是同一个类型，但在 AggregatingState 输入的 IN，输出的是 OUT。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143217922_688BF6B699D86F2D4A0129F59E6CC1F5" alt="图片说明" title="图片标题"><br>下面以 ValueState 为例，来阐述一下具体如何使用，以状态机的案例来讲解 。</p>
<p>源代码地址：<a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java" target="_blank" rel="noopener">https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java</a></p>
<p>感兴趣的同学可直接查看完整源代码，在此截取部分。如图为 Flink 作业的主方法与主函数中的内容，前面的输入、后面的输出以及一些个性化的配置项都已去掉，仅保留了主干。</p>
<p>首先 events 是一个 DataStream，通过 env.addSource 加载数据进来，接下来有一个 DataStream 叫 alerts，先 keyby 一个 sourceAddress，然后在 flatMap 一个StateMachineMapper。StateMachineMapper 就是一个状态机，状态机指有不同的状态与状态间有不同的转换关系的结合，以买东西的过程简单举例。首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货。已付款，待发货的状态再来一个事件发货，订单状态将会变为配送中，配送中的状态再来一个事件签收，则该订单的状态就变为已签收。在整个过程中，随时都可以来一个事件，取消订单，无论哪个状态，一旦触发了取消订单事件最终就会将状态转移到已取消，至此状态就结束了。</p>
<p>Flink 写状态机是如何实现的？首先这是一个 RichFlatMapFunction，要用 Keyed State getRuntimeContext，getRuntimeContext 的过程中需要 RichFunction，所以需要在 open 方法中获取 currentState ，然后 getState，currentState 保存的是当前状态机上的状态。</p>
<p>如果刚下订单，那么 currentState 就是待付款状态，初始化后，currentState 就代表订单完成。订单来了后，就会走 flatMap 这个方法，在 flatMap 方法中，首先定义一个 State，从 currentState 取出，即 Value，Value 取值后先判断值是否为空，如果 sourceAddress state 是空，则说明没有被使用过，那么此状态应该为刚创建订单的初始状态，即待付款。然后赋值 state = State.Initial，注意此处的 State 是本地的变量，而不是 Flink 中管理的状态，将它的值从状态中取出。接下来在本地又会来一个变量，然后 transition，将事件对它的影响加上，刚才待付款的订单收到付款成功的事件，就会变成已付款，待发货，然后 nextState 即可算出。此外，还需要判断 State 是否合法，比如一个已签收的订单，又来一个状态叫取消订单，会发现已签收订单不能被取消，此时这个状态就会下发，订单状态为非法状态。</p>
<p>如果不是非法的状态，还要看该状态是否已经无法转换，比如这个状态变为已取消时，就不会在有其他的状态再发生了，此时就会从 state 中 clear。clear 是所有的 Flink 管理 keyed state 都有的公共方法，意味着将信息删除，如果既不是一个非法状态也不是一个结束状态，后面可能还会有更多的转换，此时需要将订单的当前状态 update ，这样就完成了 ValueState 的初始化、取值、更新以及清零，在整个过程中状态机的作用就是将非法的状态进行下发，方便下游进行处理。其他的状态也是类似的使用方式。</p>
<h1 id="三-容错机制与故障恢复"><a href="#三-容错机制与故障恢复" class="headerlink" title="三. 容错机制与故障恢复"></a>三. 容错机制与故障恢复</h1><h2 id="1-状态如何保存及恢复"><a href="#1-状态如何保存及恢复" class="headerlink" title="1. 状态如何保存及恢复"></a>1. 状态如何保存及恢复</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143252142_BE28DFB6669E19340E15658720E17D19" alt="图片说明" title="图片标题"><br>Flink 状态保存主要依靠 Checkpoint 机制，Checkpoint 会定时制作分布式快照，对程序中的状态进行备份。分布式快照是如何实现的可以参考【第二课时】的内容，这里就不再阐述分布式快照具体是如何实现的。分布式快照 Checkpoint 完成后，当作业发生故障了如何去恢复？假如作业分布跑在 3 台机器上，其中一台挂了。这个时候需要把进程或者线程移到 active 的 2 台机器上，此时还需要将整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态，然后从该点开始继续处理。</p>
<p>如果要从 Checkpoint 恢复，必要条件是数据源需要支持数据重新发送。Checkpoint恢复后， Flink 提供两种一致性语义，一种是恰好一次，一种是至少一次。在做 Checkpoint时，可根据 Barries 对齐来判断是恰好一次还是至少一次，如果对齐，则为恰好一次，否则没有对齐即为至少一次。如果只有一个上游，也就是说 Barries 是不需要对齐的的；如果只有一个 Checkpoint 在做，不管什么时候从 Checkpoint 恢复，都会恢复到刚才的状态；如果有多个上游，假如一个上游的 Barries 到了，另一个 Barries 还没有来，如果这个时候对状态进行快照，那么从这个快照恢复的时候其中一个上游的数据可能会有重复。</p>
<p>Checkpoint 通过代码的实现方法如下：</p>
<p>首先从作业的运行环境 env.enableCheckpointing 传入 1000，意思是做 2 个 Checkpoint 的事件间隔为 1 秒。Checkpoint 做的越频繁，恢复时追数据就会相对减少，同时 Checkpoint 相应的也会有一些 IO 消耗。</p>
<p>接下来是设置 Checkpoint 的 model，即设置了 Exactly_Once 语义，表示需要 Barrier 对齐，这样可以保证消息不会丢失也不会重复。</p>
<p>setMinPauseBetweenCheckpoints 是 2 个 Checkpoint 之间最少是要等 500ms，也就是刚做完一个 Checkpoint。比如某个 Checkpoint 做了700ms，按照原则过 300ms 应该是做下一个 Checkpoint，因为设置了 1000ms 做一次 Checkpoint 的，但是中间的等待时间比较短，不足 500ms 了，需要多等 200ms，因此以这样的方式防止 Checkpoint 太过于频繁而导致业务处理的速度下降。</p>
<p>setCheckpointTimeout 表示做 Checkpoint 多久超时，如果 Checkpoint 在 1min 之内尚未完成，说明 Checkpoint 超时失败。</p>
<p>setMaxConcurrentCheckpoints 表示同时有多少个 Checkpoint 在做快照，这个可以根据具体需求去做设置。</p>
<p>enableExternalizedCheckpoints 表示下 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。</p>
<p>上面讲过，除了故障恢复之外，还需要可以手动去调整并发重新分配这些状态。手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在，那么作业如何恢复数据？</p>
<p>一方面 Flink 在 Cancel 时允许在外部介质保留 Checkpoint ；另一方面，Flink 还有另外一个机制是 SavePoint。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143266754_72F6543E219A34B25A05F127BF7A64E8" alt="图片说明" title="图片标题"><br>Savepoint 与 Checkpoint 类似，同样是把状态存储到外部介质。当作业失败时，可以从外部恢复。Savepoint 与 Checkpoint 有什么区别呢？</p>
<p>从触发管理方式来讲，Checkpoint 由 Flink 自动触发并管理，而 Savepoint 由用户手动触发并人肉管理；</p>
<p>从用途来讲，Checkpoint 在 Task 发生异常时快速恢复，例如网络抖动或超时异常，而 Savepoint 有计划地进行备份，使作业能停止后再恢复，例如修改代码、调整并发；</p>
<p>最后从特点来讲，Checkpoint 比较轻量级，作业出现问题会自动从故障中恢复，在作业停止后默认清除；而 Savepoint 比较持久，以标准格式存储，允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复。</p>
<h2 id="2-可选的状态存储方式"><a href="#2-可选的状态存储方式" class="headerlink" title="2. 可选的状态存储方式"></a>2. 可选的状态存储方式</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143282020_64E753626240D5EA470570F497051FEB" alt="图片说明" title="图片标题"><br>Checkpoint 的存储，第一种是内存存储，即 MemoryStateBackend，构造方法是设置最大的StateSize，选择是否做异步快照，这种存储状态本身存储在 TaskManager 节点也就是执行节点内存中的，因为内存有容量限制，所以单个 State maxStateSize 默认 5 M，且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M。Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。推荐使用的场景为：本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191015/9094293_1571143322242_5498DC3E67A0518E3A1BA95DE5F26257" alt="图片说明" title="图片标题"><br>另一种就是在文件系统上的 FsStateBackend ，构建方法是需要传一个文件路径和是否异步快照。State 依然在 TaskManager 内存中，但不会像 MemoryStateBackend 有 5 M 的设置上限，Checkpoint 存储在外部文件系统（本地或 HDFS），打破了总大小 Jobmanager 内存的限制。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。推荐使用的场景、常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启HA的作业。</p>
<p>还有一种存储为 RocksDBStateBackend ，RocksDB 是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key最大 2G，总大小不超过配置的文件系统容量即可。推荐使用的场景为：超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。</p>
<h1 id="四-总结"><a href="#四-总结" class="headerlink" title="四. 总结"></a>四. 总结</h1><h2 id="1-为什么要使用状态？"><a href="#1-为什么要使用状态？" class="headerlink" title="1. 为什么要使用状态？"></a>1. 为什么要使用状态？</h2><p>前面提到有状态的作业要有有状态的逻辑，有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。所以需要通过状态来满足业务逻辑。</p>
<h2 id="2-为什么要管理状态？"><a href="#2-为什么要管理状态？" class="headerlink" title="2.为什么要管理状态？"></a>2.为什么要管理状态？</h2><p>使用了状态，为什么要管理状态？因为实时作业需要7*24不间断的运行，需要应对不可靠的因素而带来的影响。</p>
<h2 id="3-如何选择状态的类型和存储方式？"><a href="#3-如何选择状态的类型和存储方式？" class="headerlink" title="3.如何选择状态的类型和存储方式？"></a>3.如何选择状态的类型和存储方式？</h2><p>那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>状态管理与容错机制</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Flink DataStream API 编程</title>
    <url>/2019/10/08/Apache-Flink-DataStream-API-%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<p>#转载<br>作者：崔星灿<br>整理：高赟<br><a href="https://ververica.cn/developers/apache-flink-basic-zero-iii-datastream-api-programming/" target="_blank" rel="noopener">https://ververica.cn/developers/apache-flink-basic-zero-iii-datastream-api-programming/</a></p>
<h1 id="1-流处理基本概念"><a href="#1-流处理基本概念" class="headerlink" title="1. 流处理基本概念"></a>1. 流处理基本概念</h1><p>对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是对立统一的，它们的关系有点类似于对于 Java 中的 ArrayList 中的元素，是直接看作一个有限数据集并用下标去访问，还是用迭代器去访问。<br><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570537175558_C8AF084FBC88BD49BD6B715195F1B6C1" alt="图片说明" title="图片标题"> </p>
<p>图1. 左图硬币分类器。硬币分类器也可以看作一个流处理系统，用于硬币分类的各部分组件提前串联在一起，硬币不断进入系统，并最终被输出到不同的队列中供后续使用。右图同理。<br>流处理系统本身有很多自己的特点。一般来说，由于需要支持无限数据集的处理，流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中。<br><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570537192886_27A88C12397D0EF8F47E865DFEE4A173" alt="图片说明" title="图片标题"> </p>
<p>图2. 一个 DAG 计算逻辑图与实际的物理时模型。<br>逻辑图中的每个算子在物理图中可能有多个并发。<br>对于实际的分布式流处理引擎，它们的实际运行时物理模型要更复杂一些，这是由于每个算子都可能有多个实例。如图 2 所示，作为 Source 的 A 算子有两个实例，中间算子 C 也有两个实例。在逻辑模型中，A 和 B 是 C 的上游节点，而在对应的物理逻辑中，C 的所有实例和 A、B 的所有实例之间可能都存在数据交换。在物理模型中，我们会根据计算逻辑，采用系统自动优化或人为指定的方式将计算工作分布到不同的实例中。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。</p>
<p>▼示例1. Apache Storm 构造 DAG 计算图。Apache Storm 的接口定义更加“面向操作”，因此更加底层。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TopologyBuilder builder &#x3D; new TopologyBuilder();</span><br><span class="line"></span><br><span class="line">builder.setSpout(&quot;spout&quot;, new RandomSentenceSpout(), 5);</span><br><span class="line">builder.setBolt(&quot;split&quot;, new SplitSentence(), 8).shuffleGrouping(&quot;spout&quot;);</span><br><span class="line">builder.setBolt(&quot;count&quot;, new WordCount(), 12).fieldsGrouping(&quot;split&quot;, new Fields(&quot;word&quot;));</span><br></pre></td></tr></table></figure>
<p>▼ 示例2. Apache Flink 构造 DAG 计算图。Apache Flink 的接口定义更加“面向数据”，因此更加高层。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text &#x3D; env.readTextFile (&quot;input&quot;);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts &#x3D; text.flatMap(new Tokenizer()).keyBy(0).sum(1);</span><br><span class="line">counts.writeAsText(&quot;output&quot;);</span><br></pre></td></tr></table></figure>
<p>由于流处理的计算逻辑是通过 DAG 图来表示的，因此它们的大部分 API 都是围绕构建这种计算逻辑图来设计的。例如，对于几年前非常流行的 Apache Storm，它的 Word Count 的示例如表 1 所示。基于 Apache Storm 用户需要在图中添加 Spout 或 Bolt 这种算子，并指定算子之前的连接方式。这样，在完成整个图的构建之后，就可以将图提交到远程或本地集群运行。</p>
<p>与之对比，Apache Flink 的接口虽然也是在构建计算逻辑图，但是 Flink 的 API 定义更加面向数据本身的处理逻辑，它把数据流抽象成为一个无限集，然后定义了一组集合上的操作，然后在底层自动构建相应的 DAG 图。可以看出，Flink 的 API 要更“上层”一些。许多研究者在进行实验时，可能会更喜欢自由度高的 Storm，因为它更容易保证实现预想的图结构；而在工业界则更喜欢 Flink 这类高级 API，因为它使用更加简单。</p>
<h1 id="2-Flink-DataStream-API-概览"><a href="#2-Flink-DataStream-API-概览" class="headerlink" title="2. Flink DataStream API 概览"></a>2. Flink DataStream API 概览</h1><p>基于前面对流处理的基本概念，本节将详细介绍 Flink DataStream API 的使用方式。我们首先还是从一个简单的例子开始看起。表3是一个流式 Word Count 的示例，虽然它只有 5 行代码，但是它给出了基于 Flink DataStream API 开发程序的基本结构。</p>
<p>▼ 示例2. 基于 Flink DataStream API 的 Word Count 示例。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;1、设置运行环境</span><br><span class="line">StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">&#x2F;&#x2F;2、配置数据源读取数据</span><br><span class="line">DataStream&lt;String&gt; text &#x3D; env.readTextFile (&quot;input&quot;);</span><br><span class="line">&#x2F;&#x2F;3、进行一系列转换</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts &#x3D; text.flatMap(new Tokenizer()).keyBy(0).sum(1);</span><br><span class="line">&#x2F;&#x2F;4、配置数据汇写出数据</span><br><span class="line">counts.writeAsText(&quot;output&quot;);</span><br><span class="line">&#x2F;&#x2F;5、提交执行</span><br><span class="line">env.execute(&quot;Streaming WordCount&quot;);</span><br></pre></td></tr></table></figure>
<p>为了实现流式 Word Count，我们首先要先获得一个 StreamExecutionEnvironment 对象。它是我们构建图过程中的上下文对象。基于这个对象，我们可以添加一些算子。对于流处理程度，我们一般需要首先创建一个数据源去接入数据。在这个例子中，我们使用了 Environment 对象中内置的读取文件的数据源。这一步之后，我们拿到的是一个 DataStream 对象，它可以看作一个无限的数据集，可以在该集合上进行一序列的操作。例如，在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。</p>
<p>最后，我们需要调用 env#execute 方法来开始程序的执行。需要强调的是，前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。</p>
<p>基于流式 Word Count 的例子可以看出，基于 Flink 的 DataStream API 来编写流处理程序一般需要三步：通过 Source 接入数据、进行一系统列的处理以及将数据写出。最后，不要忘记显式调用 Execute 方式，否则前面编写的逻辑并不会真正执行。<br><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570537294810_1004D3B6DA52ECDB957ACA30B9C3F264" alt="图片说明" title="图片标题"> </p>
<p>图3. Flink DataStream 操作概览<br>从上面的例子中还可以看出，Flink DataStream API 的核心，就是代表流数据的 DataStream 对象。整个计算逻辑图的构建就是围绕调用 DataStream 对象上的不同操作产生新的 DataStream 对象展开的。整体来说，DataStream 上的操作可以分为四类。第一类是对于单条记录的操作，比如筛除掉不符合要求的记录（Filter 操作），或者将每条记录都做一个转换（Map 操作）。第二类是对多条记录的操作。比如说统计一个小时内的订单总成交量，就需要将一个小时内的所有订单记录的成交量加到一起。为了支持这种类型的操作，就得通过 Window 将需要的记录关联到一起进行处理。第三类是对多个流进行操作并转换为单个流。例如，多个流可以通过 Union、Join 或 Connect 等操作合到一起。这些操作合并的逻辑不同，但是它们最终都会产生了一个新的统一的流，从而可以进行一些跨流的操作。最后， DataStream 还支持与合并对称的操作，即把一个流按一定规则拆分为多个流（Split 操作），每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570537308371_60CD8BAC6B1F4A4CAFC6E077E20E337A" alt="图片说明" title="图片标题"><br>图4. 不同类型的 DataStream 子类型。不同的子类型支持不同的操作集合。<br>为了支持这些不同的流操作，Flink 引入了一组不同的流类型，用来表示某些操作的中间流数据集类型。完整的类型转换关系如图4所示。首先，对于一些针对单条记录的操作，如 Map 等，操作的结果仍然是是基本的 DataStream 类型。然后，对于 Split 操作，它会首先产生一个 SplitStream，基于 SplitStream 可以使用 Select 方法来筛选出符合要求的记录并再将得到一个基本的流。</p>
<p>类似的，对于 Connect 操作，在调用 streamA.connect(streamB)后可以得到一个专门的 ConnectedStream。ConnectedStream 支持的操作与普通的 DataStream 有所区别，由于它代表两个不同的流混合的结果，因此它允许用户对两个流中的记录分别指定不同的处理逻辑，然后它们的处理结果形成一个新的 DataStream 流。由于不同记录的处理是在同一个算子中进行的，因此它们在处理时可以方便的共享一些状态信息。上层的一些 Join 操作，在底层也是需要依赖于 Connect 操作来实现的。</p>
<p>另外，如前所述，我们可以通过 Window 操作对流可以按时间或者个数进行一些切分，从而将流切分成一个个较小的分组。具体的切分逻辑可以由用户进行选择。当一个分组中所有记录都到达后，用户可以拿到该分组中的所有记录，从而可以进行一些遍历或者累加操作。这样，对每个分组的处理都可以得到一组输出数据，这些输出数据形成了一个新的基本流。</p>
<p>对于普通的 DataStream，我们必须使用 allWindow 操作，它代表对整个流进行统一的 Window 处理，因此是不能使用多个算子实例进行同时计算的。针对这一问题，就需要我们首先使用 KeyBy 方法对记录按 Key 进行分组，然后才可以并行的对不同 Key 对应的记录进行单独的 Window 操作。KeyBy 操作是我们日常编程中最重要的操作之一，下面我们会更详细的介绍。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570538395925_84B3A35C34414BE73CE78565B377C9BE" alt="图片说明" title="图片标题"><br>图5. 基本流上的 Window 操作与 KeyedStream 上的 Window 操对比。KeyedStream 上的 Window 操作使采用多个实例并发处理成为了可能。<br>基本 DataStream 对象上的 allWindow 与 KeyedStream 上的 Window 操作的对比如图5所示。为了能够在多个并发实例上并行的对数据进行处理，我们需要通过 KeyBy 将数据进行分组。KeyBy 和 Window 操作都是对数据进行分组，但是 KeyBy 是在水平分向对流进行切分，而 Window 是在垂直方式对流进行切分。</p>
<p>使用 KeyBy 进行数据切分之后，后续算子的每一个实例可以只处理特定 Key 集合对应的数据。除了处理本身外，Flink 中允许算子维护一部分状态（State），在KeyedStream 算子的状态也是可以分布式存储的。由于 KeyBy 是一种确定的数据分配方式（下文将介绍其它分配方式），因此即使发生 Failover 作业重启，甚至发生了并发度的改变，Flink 都可以重新分配 Key 分组并保证处理某个 Key 的分组一定包含该 Key 的状态，从而保证一致性。</p>
<p>最后需要强调的是，KeyBy 操作只有当 Key 的数量超过算子的并发实例数才可以较好的工作。由于同一个 Key 对应的所有数据都会发送到同一个实例上，因此如果Key 的数量比实例数量少时，就会导致部分实例收不到数据，从而导致计算能力不能充分发挥。</p>
<h1 id="3-其它问题"><a href="#3-其它问题" class="headerlink" title="3. 其它问题"></a>3. 其它问题</h1><p>除 KeyBy 之外，Flink 在算子之前交换数据时还支持其它的物理分组方式。如图 1 所示，Flink DataStream 中物理分组方式包括：</p>
<p>Global: 上游算子将所有记录发送给下游算子的第一个实例。</p>
<p>Broadcast: 上游算子将每一条记录发送给下游算子的所有实例。</p>
<p>Forward：只适用于上游算子实例数与下游算子相同时，每个上游算子实例将记录发送给下游算子对应的实例。</p>
<p>Shuffle：上游算子对每条记录随机选择一个下游算子进行发送。</p>
<p>Rebalance：上游算子通过轮询的方式发送数据。</p>
<p>Rescale：当上游和下游算子的实例数为 n 或 m 时，如果 n &lt; m，则每个上游实例向ceil(m/n)或floor(m/n)个下游实例轮询发送数据；如果 n &gt; m，则 floor(n/m) 或 ceil(n/m) 个上游实例向下游实例轮询发送数据。</p>
<p>PartitionCustomer：当上述内置分配方式不满足需求时，用户还可以选择自定义分组方式。<br><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570538419242_C7E0BBE42E8516B40C29F7E7F9ABDCA5" alt="图片说明" title="图片标题"> </p>
<p>图6. 除keyBy外其它的物理分组方式<br>除分组方式外，Flink DataStream API 中另一个重要概念就是类型系统。图 7 所示，Flink DataStream 对像都是强类型的，每一个 DataStream 对象都需要指定元素的类型，Flink 自己底层的序列化机制正是依赖于这些信息对序列化等进行优化。具体来说，在 Flink 底层，它是使用 TypeInformation 对象对类型进行描述的，TypeInformation 对象定义了一组类型相关的信息供序列化框架使用。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570538432289_8C94F5A6149A0B5B45C667CC818EB879" alt="图片说明" title="图片标题"><br>图7. Flink DataStream API 中的类型系统<br>Flink 内置了一部分常用的基本类型，对于这些类型，Flink 也内置了它们的TypeInformation，用户一般可以直接使用而不需要额外的声明，Flink 自己可以通过类型推断机制识别出相应的类型。但是也会有一些例外的情况，比如，Flink DataStream API 同时支持 Java 和 Scala，Scala API 许多接口是通过隐式的参数来传递类型信息的，所以如果需要通过 Java 调用 Scala 的 API，则需要把这些类型信息通过隐式参数传递过去。另一个例子是 Java 中对泛型存在类型擦除，如果流的类型本身是一个泛型的话，则可能在擦除之后无法推断出类型信息，这时候也需要显式的指定。</p>
<p>在 Flink 中，一般 Java 接口采用 Tuple 类型来组合多个字段，而 Scala 则更经常使用 Row 类型或 Case Class。相对于 Row，Tuple 类型存在两个问题，一个是字段个数不能超过 25 个，此外，所有字段不允许有 null 值。最后，Flink 也支持用户自定义新的类型和 TypeInformation，并通过 Kryo 来实现序列化，但是这种方式可带来一些迁移等方面的问题，所以尽量不要使用自定义的类型。</p>
<h1 id="4-示例"><a href="#4-示例" class="headerlink" title="4. 示例"></a>4. 示例</h1><p>然后，我们再看一个更复杂的例子。假设我们有一个数据源，它监控系统中订单的情况，当有新订单时，它使用 Tuple2&lt;String, Integer&gt; 输出订单中商品的类型和交易额。然后，我们希望实时统计每个类别的交易额，以及实时统计全部类别的交易额。</p>
<p>▼ 示例4. 实时订单统计示例。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class GroupedProcessingTimeWindowSample &#123;</span><br><span class="line">    private static class DataSource extends RichParallelSourceFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123;</span><br><span class="line">        private volatile boolean isRunning &#x3D; true;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void run(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx) throws Exception &#123;</span><br><span class="line">            Random random &#x3D; new Random();</span><br><span class="line">            while (isRunning) &#123;</span><br><span class="line">                Thread.sleep((getRuntimeContext().getIndexOfThisSubtask() + 1) * 1000 * 5);</span><br><span class="line">                String key &#x3D; &quot;类别&quot; + (char) (&#39;A&#39; + random.nextInt(3));</span><br><span class="line">                int value &#x3D; random.nextInt(10) + 1;</span><br><span class="line"></span><br><span class="line">                System.out.println(String.format(&quot;Emits\t(%s, %d)&quot;, key, value));</span><br><span class="line">                ctx.collect(new Tuple2&lt;&gt;(key, value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void cancel() &#123;</span><br><span class="line">            isRunning &#x3D; false;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(2);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds &#x3D; env.addSource(new DataSource());</span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream &#x3D; ds.keyBy(0);</span><br><span class="line"></span><br><span class="line">        keyedStream.sum(1).keyBy(new KeySelector&lt;Tuple2&lt;String, Integer&gt;, Object&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Object getKey(Tuple2&lt;String, Integer&gt; stringIntegerTuple2) throws Exception &#123;</span><br><span class="line">                return &quot;&quot;;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).fold(new HashMap&lt;String, Integer&gt;(), new FoldFunction&lt;Tuple2&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public HashMap&lt;String, Integer&gt; fold(HashMap&lt;String, Integer&gt; accumulator, Tuple2&lt;String, Integer&gt; value) throws Exception &#123;</span><br><span class="line">                accumulator.put(value.f0, value.f1);</span><br><span class="line">                return accumulator;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).addSink(new SinkFunction&lt;HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void invoke(HashMap&lt;String, Integer&gt; value, Context context) throws Exception &#123;</span><br><span class="line">                  &#x2F;&#x2F; 每个类型的商品成交量</span><br><span class="line">                  System.out.println(value);</span><br><span class="line">                  &#x2F;&#x2F; 商品成交总量                </span><br><span class="line">                  System.out.println(value.values().stream().mapToInt(v -&gt; v).sum());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>示例的实现如表4所示。首先，在该实现中，我们首先实现了一个模拟的数据源，它继承自 RichParallelSourceFunction，它是可以有多个实例的 SourceFunction 的接口。它有两个方法需要实现，一个是 Run 方法，Flink 在运行时对 Source 会直接调用该方法，该方法需要不断的输出数据，从而形成初始的流。在 Run 方法的实现中，我们随机的产生商品类别和交易量的记录，然后通过 ctx#collect 方法进行发送。另一个方法是 Cancel 方法，当 Flink 需要 Cancel Source Task 的时候会调用该方法，我们使用一个 Volatile 类型的变量来标记和控制执行的状态。</p>
<p>然后，我们在 Main 方法中就可以开始图的构建。我们首先创建了一个 StreamExecutioniEnviroment 对象。创建对象调用的 getExecutionEnvironment 方法会自动判断所处的环境，从而创建合适的对象。例如，如果我们在 IDE 中直接右键运行，则会创建 LocalStreamExecutionEnvironment 对象；如果是在一个实际的环境中，则会创建 RemoteStreamExecutionEnvironment 对象。</p>
<p>基于 Environment 对象，我们首先创建了一个 Source，从而得到初始的&lt;商品类型，成交量&gt;流。然后，为了统计每种类别的成交量，我们使用 KeyBy 按 Tuple 的第 1 个字段（即商品类型）对输入流进行分组，并对每一个 Key 对应的记录的第 2 个字段（即成交量）进行求合。在底层，Sum 算子内部会使用 State 来维护每个Key（即商品类型）对应的成交量之和。当有新记录到达时，Sum 算子内部会更新所维护的成交量之和，并输出一条&lt;商品类型，更新后的成交量&gt;记录。</p>
<p>如果只统计各个类型的成交量，则程序可以到此为止，我们可以直接在 Sum 后添加一个 Sink 算子对不断更新的各类型成交量进行输出。但是，我们还需要统计所有类型的总成交量。为了做到这一点，我们需要将所有记录输出到同一个计算节点的实例上。我们可以通过 KeyBy 并且对所有记录返回同一个 Key，将所有记录分到同一个组中，从而可以全部发送到同一个实例上。</p>
<p>然后，我们使用 Fold 方法来在算子中维护每种类型商品的成交量。注意虽然目前 Fold 方法已经被标记为 Deprecated，但是在 DataStream API 中暂时还没有能替代它的其它操作，所以我们仍然使用 Fold 方法。这一方法接收一个初始值，然后当后续流中每条记录到达的时候，算子会调用所传递的 FoldFunction 对初始值进行更新，并发送更新后的值。我们使用一个 HashMap 来对各个类别的当前成交量进行维护，当有一条新的&lt;商品类别，成交量&gt;到达时，我们就更新该 HashMap。这样在 Sink 中，我们收到的是最新的商品类别和成交量的 HashMap，我们可以依赖这个值来输出各个商品的成交量和总的成交量。</p>
<p>需要指出的是，这个例子主要是用来演示 DataStream API 的用法，实际上还会有更高效的写法，此外，更上层的 Table / SQL 还支持 Retraction 机制，可以更好的处理这种情况。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191008/9094293_1570538470903_1A525E00557FBEAA217657B4AC618FB1" alt="图片说明" title="图片标题"><br>图8. API 原理图<br>最后，我们对 DataStream API 的原理进行简要的介绍。当我们调用 DataStream#map 算法时，Flink 在底层会创建一个 Transformation 对象，这一对象就代表我们计算逻辑图中的节点。它其中就记录了我们传入的 MapFunction，也就是 UDF（User Define Function）。随着我们调用更多的方法，我们创建了更多的 DataStream 对象，每个对象在内部都有一个 Transformation 对象，这些对象根据计算依赖关系组成一个图结构，就是我们的计算图。后续 Flink 将对这个图结构进行进一步的转换，从而最终生成提交作业所需要的 JobGraph。</p>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>本文主要介绍了 Flink DataStream API，它是当前 Flink 中比较底层的一套 API。在实际的开发中，基于该 API 需要用户自己处理 State 与 Time 等一些概念，因此需要较大的工作量。后续课程还会介绍更上层的 Table / SQL 层的 API，未来 Table / SQL 可能会成为 Flink 主流的 API，但是对于接口来说，越底层的接口表达能力越强，在一些需要精细操作的情况下，仍然需要依赖于 DataStream API。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Flink 简单的构建一个应用程序</title>
    <url>/2019/10/10/Apache-Flink-%E7%AE%80%E5%8D%95%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="简单的构建一个ApacheFlink的应用程序"><a href="#简单的构建一个ApacheFlink的应用程序" class="headerlink" title="简单的构建一个ApacheFlink的应用程序"></a>简单的构建一个ApacheFlink的应用程序</h1><h2 id="开发环境的准备："><a href="#开发环境的准备：" class="headerlink" title="开发环境的准备："></a>开发环境的准备：</h2><p>Flink 可以运行在 Linux, Max OS X, 或者是 Windows 上。这里我是在Windows上运行的。在本地机器上需要有Java8.x和maven环境，另外我们推荐使用 ItelliJ IDEA 作为 Flink 应用程序的开发 IDE。<br>首先在我们的pom.xml文件中添加Flink相关的依赖。</p>
<p>工作目录：<br><img src="https://uploadfiles.nowcoder.com/images/20191012/9094293_1570843930821_A4D48C34269A213CE6F6CE74BF95681C" alt="图片说明" title="图片标题"> </p>
<h1 id="编写Flink程序"><a href="#编写Flink程序" class="headerlink" title="编写Flink程序"></a>编写Flink程序</h1><p>创建 SocketWindowWordCount.java 文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package FlinkDemo;</span><br><span class="line">import org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">public class SocketWindowWordCount &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 创建 execution environment</span><br><span class="line">        StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 通过连接 socket 获取输入数据，这里连接到本地9000端口，如果9000端口已被占用，请换一个端口</span><br><span class="line">        DataStream&lt;String&gt; text &#x3D; env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 解析数据，按 word 分组，开窗，聚合</span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts &#x3D; text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123;</span><br><span class="line">                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, 1));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(0)</span><br><span class="line">                .timeWindow(Time.seconds(5))</span><br><span class="line">                .sum(1);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 将结果打印到控制台，注意这里使用的是单线程打印，而非多线程</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h1><p>要运行示例程序，首先我们在终端启动 netcat 获得输入流：<br>nc -lk 9000<br>然后直接运行SocketWindowWordCount的 main 方法。</p>
<p>只需要在 netcat 控制台输入单词，就能在 SocketWindowWordCount 的输出控制台看到每个单词的词频统计。如果想看到大于1的计数，请在5秒内反复键入相同的单词。<br>如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191012/9094293_1570862262668_C587F81A2896284000DDEC3E5471605F" alt="图片说明" title="图片标题"> </p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Flink概念</title>
    <url>/2019/10/05/Apache-Flink%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h1 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h1><p>作者：陈守元 &amp; 戴资力<a href="https://ververica.cn/developers/flink-basic-tutorial-1-basic-concept/" target="_blank" rel="noopener">https://ververica.cn/developers/flink-basic-tutorial-1-basic-concept/</a></p>
<h1 id="一、Apache-Flink-的定义、架构及原理"><a href="#一、Apache-Flink-的定义、架构及原理" class="headerlink" title="一、Apache Flink 的定义、架构及原理"></a>一、Apache Flink 的定义、架构及原理</h1><p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态或无状态的计算，能够部署在各种集群环境，对各种规模大小的数据进行快速计算。</p>
<h2 id="1-Flink-Application"><a href="#1-Flink-Application" class="headerlink" title="1. Flink Application"></a>1. Flink Application</h2><p>了解Flink 应用开发需要先理解Flink 的Streams、State、Time 等基础处理语义以及Flink 兼顾灵活性和方便性的多层次API。</p>
<p>Streams：流，分为有限数据流与无限数据流，unbounded stream 是有始无终的数据流，即无限数据流；而bounded stream 是限定大小的有始有终的数据集合，即有限数据流，二者的区别在于无限数据流的数据会随时间的推演而持续增加，计算持续进行且不存在结束的状态，相对的有限数据流数据大小固定，计算最终会完成并处于结束的状态。</p>
<p>State，状态是计算过程中的数据信息，在容错恢复和Checkpoint 中有重要的作用，流计算在本质上是Incremental Processing，因此需要不断查询保持状态；另外，为了确保Exactly- once 语义，需要数据能够写入到状态中；而持久化存储，能够保证在整个分布式系统运行失败或者挂掉的情况下做到Exactly- once，这是状态的另外一个价值。</p>
<p>Time，分为Event time、Ingestion time、Processing time，Flink 的无限数据流是一个持续的过程，时间是我们判断业务状态是否滞后，数据处理是否及时的重要依据。</p>
<p>API，API 通常分为三层，由上而下可分为SQL / Table API、DataStream API、ProcessFunction 三层，API 的表达能力及业务抽象能力都非常强大，但越接近SQL 层，表达能力会逐步减弱，抽象能力会增强，反之，ProcessFunction 层API 的表达能力非常强，可以进行多种灵活方便的操作，但抽象能力也相对越小。</p>
<h2 id="2-Flink-Architecture"><a href="#2-Flink-Architecture" class="headerlink" title="2. Flink Architecture"></a>2. Flink Architecture</h2><p>在架构部分，主要分为以下四点：<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276646050_A977E198D8048EFD6DC1F6636BFE9453" alt="图片说明" title="图片标题"><br>第一，Flink 具备统一的框架处理有界和无界两种数据流的能力</p>
<p>第二， 部署灵活，Flink 底层支持多种资源调度器，包括Yarn、Kubernetes 等。Flink 自身带的Standalone 的调度器，在部署上也十分灵活。</p>
<p>第三， 极高的可伸缩性，可伸缩性对于分布式系统十分重要，阿里巴巴双11大屏采用Flink 处理海量数据，使用过程中测得Flink 峰值可达17 亿/秒。</p>
<p>第四， 极致的流式处理性能。Flink 相对于Storm 最大的特点是将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络IO，可以极大提升状态存取的性能。</p>
<h2 id="3-Flink-Operation"><a href="#3-Flink-Operation" class="headerlink" title="3. Flink Operation"></a>3. Flink Operation</h2><p>后面会有专门课程讲解，此处简单分享Flink 关于运维及业务监控的内容：</p>
<p>Flink具备7 X 24 小时高可用的SOA（面向服务架构），原因是在实现上Flink 提供了一致性的Checkpoint。Checkpoint是Flink 实现容错机制的核心，它周期性的记录计算过程中Operator 的状态，并生成快照持久化存储。当Flink 作业发生故障崩溃时，可以有选择的从Checkpoint 中恢复，保证了计算的一致性。</p>
<p>Flink本身提供监控、运维等功能或接口，并有内置的WebUI，对运行的作业提供DAG 图以及各种Metric 等，协助用户管理作业状态。</p>
<h2 id="4-Flink-的应用场景"><a href="#4-Flink-的应用场景" class="headerlink" title="4. Flink 的应用场景"></a>4. Flink 的应用场景</h2><h2 id="4-1-Flink-的应用场景：Data-Pipeline"><a href="#4-1-Flink-的应用场景：Data-Pipeline" class="headerlink" title="4.1 Flink 的应用场景：Data Pipeline"></a>4.1 Flink 的应用场景：Data Pipeline</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276745180_8D7749B09365031340991DD2E127A69D" alt="图片说明" title="图片标题"><br>Data Pipeline 的核心场景类似于数据搬运并在搬运的过程中进行部分数据清洗或者处理，而整个业务架构图的左边是Periodic ETL，它提供了流式ETL 或者实时ETL，能够订阅消息队列的消息并进行处理，清洗完成后实时写入到下游的Database或File system 中。场景举例：</p>
<p>实时数仓<br>当下游要构建实时数仓时，上游则可能需要实时的Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数仓的整个链路中，可保证数据查询的时效性，形成实时数据采集、实时数据处理以及下游的实时Query。</p>
<p>搜索引擎推荐<br>搜索引擎这块以淘宝为例，当卖家上线新商品时，后台会实时产生消息流，该消息流经过Flink 系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样当淘宝卖家上线新商品时，能在秒级或者分钟级实现搜索引擎的搜索。</p>
<h2 id="4-2-Flink-应用场景：Data-Analytics"><a href="#4-2-Flink-应用场景：Data-Analytics" class="headerlink" title="4.2 Flink 应用场景：Data Analytics"></a>4.2 Flink 应用场景：Data Analytics</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276787069_136FDC2F733378161152115CD1F9140C" alt="图片说明" title="图片标题"><br>Data Analytics，如图，左边是Batch Analytics，右边是Streaming Analytics。Batch Analytics 就是传统意义上使用类似于Map Reduce、Hive、Spark Batch 等，对作业进行分析、处理、生成离线报表；Streaming Analytics 使用流式分析引擎如Storm、Flink 实时处理分析数据，应用较多的场景如实时大屏、实时报表。</p>
<h2 id="4-3-Flink-应用场景：Data-Driven"><a href="#4-3-Flink-应用场景：Data-Driven" class="headerlink" title="4.3 Flink 应用场景：Data Driven"></a>4.3 Flink 应用场景：Data Driven</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276813600_B7803A0512D98F04701816656E67B8BD" alt="图片说明" title="图片标题"><br>从某种程度上来说，所有的实时的数据处理或者是流式数据处理都是属于Data Driven，流计算本质上是Data Driven 计算。应用较多的如风控系统，当风控系统需要处理各种各样复杂的规则时，Data Driven 就会把处理的规则和逻辑写入到Datastream 的API 或者是ProcessFunction 的API 中，然后将逻辑抽象到整个Flink 引擎，当外面的数据流或者是事件进入就会触发相应的规则，这就是Data Driven 的原理。在触发某些规则后，Data Driven 会进行处理或者是进行预警，这些预警会发到下游产生业务通知，这是Data Driven 的应用场景，Data Driven 在应用上更多应用于复杂事件的处理。</p>
<h1 id="二、「有状态的流式处理」概念解析"><a href="#二、「有状态的流式处理」概念解析" class="headerlink" title="二、「有状态的流式处理」概念解析"></a>二、「有状态的流式处理」概念解析</h1><h2 id="1-传统批处理"><a href="#1-传统批处理" class="headerlink" title="1. 传统批处理"></a>1. 传统批处理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276859239_6A1AC20A97B0A786629AFFF3B8EBE515" alt="图片说明" title="图片标题"><br>传统批处理方法是持续收取数据，以时间作为划分多个批次的依据，再周期性地执行批次运算。但假设需要计算每小时出现事件转换的次数，如果事件转换跨越了所定义的时间划分，传统批处理会将中介运算结果带到下一个批次进行计算；除此之外，当出现接收到的事件顺序颠倒情况下，传统批处理仍会将中介状态带到下一批次的运算结果中，这种处理方式也不尽如人意。</p>
<h2 id="2-理想方法"><a href="#2-理想方法" class="headerlink" title="2. 理想方法"></a>2. 理想方法</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276897730_A901CD03516649C53E785F0D1B0FC2B7" alt="图片说明" title="图片标题"><br>第一点，要有理想方法，这个理想方法是引擎必须要有能力可以累积状态和维护状态，累积状态代表着过去历史中接收过的所有事件，会影响到输出。</p>
<p>第二点，时间，时间意味着引擎对于数据完整性有机制可以操控，当所有数据都完全接收到后，输出计算结果。</p>
<p>第三点，理想方法模型需要实时产生结果，但更重要的是采用新的持续性数据处理模型来处理实时数据，这样才最符合Continuous data 的特性。</p>
<h2 id="3-流式处理"><a href="#3-流式处理" class="headerlink" title="3.流式处理"></a>3.流式处理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276921632_0C5C777CEE1105EFB2D1403DA0D58C20" alt="图片说明" title="图片标题"><br>流式处理简单来讲即有一个无穷无尽的数据源在持续收取数据，以代码作为数据处理的基础逻辑，数据源的数据经过代码处理后产生出结果，然后输出，这就是流式处理的基本原理。</p>
<h2 id="4-分布式流式处理"><a href="#4-分布式流式处理" class="headerlink" title="4.分布式流式处理"></a>4.分布式流式处理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276957801_36789D1AAE3E0699128254C8F4844110" alt="图片说明" title="图片标题"><br>假设Input Streams 有很多个使用者，每个使用者都有自己的ID，如果计算每个使用者出现的次数，我们需要让同一个使用者的出现事件流到同一运算代码，这跟其他批次需要做Group by 是同样的概念，所以跟Stream 一样需要做分区，设定相应的Key，然后让同样的 Key 流到同一个 Computation instance 做同样的运算。</p>
<h2 id="5-有状态分布式流式处理"><a href="#5-有状态分布式流式处理" class="headerlink" title="5. 有状态分布式流式处理"></a>5. 有状态分布式流式处理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570276988699_BD17D461E0C4514D6362646AB611A327" alt="图片说明" title="图片标题"><br>如图，上述代码中定义了变数X，X 在数据处理过程中会进行读和写，在最后输出结果时，可以依据变数X 决定输出的内容，即状态X 会影响最终的输出结果。这个过程中，第一个重点是先进行了状态Co-partitioned key by，同样的 Key 都会流到Computation instance，与使用者出现次数的原理相同，次数即所谓的状态，这个状态一定会跟同一个Key 的事件累积在同一个 Computation instance。类似于根据输入流的Key 重新分区的状态，当分区进入 Stream 之后，这个 Stream 会累积起来的状态也变成 Copartiton 。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277013707_B97FF832FFD370F6FCF7BECE3AB95233" alt="图片说明" title="图片标题"><br>第二个重点是embeded local state backend。有状态分散式流式处理的引擎，状态可能会累积到非常大，当 Key 非常多时，状态可能就会超出单一节点的 Memory 的负荷量，这时候状态必须有状态后端去维护它；在这个状态后端在正常状况下，用In-memory 维护即可。</p>
<h1 id="三、Apache-Flink-的优势"><a href="#三、Apache-Flink-的优势" class="headerlink" title="三、Apache Flink 的优势"></a>三、Apache Flink 的优势</h1><h2 id="1-状态容错"><a href="#1-状态容错" class="headerlink" title="1.状态容错"></a>1.状态容错</h2><p>当我们考虑状态容错时难免会想到精确一次的状态容错，应用在运算时累积的状态，每笔输入的事件反映到状态，更改状态都是精确一次，如果修改超过一次的话也意味着数据引擎产生的结果是不可靠的。</p>
<p>如何确保状态拥有精确一次（Exactly-once guarantee）的容错保证？</p>
<p>如何在分散式场景下替多个拥有本地状态的运算子产生一个全域一致的快照（Global consistent snapshot）？</p>
<p>更重要的是，如何在不中断运算的前提下产生快照？</p>
<h2 id="1-1-简单场景的精确一次容错方法"><a href="#1-1-简单场景的精确一次容错方法" class="headerlink" title="1.1 简单场景的精确一次容错方法"></a>1.1 简单场景的精确一次容错方法</h2><p>还是以使用者出现次数来看，如果某个使用者出现的次数计算不准确，不是精确一次，那么产生的结果是无法作为参考的。在考虑精确的容错保证前，我们先考虑最简单的使用场景，如无限流的数据进入，后面单一的Process 进行运算，每处理完一笔计算即会累积一次状态，这种情况下如果要确保Process 产生精确一次的状态容错，每处理完一笔数据，更改完状态后进行一次快照，快照包含在队列中并与相应的状态进行对比，完成一致的快照，就能确保精确一次。</p>
<h2 id="1-2-分布式状态容错"><a href="#1-2-分布式状态容错" class="headerlink" title="1.2 分布式状态容错"></a>1.2 分布式状态容错</h2><p>Flink作为分布式的处理引擎，在分布式的场景下，进行多个本地状态的运算，只产生一个全域一致的快照，如需要在不中断运算值的前提下产生全域一致的快照，就涉及到分散式状态容错。</p>
<p>Global consistent snapshot<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277051016_E86271345C7D773AE4D952CCA23C5097" alt="图片说明" title="图片标题"><br>关于Global consistent snapshot，当Operator 在分布式的环境中，在各个节点做运算，首先产生Global consistent snapshot 的方式就是处理每一笔数据的快照点是连续的，这笔运算流过所有的运算值，更改完所有的运算值后，能够看到每一个运算值的状态与该笔运算的位置，即可称为Consistent snapshot，当然，Global consistent snapshot 也是简易场景的延伸。</p>
<p>容错恢复</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277083117_FD70E2F6BA35FC84C0C08B008FB0F040" alt="图片说明" title="图片标题"> </p>
<p>首先了解一下Checkpoint，上面提到连续性快照每个Operator 运算值本地的状态后端都要维护状态，也就是每次将产生检查点时会将它们传入共享的DFS 中。当任何一个Process 挂掉后，可以直接从三个完整的Checkpoint 将所有的运算值的状态恢复，重新设定到相应位置。Checkpoint的存在使整个Process 能够实现分散式环境中的Exactly-once。</p>
<h2 id="1-3-分散式快照（Distributed-Snapshots）方法"><a href="#1-3-分散式快照（Distributed-Snapshots）方法" class="headerlink" title="1.3 分散式快照（Distributed Snapshots）方法"></a>1.3 分散式快照（Distributed Snapshots）方法</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277108380_1973D803C8683A5F7DA57AE4C17ADD12" alt="图片说明" title="图片标题"><br>关于Flink 如何在不中断运算的状况下持续产生Global consistent snapshot，其方式是基于用 Simple lamport 演算法机制下延伸的。已知的一个点Checkpoint barrier，Flink 在某个Datastream 中会一直安插Checkpoint barrier，Checkpoint barrier 也会N – 1等等，Checkpoint barrier N 代表着所有在这个范围里面的数据都是Checkpoint barrier N。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277133562_65F9BD7806B8D46B1F0D721AEA6A1F3B" alt="图片说明" title="图片标题"><br>举例：假设现在需要产生Checkpoint barrier N，但实际上在Flink 中是由Job manager 触发Checkpoint，Checkpoint 被触发后开始从数据源产生Checkpoint barrier。当Job 开始做Checkpoint barrier N 的时候，可以理解为Checkpoint barrier N 需要逐步填充左下角的表格。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277167888_F0E968DD5BED4B3DB4B20946EECA3973" alt="图片说明" title="图片标题"><br>如图，当部分事件标为红色，Checkpoint barrier N 也是红色时，代表着这些数据或事件都由Checkpoint barrier N 负责。Checkpoint barrier N 后面白色部分的数据或事件则不属于Checkpoint barrier N。</p>
<p>在以上的基础上，当数据源收到Checkpoint barrier N 之后会先将自己的状态保存，以读取Kafka资料为例，数据源的状态就是目前它在Kafka 分区的位置，这个状态也会写入到上面提到的表格中。下游的Operator 1 会开始运算属于Checkpoint barrier N 的数据，当Checkpoint barrier N 跟着这些数据流动到Operator 1 之后,Operator 1 也将属于Checkpoint barrier N 的所有数据都反映在状态中，当收到Checkpoint barrier N 时也会直接对Checkpoint去做快照。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277198821_DA42B62744D7181CD6E2EB2785E54AA6" alt="图片说明" title="图片标题"> </p>
<p>当快照完成后继续往下游走，Operator 2 也会接收到所有数据，然后搜索Checkpoint barrier N 的数据并直接反映到状态，当状态收到Checkpoint barrier N 之后也会直接写入到Checkpoint N 中。以上过程到此可以看到Checkpoint barrier N 已经完成了一个完整的表格，这个表格叫做Distributed Snapshots，即分布式快照。分布式快照可以用来做状态容错，任何一个节点挂掉的时候可以在之前的Checkpoint 中将其恢复。继续以上Process，当多个Checkpoint 同时进行，Checkpoint barrier N 已经流到Job manager 2，Flink job manager 可以触发其他的Checkpoint，比如Checkpoint N + 1，Checkpoint N + 2 等等也同步进行，利用这种机制，可以在不阻挡运算的状况下持续地产生Checkpoint。</p>
<h2 id="2-状态维护"><a href="#2-状态维护" class="headerlink" title="2. 状态维护"></a>2. 状态维护</h2><p>状态维护即用一段代码在本地维护状态值，当状态值非常大时需要本地的状态后端来支持。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277236063_ACAF4389211CBB001A75DBDCC3FD8DC3" alt="图片说明" title="图片标题"><br>如图，在Flink 程序中，可以采用getRuntimeContext().getState(desc); 这组API 去注册状态。Flink 有多种状态后端，采用API 注册状态后，读取状态时都是通过状态后端来读取的。Flink 有两种不同的状态值，也有两种不同的状态后端：<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277259395_13B121DEE1F8C3DB4A0EC3EAFA480A97" alt="图片说明" title="图片标题"><br>JVM Heap状态后端，适合数量较小的状态，当状态量不大时就可以采用JVM Heap 的状态后端。JVM Heap 状态后端会在每一次运算值需要读取状态时，用Java object read / writes 进行读或写，不会产生较大代价，但当Checkpoint 需要将每一个运算值的本地状态放入Distributed Snapshots 的时候，就需要进行序列化了。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277284906_0E1ED0DC5B85939AFC377F6DB8FBE492" alt="图片说明" title="图片标题"><br>RocksDB状态后端，它是一种out of core 的状态后端。在Runtime 的本地状态后端让使用者去读取状态的时候会经过磁盘，相当于将状态维护在磁盘里，与之对应的代价可能就是每次读取状态时，都需要经过序列化和反序列化的过程。当需要进行快照时只将应用序列化即可，序列化后的数据直接传输到中央的共享DFS 中。</p>
<p>Flink目前支持以上两种状态后端，一种是纯 Memory 的状态后端，另一种是有资源磁盘的状态后端，在维护状态时可以根据状态的数量选择相应的状态后端。</p>
<h1 id="3-Event-–-Time"><a href="#3-Event-–-Time" class="headerlink" title="3. Event – Time"></a>3. Event – Time</h1><h2 id="3-1-不同时间种类"><a href="#3-1-不同时间种类" class="headerlink" title="3.1 不同时间种类"></a>3.1 不同时间种类</h2><p>在Flink 及其他进阶的流式处理引擎出现之前，大数据处理引擎一直只支持Processing-time 的处理。假设定义一个运算 Windows 的窗口，Windows 运算设定每小时进行结算。Processing-time 进行运算时，可以发现数据引擎将3 点至4 点间收到的数据进行结算。实际上在做报表或者分析结果时是想了解真实世界中3 点至4 点之间实际产生数据的输出结果，了解实际数据的输出结果就必须采用Event – Time 了。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277314899_C86B0E1CC29E09723ADD59312CE14E9E" alt="图片说明" title="图片标题"><br>如图，Event – Time 相当于事件，它在数据最源头产生时带有时间戳，后面都需要用时间戳来进行运算。用图来表示，最开始的队列收到数据，每小时对数据划分一个批次，这就是Event – Time Process 在做的事情。</p>
<h2 id="3-2-Event-–-Time-处理"><a href="#3-2-Event-–-Time-处理" class="headerlink" title="3.2 Event – Time 处理"></a>3.2 Event – Time 处理</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277337150_4A55B468AA19FDB340A1F54FE65E7B28" alt="图片说明" title="图片标题"><br>Event – Time 是用事件真实产生的时间戳去做Re-bucketing，把对应时间3 点到4 点的数据放在3 点到4 点的Bucket，然后Bucket 产生结果。所以Event – Time 跟Processing – time 的概念是这样对比的存在。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277366521_BE05EF9EB19A0CB3718F632FC6912425" alt="图片说明" title="图片标题"><br>Event – Time 的重要性在于记录引擎输出运算结果的时间。简单来说，流式引擎连续24 小时在运行、搜集资料，假设Pipeline 里有一个 Windows Operator 正在做运算，每小时能产生结果，何时输出 Windows的运算值，这个时间点就是Event – Time 处理的精髓，用来表示该收的数据已经收到。</p>
<h2 id="3-3-Watermarks"><a href="#3-3-Watermarks" class="headerlink" title="3.3 Watermarks"></a>3.3 Watermarks</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277387632_B8E06B3E6F548DB536E4C97D9EF99105" alt="图片说明" title="图片标题"><br>Flink实际上是用 Watermarks 来实现Event – Time 的功能。Watermarks 在Flink 中也属于特殊事件，其精髓在于当某个运算值收到带有时间戳“ T ”的 Watermarks 时就意味着它不会接收到新的数据了。使用Watermarks 的好处在于可以准确预估收到数据的截止时间。举例，假设预期收到数据时间与输出结果时间的时间差延迟5 分钟，那么Flink 中所有的 Windows Operator 搜索3 点至4 点的数据，但因为存在延迟需要再多等5分钟直至收集完4：05 分的数据，此时方能判定4 点钟的资料收集完成了，然后才会产出3 点至4 点的数据结果。这个时间段的结果对应的就是 Watermarks 的部分。</p>
<h2 id="4-状态保存与迁移"><a href="#4-状态保存与迁移" class="headerlink" title="4. 状态保存与迁移"></a>4. 状态保存与迁移</h2><p>流式处理应用无时无刻不在运行，运维上有几个重要考量：</p>
<p>更改应用逻辑/修bug 等，如何将前一执行的状态迁移到新的执行？<br>如何重新定义运行的平行化程度？<br>如何升级运算丛集的版本号？</p>
<p>Checkpoint完美符合以上需求，不过Flink 中还有另外一个名词保存点（Savepoint），当手动产生一个Checkpoint 的时候，就叫做一个Savepoint。Savepoint 跟Checkpoint 的差别在于Checkpoint是Flink 对于一个有状态应用在运行中利用分布式快照持续周期性的产生Checkpoint，而Savepoint 则是手动产生的Checkpoint，Savepoint 记录着流式应用中所有运算元的状态。<br><img src="https://uploadfiles.nowcoder.com/images/20191005/9094293_1570277415823_4638D1CF6191C1511AAFD86DB0A6CA2B" alt="图片说明" title="图片标题"><br>如图，Savepoint A 和Savepoint B，无论是变更底层代码逻辑、修bug 或是升级Flink 版本，重新定义应用、计算的平行化程度等，最先需要做的事情就是产生Savepoint。</p>
<p>Savepoint产生的原理是在Checkpoint barrier 流动到所有的Pipeline 中手动插入从而产生分布式快照，这些分布式快照点即Savepoint。Savepoint 可以放在任何位置保存，当完成变更时，可以直接从Savepoint 恢复、执行。</p>
<p>从Savepoint 的恢复执行需要注意，在变更应用的过程中时间在持续，如Kafka 在持续收集资料，当从Savepoint 恢复时，Savepoint 保存着Checkpoint 产生的时间以及Kafka 的相应位置，因此它需要恢复到最新的数据。无论是任何运算，Event – Time 都可以确保产生的结果完全一致。</p>
<p>假设恢复后的重新运算用Process Event – Time，将 Windows 窗口设为1 小时，重新运算能够在10 分钟内将所有的运算结果都包含到单一的 Windows 中。而如果使用Event – Time，则类似于做Bucketing。在Bucketing 的状况下，无论重新运算的数量多大，最终重新运算的时间以及Windows 产生的结果都一定能保证完全一致。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink DataStream API编程</title>
    <url>/2020/06/21/Flink-DataStream-API%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="流处理基本概念"><a href="#流处理基本概念" class="headerlink" title="流处理基本概念"></a>流处理基本概念</h1><p>对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是对立统一的，它们的关系有点类似于对于 Java 中的 ArrayList 中的元素，是直接看作一个有限数据集并用下标去访问，还是用迭代器去访问。</p>
<p>流处理系统本身有很多自己的特点。一般来说，由于需要支持无限数据集的处理，流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中</p>
<p><img src="/2020/06/21/Flink-DataStream-API%E7%BC%96%E7%A8%8B/1.png" alt></p>
<center>图2. 一个 DAG 计算逻辑图与实际的物理时模型。<br>逻辑图中的每个算子在物理图中可能有多个并发。</center>

<p>对于实际的分布式流处理引擎，它们的实际运行时物理模型要更复杂一些，这是由于每个算子都可能有多个实例。如图 2 所示，作为 Source 的 A 算子有两个实例，中间算子 C 也有两个实例。在逻辑模型中，A 和 B 是 C 的上游节点，而在对应的物理逻辑中，C 的所有实例和 A、B 的所有实例之间可能都存在数据交换。在物理模型中，我们会根据计算逻辑，采用系统自动优化或人为指定的方式将计算工作分布到不同的实例中。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。</p>
<h1 id="DataStream编程概述"><a href="#DataStream编程概述" class="headerlink" title="DataStream编程概述"></a>DataStream编程概述</h1><p>Flink中的DataStream程序是常规程序，可对数据流实施转换（例如，过滤，更新状态，定义窗口，聚合）。最初从各种来源（例如，消息队列，套接字流，文件）创建数据流。结果通过接收器返回，接收器可以例如将数据写入文件或标准输出（例如命令行终端）。Flink程序可在各种上下文中运行，独立运行或嵌入其他程序中。执行可以在本地JVM或许多计算机的群集中进行。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/16</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:JavaDataStreamSourceApp</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaDataStreamSourceApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//socketFunction(env);</span></span><br><span class="line">        DataStreamSource&lt;String&gt;data = env.socketTextStream(<span class="string">"local"</span>,<span class="number">9999</span>);</span><br><span class="line">        DataStream&lt;word&gt; result= data.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, word&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;word&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span>(String value:s.split(<span class="string">","</span>)) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> word(value,<span class="number">1L</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="string">"name"</span>).timeWindow(Time.seconds(<span class="number">5</span>),Time.seconds(<span class="number">1</span>)).reduce(<span class="keyword">new</span> ReduceFunction&lt;word&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> word <span class="title">reduce</span><span class="params">(word word, word t1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> word(word.name,word.count+t1.count);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        result.print();</span><br><span class="line">        env.execute(<span class="string">"JavaDataStreamSourceApp"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span>  <span class="class"><span class="keyword">class</span> <span class="title">word</span></span>&#123;</span><br><span class="line">            <span class="keyword">private</span> String name;</span><br><span class="line">            <span class="keyword">private</span> Long count;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> name;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">word</span><span class="params">()</span></span>&#123;&#125; <span class="comment">//需要不包含参数的空参构造器</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">word</span><span class="params">(String name, Long count)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.name = name;</span><br><span class="line">            <span class="keyword">this</span>.count = count;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.name = name;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> Long <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> count;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"word&#123;"</span> +</span><br><span class="line">                    <span class="string">"name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                    <span class="string">", count="</span> + count +</span><br><span class="line">                    <span class="string">'&#125;'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCount</span><span class="params">(Long count)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.count = count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line">  * @Author:落墨</span><br><span class="line">  * @CreateTime:2020&#x2F;6&#x2F;16</span><br><span class="line">  * @Description:DataStreamSourceApp</span><br><span class="line">  *&#x2F;</span><br><span class="line">object Wordcount &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val data &#x3D; env.socketTextStream(&quot;localhost&quot;,9999)</span><br><span class="line">    import  org.apache.flink.api.scala._</span><br><span class="line">    data.flatMap(_.split(&quot;,&quot;)).map(x &#x3D;&gt;(x,1)).keyBy(0)</span><br><span class="line">      .timeWindow(Time.seconds(5),Time.seconds(1))</span><br><span class="line">      .sum(1)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(&quot;WordCount&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了实现流式 Word Count，我们首先要先获得一个 StreamExecutionEnvironment 对象。它是我们构建图过程中的上下文对象。基于这个对象，我们可以添加一些算子。对于流处理程度，我们一般需要首先创建一个数据源去接入数据。在这个例子中，我们使用了 Environment 对象中内置的读取文件的数据源。这一步之后，我们拿到的是一个 DataStream 对象，它可以看作一个无限的数据集，可以在该集合上进行一序列的操作。例如，在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。</p>
<p>最后，我们需要调用 env.execute 方法来开始程序的执行。需要强调的是，前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。</p>
<p>基于流式 Word Count 的例子可以看出，基于 Flink 的 DataStream API 来编写流处理程序一般需要三步：通过 Source 接入数据、进行一系统列的处理以及将数据写出。最后，不要忘记显式调用 Execute 方式，否则前面编写的逻辑并不会真正执行。</p>
<h2 id="Flink中使用数据源"><a href="#Flink中使用数据源" class="headerlink" title="Flink中使用数据源"></a>Flink中使用数据源</h2><p>source是程序从中读取其输入的位置。我们可以使用StreamExecutionEnvironment.addSource(sourceFunction)将一个source附加到程序中。Flink提供了许多预先实现的sourceFunction，但是我们可以通过实现SourceFunction 非并行源，实现ParallelSourceFunction接口或扩展RichParallelSourceFunction 并行源来编写自己的自定义源。</p>
<h3 id="基于文件："><a href="#基于文件：" class="headerlink" title="基于文件："></a>基于文件：</h3><ul>
<li><p><code>readTextFile(path)</code>- <code>TextInputFormat</code>逐行读取文本文件，即符合规范的文件，并将其作为字符串返回。</p>
</li>
<li><p><code>readFile(fileInputFormat, path)</code> -根据指定的文件输入格式读取（一次）文件。</p>
</li>
<li><p><code>readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)</code>-这是前两个内部调用的方法。它<code>path</code>根据给定的读取文件<code>fileInputFormat</code>。根据提供的内容<code>watchType</code>，此源可以定期（每<code>interval</code>ms）监视路径中的新数据（<code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>），或者处理一次当前路径中的数据并退出（<code>FileProcessingMode.PROCESS_ONCE</code>）。使用<code>pathFilter</code>，用户可以进一步从文件中排除文件。       </p>
</li>
</ul>
<h3 id="基于套接字："><a href="#基于套接字：" class="headerlink" title="基于套接字："></a>基于套接字：</h3><ul>
<li><code>socketTextStream</code>-从套接字读取。元素可以由定界符分隔。</li>
</ul>
<h3 id="基于集合："><a href="#基于集合：" class="headerlink" title="基于集合："></a>基于集合：</h3><ul>
<li><p>fromCollection(Collection)-从Java Java.util.Collection创建数据流。集合中的所有元素必须具有相同的类型。</p>
</li>
<li><p>fromCollection(Iterator, Class)-从迭代器创建数据流。该类指定迭代器返回的元素的数据类型。</p>
</li>
<li><p>fromElements(T …)-从给定的对象序列创建数据流。所有对象必须具有相同的类型。</p>
</li>
<li><p>fromParallelCollection(SplittableIterator, Class)-从迭代器并行创建数据流。该类指定迭代器返回的元素的数据类型。</p>
</li>
<li><p>generateSequence(from, to) -并行生成给定间隔中的数字序列。</p>
</li>
</ul>
<h3 id="基于连接器："><a href="#基于连接器：" class="headerlink" title="基于连接器："></a>基于连接器：</h3><ul>
<li>addSource-附加新的源功能。例如，要阅读Apache Kafka，可以使用 addSource(new FlinkKafkaConsumer08&lt;&gt;(…))。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">implementing the SourceFunction <span class="keyword">for</span> non-parallel sources</span><br><span class="line">implementing the ParallelSourceFunction <span class="class"><span class="keyword">interface</span></span></span><br><span class="line"><span class="class"><span class="title">extending</span> <span class="title">the</span> <span class="title">RichParallelSourceFunction</span> <span class="title">for</span> <span class="title">parallel</span> <span class="title">sources</span>.</span></span><br></pre></td></tr></table></figure>
<h3 id="自定义-SourceFunction"><a href="#自定义-SourceFunction" class="headerlink" title="自定义 SourceFunction"></a>自定义 SourceFunction</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.SourceFunction</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:SourceFunction</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">class CustomNonParallelSourceFunction extends SourceFunction[Long]&#123;</span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">1L</span></span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="keyword">true</span></span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">(sourceContext: SourceFunction.SourceContext[Long])</span>: Unit </span>= &#123;</span><br><span class="line">   <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">     sourceContext.collect(count)</span><br><span class="line">     count += <span class="number">1</span></span><br><span class="line">     Thread.sleep(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">override def <span class="title">cancel</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    isRunning = <span class="keyword">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="自定义ParallelSourceFunction"><a href="#自定义ParallelSourceFunction" class="headerlink" title="自定义ParallelSourceFunction"></a>自定义ParallelSourceFunction</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;ParallelSourceFunction, SourceFunction&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:ParallelSourceFunction</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">class CustomParallelSourceFunction extends  ParallelSourceFunction[Long]&#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="keyword">true</span></span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">1L</span></span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">(sourceContext: SourceFunction.SourceContext[Long])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">      sourceContext.collect(count)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">      Thread.sleep(<span class="number">10000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">override def <span class="title">cancel</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    isRunning = <span class="keyword">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="自定义-RichParallelSourceFunction"><a href="#自定义-RichParallelSourceFunction" class="headerlink" title="自定义 RichParallelSourceFunction"></a>自定义 RichParallelSourceFunction</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;RichParallelSourceFunction, SourceFunction&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:RichParallelSourceFunction</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomRichParallelSourceFunction</span> <span class="keyword">extends</span></span></span><br><span class="line">  RichParallelSourceFunction[Long] &#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="keyword">true</span></span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">1L</span></span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">(sourceContext: SourceFunction.SourceContext[Long])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">      sourceContext.collect(count)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line">      Thread.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">override def <span class="title">cancel</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    isRunning = <span class="keyword">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="使用自定义数据源"><a href="#使用自定义数据源" class="headerlink" title="使用自定义数据源"></a>使用自定义数据源</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> DataStream</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/24</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:使用自定义数据源</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object DataStreamSourceApp &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//socketFunction(env)</span></span><br><span class="line">    <span class="comment">//nonParallelSourceFunction(env)</span></span><br><span class="line">    <span class="comment">//ParallelSourceFunction(env)</span></span><br><span class="line">    RichParallelSourceFunction(env)</span><br><span class="line">    env.execute(<span class="string">"DataStreamSourceApp"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">socketFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">      val data =  environment.socketTextStream(<span class="string">"localhost"</span>,<span class="number">9999</span>)</span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">nonParallelSourceFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">   val data = environment.addSource(<span class="keyword">new</span> CustomNonParallelSourceFunction).setParallelism(<span class="number">1</span>)<span class="comment">//因为实现的不是并行的SourceFunction，所以参数不能设置为2</span></span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">ParallelSourceFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">    val data = environment.addSource(<span class="keyword">new</span> CustomParallelSourceFunction).setParallelism(<span class="number">1</span>)<span class="comment">//因为实现的是并行的sourceFunction所以可以设置大于1的并行参数</span></span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">RichParallelSourceFunction</span><span class="params">(environment: StreamExecutionEnvironment)</span>: Unit </span>=&#123;</span><br><span class="line">    val data = environment.addSource(<span class="keyword">new</span> CustomRichParallelSourceFunction).setParallelism(<span class="number">1</span>)<span class="comment">//因为实现的是并行的sourceFunction所以可以设置大于1的并行参数</span></span><br><span class="line">    data.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>DataStream API</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Spark简单构建一个应用程序</title>
    <url>/2019/10/10/Apache-Spark%E7%AE%80%E5%8D%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="简单的构建一个Apache-Spark应用程序"><a href="#简单的构建一个Apache-Spark应用程序" class="headerlink" title="简单的构建一个Apache Spark应用程序"></a>简单的构建一个Apache Spark应用程序</h1><h2 id="开发环境的准备："><a href="#开发环境的准备：" class="headerlink" title="开发环境的准备："></a>开发环境的准备：</h2><p>Spark 可以运行在 Linux, Max OS X, 或者是 Windows 上。这里我是在Windows上运行的。在本地机器上需要有Java8.x和maven环境，另外我们推荐使用 ItelliJ IDEA 作为 Flink 应用程序的开发 IDE。<br>首先在我们的pom.xml文件中添加Spark相关的依赖。<br>pom如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;maven-v4_0_0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line">  &lt;groupId&gt;SparkDemo&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;SparkDemo&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">  &lt;inceptionYear&gt;2008&lt;&#x2F;inceptionYear&gt;</span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;scala.version&gt;2.11.1&lt;&#x2F;scala.version&gt;</span><br><span class="line">  &lt;&#x2F;properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;scala-tools.org&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;name&gt;Scala-Tools Maven2 Repository&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;url&gt;http:&#x2F;&#x2F;scala-tools.org&#x2F;repo-releases&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">  &lt;&#x2F;repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;pluginRepositories&gt;</span><br><span class="line">    &lt;pluginRepository&gt;</span><br><span class="line">      &lt;id&gt;scala-tools.org&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;name&gt;Scala-Tools Maven2 Repository&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;url&gt;http:&#x2F;&#x2F;scala-tools.org&#x2F;repo-releases&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;pluginRepository&gt;</span><br><span class="line">  &lt;&#x2F;pluginRepositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.scala-lang&lt;&#x2F;groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;scala-library&lt;&#x2F;artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;scala.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.apache.spark&#x2F;spark-core --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.2.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.4&lt;&#x2F;version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.specs&lt;&#x2F;groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;specs&lt;&#x2F;artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.2.5&lt;&#x2F;version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">  &lt;&#x2F;dependencies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;build&gt;</span><br><span class="line">    &lt;sourceDirectory&gt;src&#x2F;main&#x2F;scala&lt;&#x2F;sourceDirectory&gt;</span><br><span class="line">    &lt;testSourceDirectory&gt;src&#x2F;test&#x2F;scala&lt;&#x2F;testSourceDirectory&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.scala-tools&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-scala-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;executions&gt;</span><br><span class="line">          &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">              &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">              &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">            &lt;&#x2F;goals&gt;</span><br><span class="line">          &lt;&#x2F;execution&gt;</span><br><span class="line">        &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">          &lt;args&gt;</span><br><span class="line">            &lt;arg&gt;-target:jvm-1.5&lt;&#x2F;arg&gt;</span><br><span class="line">          &lt;&#x2F;args&gt;</span><br><span class="line">        &lt;&#x2F;configuration&gt;</span><br><span class="line">      &lt;&#x2F;plugin&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-eclipse-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;downloadSources&gt;true&lt;&#x2F;downloadSources&gt;</span><br><span class="line">          &lt;buildcommands&gt;</span><br><span class="line">            &lt;buildcommand&gt;ch.epfl.lamp.sdt.core.scalabuilder&lt;&#x2F;buildcommand&gt;</span><br><span class="line">          &lt;&#x2F;buildcommands&gt;</span><br><span class="line">          &lt;additionalProjectnatures&gt;</span><br><span class="line">            &lt;projectnature&gt;ch.epfl.lamp.sdt.core.scalanature&lt;&#x2F;projectnature&gt;</span><br><span class="line">          &lt;&#x2F;additionalProjectnatures&gt;</span><br><span class="line">          &lt;classpathContainers&gt;</span><br><span class="line">            &lt;classpathContainer&gt;org.eclipse.jdt.launching.JRE_CONTAINER&lt;&#x2F;classpathContainer&gt;</span><br><span class="line">            &lt;classpathContainer&gt;ch.epfl.lamp.sdt.launching.SCALA_CONTAINER&lt;&#x2F;classpathContainer&gt;</span><br><span class="line">          &lt;&#x2F;classpathContainers&gt;</span><br><span class="line">        &lt;&#x2F;configuration&gt;</span><br><span class="line">      &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">  &lt;&#x2F;build&gt;</span><br><span class="line">  &lt;reporting&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.scala-tools&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-scala-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">        &lt;&#x2F;configuration&gt;</span><br><span class="line">      &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">  &lt;&#x2F;reporting&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><br>工作目录：<br><img src="https://uploadfiles.nowcoder.com/images/20191024/9094293_1571913840705_C0C094E4A298BFA098F8123797C7F7CC" alt="图片说明" title="图片标题"><br>编写Spark程序：<br>创建WordCount程序：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package SparkDemo</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">&#x2F;**</span><br><span class="line">  * @Author: luomo</span><br><span class="line">  * @CreateTime: 2019&#x2F;10&#x2F;24</span><br><span class="line">  * @Description:WordCount</span><br><span class="line">  *&#x2F;</span><br><span class="line">object WordCount &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    &#x2F;&#x2F;配置文件</span><br><span class="line">    val conf &#x3D; new SparkConf().</span><br><span class="line">      setAppName(&quot;wordcount&quot;) &#x2F;&#x2F; 运行时候的作业名称</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">    &#x2F;&#x2F;上下文 拿着Conf信息创建出来 写spark应用程序的对象  通往集群的入口</span><br><span class="line">    val sc &#x3D; new SparkContext(conf)</span><br><span class="line">    &#x2F;&#x2F;传入文件对象 返回RDD集合</span><br><span class="line">    val input &#x3D; sc.textFile(&quot;E:&#x2F;&#x2F;&#x2F;test.txt&quot;)</span><br><span class="line">    &#x2F;&#x2F;对文件行数据 按照空格切割 返回RDD集合 得到每个单词</span><br><span class="line">    val lines &#x3D; input.flatMap(line &#x3D;&gt; line.split(&quot; &quot;))</span><br><span class="line">    &#x2F;&#x2F;统计单词数量 计数 得到RDD集合 按照相同的Key先分组，之后再对组内的Value进行操作</span><br><span class="line">    val count &#x3D; lines.map(word &#x3D;&gt; (word, 1)).reduceByKey(_ + _)</span><br><span class="line">    &#x2F;&#x2F;将结果遍历打印到控制台</span><br><span class="line">    count.foreach(x &#x3D;&gt;&#123;</span><br><span class="line">      println(x)</span><br><span class="line">    &#125;)</span><br><span class="line">    &#x2F;&#x2F;将结果输出到文件中</span><br><span class="line">    val output &#x3D; count.saveAsTextFile(&quot;E:&#x2F;&#x2F;&#x2F;wordCount&quot;)</span><br><span class="line">    &#x2F;&#x2F;关闭流 在内存中释放这个spark对象</span><br><span class="line">    &#x2F;&#x2F;sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>运行程序如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191024/9094293_1571914229568_474EA976C3A9A912CB36A0631F855823" alt="图片说明" title="图片标题"> </p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title>Flink Table API&amp;SQL编程</title>
    <url>/2020/06/21/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="什么是Flink关系型API"><a href="#什么是Flink关系型API" class="headerlink" title="什么是Flink关系型API"></a>什么是Flink关系型API</h1><p>Flink提供了三层API。每个API在简介型和表达性之间提供了不同的权衡，并且针对不同的用例</p>
<p><img src="/2020/06/21/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B/1.png" alt></p>
<p>Flink具有两个关系API，即Table API和SQL。这两个API都是用于批处理和流处理的统一API，即，对无界的实时流或有界的记录流以相同的语义执行查询，并产生相同的结果。Table API和SQL利用Apache Calcite进行解析，验证和查询优化。它们可以与DataStream和DataSet API无缝集成。无论输入是批处理输入（DataSet）还是流输入（DataStream），在两个接口中指定的查询具有相同的语义并指定相同的结果。</p>
<p><strong>请注意，Table API和SQL尚未完成功能，正在积极开发中。</strong> <strong>[Table API，SQL]和[stream，batch]输入的每种组合都不支持所有操作。</strong></p>
<p>Flink的关系API旨在简化数据分析，数据管道和ETL应用程序的定义。</p>
<p>以下示例显示了SQL查询，以会话化点击流并计算每个会话的点击次数。这与DataStream API示例中的用例相同。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SELECT userId, COUNT(*)</span><br><span class="line">FROM clicks</span><br><span class="line"><span class="function">GROUP BY <span class="title">SESSION</span><span class="params">(clicktime, INTERVAL <span class="string">'30'</span> MINUTE)</span>, userId</span></span><br></pre></td></tr></table></figure>
<h1 id="Table-API-amp-SQL特点"><a href="#Table-API-amp-SQL特点" class="headerlink" title="Table API &amp; SQL特点"></a>Table API &amp; SQL特点</h1><ul>
<li>第一，Table API &amp; SQL 是一种声明式的 API。用户只需关心做什么，不用关心怎么做，比如图中的 WordCount 例子，只需要关心按什么维度聚合，做哪种类型的聚合，不需要关心底层的实现。</li>
<li><p>第二，高性能。Table API &amp; SQL 底层会有优化器对 query 进行优化。举个例子，假如 WordCount 的例子里写了两个 count 操作，优化器会识别并避免重复的计算，计算的时候只保留一个 count 操作，输出的时候再把相同的值输出两遍即可，以达到更好的性能。</p>
</li>
<li><p>第三，流批统一。上图例子可以发现，API 并没有区分流和批，同一套 query 可以流批复用，对业务开发来说，避免开发两套代码。</p>
</li>
<li><p>第四，标准稳定。Table API &amp; SQL 遵循 SQL 标准，不易变动。API 比较稳定的好处是不用考虑 API 兼容性问题。</p>
</li>
<li><p>第五，易理解。语义明确，所见即所得。</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table API:</span><br><span class="line">tab.groupBy(<span class="string">"word"</span>)</span><br><span class="line">    .select(<span class="string">"word,count(1) as count"</span>)</span><br><span class="line">SQL:</span><br><span class="line">select word,count(*) AS cnt </span><br><span class="line">from MyTable</span><br><span class="line">group by word</span><br></pre></td></tr></table></figure>
<h1 id="Table-API-amp-SQL开发"><a href="#Table-API-amp-SQL开发" class="headerlink" title="Table API&amp;SQL开发"></a>Table API&amp;SQL开发</h1><h2 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h2><ul>
<li>Scala版</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/16</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:TableSQLAPI</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object TableSQLAPI&#123;</span><br><span class="line">    <span class="function">def <span class="title">main</span><span class="params">(args:Array[String])</span>:unit </span>= &#123;</span><br><span class="line">        val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">        val tableEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line">            val filePath = <span class="string">"file:///user/sales.csv"</span></span><br><span class="line">            <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">            <span class="comment">//已经拿到DataSet</span></span><br><span class="line">            val csv = env.readCsvFile[SalesLog](filePath,ignoreFirstLine = <span class="keyword">true</span>)</span><br><span class="line">            <span class="comment">//DataSet ==&gt; Table</span></span><br><span class="line">            val salesTable = tablelEnv.fromDataSet(csv)</span><br><span class="line">            <span class="comment">//Table ==&gt; table</span></span><br><span class="line">            tableEnv.registerTable(<span class="string">"sales"</span>,salesTable)</span><br><span class="line">            <span class="comment">//sql</span></span><br><span class="line">            val resultTable = tableEnv.sqlQuery(<span class="string">"select customerId,sum(amountPaid) money from sales group by customerId"</span>)</span><br><span class="line">            tableEnv.toDataset[Row](resultTable).print()</span><br><span class="line">	</span><br><span class="line">            <span class="function"><span class="keyword">case</span> class <span class="title">SalesLog</span><span class="params">(transactionId:String,customerId:String,itemId:String,</span></span></span><br><span class="line"><span class="function"><span class="params">                               amountPaid:Double)</span>        </span></span><br><span class="line"><span class="function">            </span></span><br><span class="line"><span class="function">    &#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Java版</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>:2020/6/16</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>:JavaTableSQLAPI</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaTableSQLAPI</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEvironment();</span><br><span class="line">        BatchTableEnvironment tableEnv = BatchTableEnvironment.getTableEnvironment(env);</span><br><span class="line">         val filePath = <span class="string">"file:///user/sales.csv"</span></span><br><span class="line">            <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">        DataSet&lt;Sales&gt; csv = env.readCsvFile(filePath).ignoreFirstLine().pojoType(Sales<span class="class">.<span class="keyword">class</span>,<span class="title">transactionId</span>,<span class="title">customerId</span>,<span class="title">itemId</span>,<span class="title">amountPaid</span>)</span>;</span><br><span class="line">        Table sales = tableEnv.fromDataSet(csv);</span><br><span class="line">        tableEnv.registerTable(<span class="string">"sales"</span>,sales);</span><br><span class="line">        Table resultTable = tableEnv.sqlQuery(<span class="string">"select customerId,sum(amountPaid) money from sales group by customerId"</span>);</span><br><span class="line">        DataSet&lt;ROw&gt; result = tableEnv.toDataSet(resultTable,Row<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        result.print();</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Sales</span></span>&#123;</span><br><span class="line">        <span class="keyword">public</span> String transactionId;</span><br><span class="line">        <span class="keyword">public</span> String customerId;</span><br><span class="line">        <span class="keyword">public</span> String itemId;</span><br><span class="line">        <span class="keyword">public</span> Double amountPaid;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Table API &amp; SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume监控之Ganglia</title>
    <url>/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。今天我们来看看如何用Ganglia来监控我们的flume集群数据。</p>
<h1 id="Ganglia的安装与部署"><a href="#Ganglia的安装与部署" class="headerlink" title="Ganglia的安装与部署"></a>Ganglia的安装与部署</h1><h2 id="1-安装httpd服务与php"><a href="#1-安装httpd服务与php" class="headerlink" title="1.安装httpd服务与php"></a>1.安装httpd服务与php</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# sudo yum -y install httpd php</span><br></pre></td></tr></table></figure>
<h2 id="2-安装其他依赖"><a href="#2-安装其他依赖" class="headerlink" title="2.安装其他依赖"></a>2.安装其他依赖</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[root@bigdata107 flume]# sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure>
<p>##3.安装ganglia<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# rpm -Uvh http:&#x2F;&#x2F;dl.fedoraproject.org&#x2F;pub&#x2F;epel&#x2F;6&#x2F;x86_64&#x2F;epel-release-6-8.noarch.rpm</span><br><span class="line"></span><br><span class="line">[root@bigdata107 flume]# yum -y install ganglia-gmetad</span><br><span class="line">[root@bigdata107 flume]# yum -y install ganglia-web</span><br><span class="line">[root@bigdata107 flume]# yum install -y ganglia-gmond</span><br></pre></td></tr></table></figure></p>
<h2 id="4-修改配置文件"><a href="#4-修改配置文件" class="headerlink" title="4.修改配置文件"></a>4.修改配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;ganglia.conf</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/1.png" alt="图片说明"> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim &#x2F;etc&#x2F;ganglia&#x2F;gmetad.conf</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/2.png" alt="图片说明"> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim &#x2F;etc&#x2F;ganglia&#x2F;gmond.conf</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/3.png" alt="图片说明"> </p>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/4.png" alt="图片说明"> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]#  vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/5.png" alt="图片说明"> </p>
<h2 id="5-selinux本次生效关闭必须重启-如果此时不想重启-可以临时生效"><a href="#5-selinux本次生效关闭必须重启-如果此时不想重启-可以临时生效" class="headerlink" title="5.selinux本次生效关闭必须重启,如果此时不想重启,可以临时生效"></a>5.selinux本次生效关闭必须重启,如果此时不想重启,可以临时生效</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]#  sudo setenforce 0</span><br></pre></td></tr></table></figure>
<h1 id="启动Ganglia"><a href="#启动Ganglia" class="headerlink" title="启动Ganglia"></a>启动Ganglia</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# service httpd start</span><br><span class="line">正在启动 httpd：</span><br><span class="line">[root@bigdata107 flume]# service gmetad start</span><br><span class="line">Starting GANGLIA gmetad:  [确定]</span><br><span class="line">[root@bigdata107 flume]# service gmond start</span><br><span class="line">Starting GANGLIA gmond:  [确定]</span><br></pre></td></tr></table></figure>
<h2 id="1-打开web-UI"><a href="#1-打开web-UI" class="headerlink" title="1.打开web UI"></a>1.打开web UI</h2><p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/6.png" alt=" "> </p>
<h2 id="2-通过Ganglia监控Flume"><a href="#2-通过Ganglia监控Flume" class="headerlink" title="2.通过Ganglia监控Flume"></a>2.通过Ganglia监控Flume</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# vim flume-env.sh</span><br></pre></td></tr></table></figure>
<p>添加如下内容：<br>export JAVA_OPTS=&quot;-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.216.107:8649 -Xms100m -Xmx200m&quot;</p>
<h2 id="3-启动flume任务"><a href="#3-启动flume任务" class="headerlink" title="3.启动flume任务"></a>3.启动flume任务</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# bin&#x2F;flume-ng agent </span><br><span class="line">--conf conf&#x2F; </span><br><span class="line">--name agent1 </span><br><span class="line">--conf-file job&#x2F;flume_telnet_logger.conf </span><br><span class="line">-Dflume.root.logger&#x3D;&#x3D;INFO,console </span><br><span class="line">-Dflume.monitoring.type&#x3D;ganglia </span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.216.107:8649</span><br></pre></td></tr></table></figure>
<h2 id="4-发送数据查看Ganglia监测图"><a href="#4-发送数据查看Ganglia监测图" class="headerlink" title="4.发送数据查看Ganglia监测图"></a>4.发送数据查看Ganglia监测图</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 flume]# telnet bigdata107 44444</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/7.png" alt="图片说明"></p>
<p><img src="/2020/04/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/8.png" alt="图片说明"> </p>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>集群监控</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS(1.0)与(2.0)</title>
    <url>/2019/09/08/HDFS-1-0-%E4%B8%8E-2-0/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当数据集超过一个单独的物理计算机的存储能力时，便有必要将它分部到多个独立的计算机。管理跨计算机网络存储的文件系统成为分布式文件系统。因为它们是基于网络的，所有网络编程的复杂性都会随之而来，所以分布式文件系统比普通磁盘文件系统更复杂。使这个文件系统能容忍节点故障而不损失数据就是一个极大的挑战。</p>
<h2 id="HDFS的设计"><a href="#HDFS的设计" class="headerlink" title="HDFS的设计"></a>HDFS的设计</h2><p>HDFS是为以流式出局访问模式存储超大文件而设计的文件系统，在商用硬件的集群上运行。让我们看看它的优势：<br><strong>超大文件</strong><br>“超大文件”在这里指几百MB，几百GB甚至几百TB大小的文件。目前已经有Hadoop集群存储PB级的数据了。<br><strong>流式数据</strong><br>HDFS建立在这样一个思想上：一次写入、多次读取模式是最高效的。一个数据集通常由数据源生成或复制，接着在此基础上进行各种各样的分析。每个分析至少都会涉及数据集中的大部分数据（甚至全部）。<br><strong>商用硬件</strong><br>Hadoop不需要运行在昂贵并且高可靠的硬件上。它被设计运行在商用硬件（在各种零售店都能买到的普通硬件）的集群上，因此至少对于大的集群来说，节点故障的几率还是较高的。HDFS在面对这种故障时，被设计为能够继续运行而让用户察觉不到明显的中断。</p>
<h2 id="HDFS-1-0架构："><a href="#HDFS-1-0架构：" class="headerlink" title="HDFS 1.0架构："></a>HDFS 1.0架构：</h2><p> HDFS采用的是Master/Slave架构一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点。</p>
<h2 id="HDFS-1-0-的问题"><a href="#HDFS-1-0-的问题" class="headerlink" title="HDFS 1.0 的问题"></a>HDFS 1.0 的问题</h2><p>进入了 PB 级的大数据时代，HDFS 1.0的设计缺陷已经无法满足生产的需求，最致命的问题有以下两点：</p>
<ul>
<li>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</li>
<li>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。  </li>
</ul>
<p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p>
<ul>
<li>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</li>
<li>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。  </li>
</ul>
<h2 id="HDFS-2-0-的-HA-实现"><a href="#HDFS-2-0-的-HA-实现" class="headerlink" title="HDFS 2.0 的 HA 实现"></a>HDFS 2.0 的 HA 实现</h2><p>这里先看下 HDFS 高可用解决方案的架构设计，如下图</p>
<p><img src="/2019/09/08/HDFS-1-0-%E4%B8%8E-2-0/1.png" alt="图片说明"> </p>
<p>这里与前面 1.0 的架构已经有很大变化，简单介绍一下的组件：</p>
<ol>
<li>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li>
<li>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于Zookeeper 的手动主备切换）；</li>
<li>Zookeeper 集群：为主备切换控制器提供主备选举支持；</li>
<li>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</li>
<li>DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li>
</ol>
<h2 id="HDFS-2-0-Federation-实现"><a href="#HDFS-2-0-Federation-实现" class="headerlink" title="HDFS 2.0 Federation 实现"></a>HDFS 2.0 Federation 实现</h2><p>在 1.0 中，HDFS 的架构设计有以下缺点：</p>
<p>namespace 扩展性差：在单一的 NN 情况下，因为所有 namespace 数据都需要加载到内存，所以物理机内存的大小限制了整个 HDFS 能够容纳文件的最大个数（namespace 指的是 HDFS 中树形目录和文件结构以及文件对应的 block 信息）；<br>性能可扩展性差：由于所有请求都需要经过 NN，单一 NN 导致所有请求都由一台机器进行处理，很容易达到单台机器的吞吐；<br>隔离性差：多租户的情况下，单一 NN 的架构无法在租户间进行隔离，会造成不可避免的相互影响。<br>而 Federation 的设计就是为了解决这些问题，采用 Federation 的最主要原因是设计实现简单，而且还能解决问题。</p>
<h2 id="Federation-架构"><a href="#Federation-架构" class="headerlink" title="Federation 架构"></a>Federation 架构</h2><p>Federation 的架构设计如下图所示（图片来自 HDFS Federation）：<br><img src="/2019/09/08/HDFS-1-0-%E4%B8%8E-2-0/2.png" alt="图片说明"> </p>
<h3 id="HDFS-Federation-架构实现"><a href="#HDFS-Federation-架构实现" class="headerlink" title="HDFS Federation 架构实现"></a>HDFS Federation 架构实现</h3><p>Federation 的核心设计思想<br>Federation 的核心思想是将一个大的 namespace 划分多个子 namespace，并且每个 namespace 分别由单独的 NameNode 负责，这些 NameNode 之间互相独立，不会影响，不需要做任何协调工作（其实跟拆集群有一些相似），集群的所有 DataNode 会被多个 NameNode 共享。</p>
<p>其中，每个子 namespace 和 DataNode 之间会由数据块管理层作为中介建立映射关系，数据块管理层由若干数据块池（Pool）构成，每个数据块只会唯一属于某个固定的数据块池，而一个子 namespace 可以对应多个数据块池。每个 DataNode 需要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告，并执行来自所有 NameNode 的命令。</p>
<ol>
<li>一个 block pool 由属于同一个 namespace 的数据块组成，每个 DataNode 可能会存储集群中所有 block pool 的数据块；</li>
<li>每个 block pool 内部自治，也就是说各自管理各自的 block，不会与其他 block pool 交流，如果一个 NameNode 挂掉了，不会影响其他 NameNode;</li>
<li>某个 NameNode 上的 namespace 和它对应的 block pool 一起被称为 namespace volume，它是管理的基本单位。当一个 NameNode/namespace 被删除后，其所有 DataNode 上对应的 block pool 也会被删除，当集群升级时，每个 namespace volume 可以作为一个基本单元进行升级。<br>参考链接：<br><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/</a><br><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html</a></li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 索引机制和原理</title>
    <url>/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h1 id="Hive索引介绍"><a href="#Hive索引介绍" class="headerlink" title="Hive索引介绍"></a>Hive索引介绍</h1><p>Hive的索引目的是提高Hive表指定列的查询速度。Hive只有有限的索引功能，但是还是可以对一些字段建立索引来加速某些操作的。一张表的索引数据存储在另外一张表中。</p>
<p>当逻辑分区实际上太多太细而几乎无法使用时，建立索引也就成为分区的另一个选择。建立索引可以帮助裁剪掉一张表的一些数据块，这样能够减少MapReduce的输入数据量。并非所有的查询都可以通过建立索引获得好处。通过EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>
<p>没有索引时，类似’WHERE tab1.col1 = 10’ 的查询，Hive会加载整张表或分区，然后处理所有的rows，但是如果在字段col1上面存在索引时，那么只会加载和处理文件的一部分。<br>Hive 0.7.0版本中，加入了索引。</p>
<p>Hive 0.8.0版本中增加了bitmap索引。</p>
<p>Hive只有有限的索引功能。没有关系型数据库中键的概念。Hive中的索引和那些关系型数据库中的一样，需要进行仔细评估才能使用。维护索引也需要额外的存储空间，同时创建索引也需要消耗计算资源。用户需要在建立索引为查询带来的好处和因此而需要付出的代价之间做出权衡。</p>
<h1 id="Hive索引机制和原理"><a href="#Hive索引机制和原理" class="headerlink" title="Hive索引机制和原理"></a>Hive索引机制和原理</h1><p>在指定列上建立索引，会产生一张索引表（Hive的一张物理表），里面的字段包括，索引列的值、该值对应的HDFS文件路径、该值在文件中的偏移量;</p>
<p><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/1.png" alt></p>
<p>在执行索引字段查询时候，首先额外生成一个MR job，根据对索引列的过滤条件，从索引表中过滤出索引列的值对应的hdfs文件路径及偏移量，输出到hdfs上的一个文件中，然后根据这些文件中的hdfs路径和偏移量，筛选原始input文件，生成新的split,作为整个job的split,这样就达到不用全表扫描的目的。</p>
<p>例如:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">select * from t_student where name = <span class="string">'zzq'</span>;</span><br></pre></td></tr></table></figure>
<p>首先用一个job，从索引表中过滤出key = ‘zzq’的记录，将其对应的HDFS文件路径及偏移量输出到HDFS临时文件中<br>接下来的job中以临时文件为input，根据里面的HDFS文件路径及偏移量，生成新的split，作为查询job的map任务input</p>
<p>不使用索引时候，如下图所示：</p>
<p><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/2.png" alt></p>
<p>table t_student的每一个split都会用一个map task去扫描，但其实只有split2中有<a href="http://my.525.life/article?id=1510739741983" target="_blank" rel="noopener">我们</a>想要的结果数据，map task1和map task3造成了资源浪费。</p>
<p>使用索引后，如下图所示：</p>
<p><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/3.png" alt></p>
<p>查询提交后，先用一个MR，扫描索引表，从索引表中找出key=’xx’的记录，获取到HDFS文件名和偏移量；<br>接下来，直接定位到该文件中的偏移量，用一个map task即可完成查询，其最终目的就是为了减少查询时候的input size</p>
<h1 id="Hive索引优点"><a href="#Hive索引优点" class="headerlink" title="Hive索引优点"></a>Hive索引优点</h1><p>索引可以避免全表扫描和资源浪费<br>索引可以加快含有group by语句的查询的计算速度</p>
<h1 id="创建索引表"><a href="#创建索引表" class="headerlink" title="创建索引表"></a>创建索引表</h1><p>对已有的表t_student根据name新建索引命名为student_index保存在表student_index_table中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create index student_index on table t_student(name)</span><br><span class="line">as  &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39;</span><br><span class="line">with deferred rebuild</span><br><span class="line">in table t_student ;</span><br></pre></td></tr></table></figure>
<p>注意，创建完索引表后里面是空的，需要重建索引才会有索引的数据。</p>
<h1 id="重建索引-生成索引数据"><a href="#重建索引-生成索引数据" class="headerlink" title="重建索引(生成索引数据)"></a>重建索引(生成索引数据)</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter index student_index on t_student rebuild;</span><br></pre></td></tr></table></figure>
<h1 id="自动使用索引"><a href="#自动使用索引" class="headerlink" title="自动使用索引"></a>自动使用索引</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET hive.input.format&#x3D;org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">SET hive.optimize.index.filter&#x3D;true;</span><br><span class="line">SET hive.optimize.index.filter.compact.minsize&#x3D;0;</span><br></pre></td></tr></table></figure>
<h1 id="手动使用索引"><a href="#手动使用索引" class="headerlink" title="手动使用索引"></a>手动使用索引</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET hive.input.format&#x3D;org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line"></span><br><span class="line">Insert overwrite directory &quot;&#x2F;tmp&#x2F;student_index_data&quot;</span><br><span class="line">select &#96;_bucketname&#96;, &#96;_offsets&#96; </span><br><span class="line">from  student_index_table </span><br><span class="line">where key &#x3D; &#39;zzq&#39;;</span><br><span class="line"></span><br><span class="line">##指定索引数据文件</span><br><span class="line">SET hive.index.compact.file&#x3D;&#x2F;tmp&#x2F;student_index_data;</span><br><span class="line">SET hive.optimize.index.filter&#x3D;false;</span><br><span class="line">SET hive.input.format&#x3D;org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;</span><br><span class="line"></span><br><span class="line">select * from t_student</span><br><span class="line">where key &#x3D; &#39;zzq&#39;;</span><br></pre></td></tr></table></figure>
<h1 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DROP INDEX student_index on t_student;</span><br></pre></td></tr></table></figure>
<h1 id="查看索引"><a href="#查看索引" class="headerlink" title="查看索引"></a>查看索引</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW INDEX on t_student;</span><br></pre></td></tr></table></figure>
<h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h1><p>从以上过程可以看出，Hive索引的使用过程比较繁琐：<br>每次查询时候都要先用一个job扫描索引表，如果索引列的值非常稀疏，那么索引表本身也会非常大；<br>索引表不会自动rebuild，如果表有数据新增或删除，那么必须手动rebuild索引表数据；</p>
<h1 id="Hive当前的索引类型"><a href="#Hive当前的索引类型" class="headerlink" title="Hive当前的索引类型"></a>Hive当前的索引类型</h1><ul>
<li>CompactIndexHandler(压缩索引)</li>
<li>Bitmap（位图坐标）</li>
</ul>
<h2 id="CompactIndexHandler"><a href="#CompactIndexHandler" class="headerlink" title="CompactIndexHandler"></a>CompactIndexHandler</h2><p>通过将列中相同的值得字段进行压缩从而减小存储和加快访问时间。需要注意的是Hive创建压缩索引时会将索引数据也存储在Hive表中。对于表tb_index (id int, name string) 而言，建立索引后的索引表中默认的三列一次为索引列（id）、hdfs文件地址(_bucketname)、偏移量(offset)。特别注意，offset列类型为array。</p>
<h2 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h2><p>位图索引作为一种常见的索引，如果索引列只有固定的几个值，那么就可以采用位图索引来加速查询。利用位图索引可以方便的进行AND/OR/XOR等各类计算，Hive0.8版本开始引入位图索引，位图索引在大数据处理方面的应用广泛，比如可以利用bitmap来计算用户留存率（索引做与运算，效率远好于join的方式）。如果Bitmap索引很稀疏，那么就需要对索引压缩以节省存储空间和加快IO。Hive的Bitmap Handler采用的是EWAH（lemire/javaewah: A compressed alternative t…）压缩方式。</p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>有张表名为table的表，由三列组成，分别是姓名、性别和婚姻状况，其中性别只有男和女两项，婚姻状况由已婚、未婚、离婚这三项，该表共有100w个记录。现在有这样的查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from table where Gender&#x3D;‘男’ and Marital&#x3D;“未婚”;</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/4.png" alt></p>
<p>1）不使用索引</p>
<p>　　不使用索引时，数据库只能一行行扫描所有记录，然后判断该记录是否满足查询条件。</p>
<p>2）B树索引</p>
<p>　　对于性别，可取值的范围只有’男’,’女’，并且男和女可能各站该表的50%的数据，这时添加B树索引还是需要取出一半的数据， 因此完全没有必要。相反，如果某个字段的取值范围很广，几乎没有重复，比如身份证号，此时使用B树索引较为合适。事实上，当取出的行数据占用表中大部分的数据时，即使添加了B树索引，数据库如oracle、MySQL也不会使用B树索引，很有可能还是一行行全部扫描。</p>
<h3 id="位图索引"><a href="#位图索引" class="headerlink" title="位图索引"></a>位图索引</h3><p>如果用户查询的列的基数非常的小， 即只有的几个固定值，如性别、婚姻状况、行政区等等。要为这些基数值比较小的列建索引，就需要建立位图索引。</p>
<p>对于性别这个列，位图索引形成两个向量，男向量为10100…，向量的每一位表示该行是否是男，如果是则位1，否为0，同理，女向量位01011。<br><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/5.png" alt></p>
<p>对于婚姻状况这一列，位图索引生成三个向量，已婚为11000…，未婚为00100…，离婚为00010…。</p>
<p><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/6.png" alt></p>
<p>当我们使用查询语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from table where Gender&#x3D;‘男’ and Marital&#x3D;“未婚”</span><br></pre></td></tr></table></figure>
<p>的时候 首先取出男向量10100…，然后取出未婚向量00100…，将两个向量做and操作，这时生成新向量00100…，可以发现第三位为1，表示该表的第三行数据就是我们需要查询的结果。</p>
<p><img src="/2020/07/12/Hive-%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E5%92%8C%E5%8E%9F%E7%90%86/7.png" alt></p>
<h3 id="位图索引的适用条件"><a href="#位图索引的适用条件" class="headerlink" title="位图索引的适用条件"></a>位图索引的适用条件</h3><p>　　上面讲了，位图索引适合只有几个固定值的列，如性别、婚姻状况、行政区等等，而身份证号这种类型不适合用位图索引。</p>
<p>　　此外，位图索引适合静态数据，而不适合索引频繁更新的列。举个例子，有这样一个字段busy，记录各个机器的繁忙与否，当机器忙碌时，busy为1，当机器不忙碌时，busy为0。</p>
<p>　　这个时候有人会说使用位图索引，因为busy只有两个值。好，我们使用位图索引索引busy字段！假设用户A使用update更新某个机器的busy值，比如update table set table.busy=1 where rowid=100;，但还没有commit，而用户B也使用update更新另一个机器的busy值，update table set table.busy=1 where rowid=12; 这个时候用户B怎么也更新不了，需要等待用户A commit。</p>
<p>　　原因：用户A更新了某个机器的busy值为1，会导致所有busy为1的机器的位图向量发生改变，因此数据库会将busy＝1的所有行锁定，只有commit之后才解锁。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们可以发现Hive的索引功能现在还相对较晚，提供的选项还较少。但是，索引被设计为可使用内置的可插拔的java代码来定制，用户可以扩展这个功能来满足自己的需求。 当然不是说所有的查询都会受惠于Hive索引。用户可以使用EXPLAIN语法来分析HiveQL语句是否可以使用索引来提升用户查询的性能。像RDBMS中的索引一样，需要评估索引创建的是否合理，毕竟，索引需要更多的磁盘空间，并且创建维护索引也会有一定的代价。 用户必须要权衡从索引得到的好处和代价。</p>
<p><strong>参考文章</strong></p>
<ul>
<li><a href="https://blog.csdn.net/qq_26937525/article/details/54631650" target="_blank" rel="noopener">HiveIndex</a></li>
<li><a href="https://blog.csdn.net/zzq900503/article/details/79391071" target="_blank" rel="noopener">Hive的索引机制和原理</a></li>
</ul>
]]></content>
      <categories>
        <category>Hadoop生态圈</category>
      </categories>
      <tags>
        <tag>Hive索引</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS NameNode的工作机制</title>
    <url>/2020/04/02/HDFS-NameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h2><p><img src="/2020/04/02/HDFS-NameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/1.png" alt=" "> </p>
<h3 id="1-第一阶段：-namenode-启动"><a href="#1-第一阶段：-namenode-启动" class="headerlink" title="1 第一阶段： namenode 启动"></a>1 第一阶段： namenode 启动</h3><p>1）第一次启动 namenode 格式化后， 创建 fsimage 和 edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>2） 客户端对元数据进行增删改的请求。<br>3） namenode 记录操作日志，更新滚动日志。<br>4） namenode 在内存中对数据进行增删改查。</p>
<h3 id="2-第二阶段：-Secondary-NameNode-工作"><a href="#2-第二阶段：-Secondary-NameNode-工作" class="headerlink" title="2 第二阶段： Secondary NameNode 工作"></a>2 第二阶段： Secondary NameNode 工作</h3><p>1） Secondary NameNode 询问 namenode 是否需要 checkpoint。 直接带回 namenode 是否检查结果。<br>2） Secondary NameNode 请求执行 checkpoint。<br>3） namenode 滚动正在写的 edits 日志。<br>4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。<br>5） Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。<br>6） 生成新的镜像文件 fsimage.chkpoint。<br>7） 拷贝 fsimage.chkpoint 到 namenode。<br>8） namenode 将 fsimage.chkpoint 重新命名成 fsimage。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop安装</title>
    <url>/2019/09/03/Hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h1 id="一、单机（伪分布式）"><a href="#一、单机（伪分布式）" class="headerlink" title="一、单机（伪分布式）"></a>一、单机（伪分布式）</h1><h2 id="1-1、环境"><a href="#1-1、环境" class="headerlink" title="1.1、环境"></a>1.1、环境</h2><p>系统环境为Centos作为系统环境，这里不再叙述系统的安装<br>基于原生Hadoop 2 ，可以适合任何Hadoop 2.x.y版本，例如 Hadoop 2.7.1, Hadoop 2.4.1等。</p>
<h3 id="Hadoop运行环境搭建"><a href="#Hadoop运行环境搭建" class="headerlink" title="Hadoop运行环境搭建"></a>Hadoop运行环境搭建</h3><p>虚拟机网络模式设置为NAT<br><img src="https://uploadfiles.nowcoder.com/images/20190904/9094293_1567608344001_42757B180952AD1472AC869C06FD55B4" alt="图片说明" title="图片标题"> </p>
<h3 id="修改为静态ip"><a href="#修改为静态ip" class="headerlink" title="修改为静态ip"></a>修改为静态ip</h3><p>在终端命令窗口中输入</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules</span><br></pre></td></tr></table></figure>
<p>进入如下页面，删除 eth0 该行；将 eth1 修改为 eth0，同时复制物理 ip 地址<br><img src="https://uploadfiles.nowcoder.com/images/20190904/9094293_1567608631305_27EA844EA176F7AF92263BE2A5B29165" alt="图片说明" title="图片标题"> </p>
<p>修改IP地址<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@hadoop101 /]# vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure></p>
<p>需要修改的内容有6项：<br>HWADDR=<br>IPADDR=<br>GATEWAY=<br>ONBOOT=yes<br>BOOTPROTO=static<br>DNS1=8.8.8.8<br>执行：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 /]# service network restart</span><br></pre></td></tr></table></figure>
<p>如果报错，reboot，重启虚拟机。</p>
<h3 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h3><p>修改 linux 的 hosts 文件<br>（1）进入Linux系统查看本机的主机名。通过 hostname 命令查看。 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop100 /]# hostname </span><br><span class="line">hadoop100</span><br></pre></td></tr></table></figure>
<p>（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop100~]# vi /etc/sysconfig/network </span><br><span class="line">修改文件中主机名称 </span><br><span class="line">NETWORKING=yes </span><br><span class="line">NETWORKING_IPV6=no </span><br><span class="line">HOSTNAME= hadoop101 </span><br><span class="line">注意：主机名称不要有“_”下划线</span><br></pre></td></tr></table></figure>
<p>（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名<br>hadoop101<br>（4）保存退出。<br>（5）打开/etc/hosts<br>[root@hadoop100 ~]# vim /etc/hosts<br>添加如下内容<br>192.168.1.100 hadoop100<br>192.168.1.101 hadoop101<br>192.168.1.102 hadoop102<br>192.168.1.103 hadoop103<br>（6）并重启设备，重启后，查看主机名，已经修改成功<br>2）修改 window10 的 hosts 文件<br> （1）进入 C:\Windows\System32\drivers\etc 路径<br> （2）打开 hosts 文件并添加如下内容<br>192.168.1.100 hadoop100<br>192.168.1.101 hadoop101<br>192.168.1.102 hadoop102<br>192.168.1.103 hadoop103 </p>
<h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><p>1）查看防火墙开机启动状态<br>[root@hadoop101 ~]# chkconfig iptables –list<br>2）关闭防火墙<br>[root@hadoop101 ~]# chkconfig iptables off  </p>
<h3 id="在-opt-目录下创建文件"><a href="#在-opt-目录下创建文件" class="headerlink" title="在 opt 目录下创建文件"></a>在 opt 目录下创建文件</h3><p>1）创建 user 用户<br> 在 root 用户里面执行如下操作 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# adduser user </span><br><span class="line">[root@hadoop101 opt]# passwd user </span><br><span class="line">更改用户 test 的密码 。 </span><br><span class="line">新的 密码： </span><br><span class="line">无效的密码： 它没有包含足够的不同字符 </span><br><span class="line">无效的密码： 是回文 </span><br><span class="line">重新输入新的 密码： </span><br><span class="line">passwd： 所有的身份验证令牌已经成功更新。</span><br></pre></td></tr></table></figure>
<p>2）设置 user 用户具有 root 权限<br>修改 /etc/sudoers 文件，找到下面一行，在 root 下面添加一行，如下所示： </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 user]# vi /etc/sudoers </span><br><span class="line">## Allow root to run any commands anywhere </span><br><span class="line">root    ALL=(ALL)     ALL </span><br><span class="line">user   ALL=(ALL)     ALL</span><br></pre></td></tr></table></figure>
<p>修改完毕，现在可以用 user 帐号登录，然后用命令 su - ，即可获得 root 权限进行<br>操作。<br>3）在/opt 目录下创建文件夹<br>（1）在 root 用户下创建 module、software 文件夹 </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# mkdir module </span><br><span class="line">[root@hadoop101 opt]# mkdir software</span><br></pre></td></tr></table></figure>
<p>（2）修改 module、software 文件夹的所有者</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# chown user:user module </span><br><span class="line">[root@hadoop101 opt]# chown user:user sofrware </span><br><span class="line">[root@hadoop101 opt]# ls -al </span><br><span class="line">总用量 <span class="number">16</span> </span><br><span class="line">drwxr-xr-x.  <span class="number">6</span> root    root <span class="number">4096</span> <span class="number">4</span> 月  <span class="number">24</span> <span class="number">09</span>:<span class="number">07</span> . </span><br><span class="line">dr-xr-xr-x. <span class="number">23</span> root    root <span class="number">4096</span> <span class="number">4</span> 月  <span class="number">24</span> <span class="number">08</span>:<span class="number">52</span> .. </span><br><span class="line">drwxr-xr-x.  <span class="number">4</span> user user <span class="number">4096</span> <span class="number">4</span> 月  <span class="number">23</span> <span class="number">16</span>:<span class="number">26</span> <span class="keyword">module</span> </span><br><span class="line">drwxr-xr-x.  <span class="number">2</span> user user <span class="number">4096</span> <span class="number">4</span> 月  <span class="number">23</span> <span class="number">16</span>:<span class="number">25</span> software</span><br></pre></td></tr></table></figure>
<h3 id="安装-jdk"><a href="#安装-jdk" class="headerlink" title="安装 jdk"></a>安装 jdk</h3><p>1）卸载现有 jdk<br>（1）查询是否安装 java 软件：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# rpm -qa|grep java</span><br></pre></td></tr></table></figure><br>（2）如果安装的版本低于 1.7，卸载该 jdk</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# rpm -e 软件包</span><br></pre></td></tr></table></figure>
<p>2）用 SecureCRT 工具将 jdk、Hadoop-2.7.2.tar.gz 导入到 opt 目录下面的 software 文件夹下面<br>3）在 linux 系统下的 opt 目录中查看软件包是否导入成功<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# cd software/ </span><br><span class="line">[root@hadoop101 software]# ls </span><br><span class="line">hadoop-<span class="number">2.7</span><span class="number">.2</span>.tar.gz  jdk-<span class="number">8</span>u144-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><br>4）解压 jdk 到/opt/module 目录下 </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
<p>5）配置 jdk 环境变量<br>（1）先获取 jdk 路径： </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_144]# pwd </span><br><span class="line">/opt/<span class="keyword">module</span>/jdk1<span class="number">.8</span><span class="number">.0_144</span></span><br></pre></td></tr></table></figure>
<p>（2）打开/etc/profile 文件：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_144]# vi /etc/profile</span><br></pre></td></tr></table></figure>
<p>在 profie 文件末尾添加 jdk 路径： </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">##JAVA_HOME </span><br><span class="line">export JAVA_HOME=/opt/<span class="keyword">module</span>/jdk1<span class="number">.8</span><span class="number">.0_144</span> </span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>
<p>（3）保存后退出：<br>:wq<br> （4）让修改后的文件生效： </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_144]# source /etc/profile</span><br></pre></td></tr></table></figure>
<p>（5）重启（如果 java -version 可以用就不用重启）：  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">代码块</span><br></pre></td></tr></table></figure>
<p>6）测试 jdk 安装成功 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_144]# java -version </span><br><span class="line">java version <span class="string">"1.8.0_144"</span></span><br></pre></td></tr></table></figure>
<h1 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h1><p>1）进入到 Hadoop 安装包路径下：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 ~]# cd /opt/software/</span><br></pre></td></tr></table></figure>
<p> 2）解压安装文件到/opt/module 下面 </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
<p>3）查看是否解压成功 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 software]# ls /opt/module/ </span><br><span class="line">hadoop-<span class="number">2.7</span><span class="number">.2</span></span><br></pre></td></tr></table></figure>
<p>4）在/opt/module/hadoop-2.7.2/etc/hadoop 路径下配置 hadoop-env.sh<br>（1）Linux 系统中获取 jdk 的安装路径： </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_144]# echo $JAVA_HOME </span><br><span class="line">/opt/<span class="keyword">module</span>/jdk1<span class="number">.8</span><span class="number">.0_144</span></span><br></pre></td></tr></table></figure>
<p>（2）修改 hadoop-env.sh 文件中 JAVA_HOME 路径： </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 hadoop]# vi hadoop-env.sh</span><br></pre></td></tr></table></figure>
<p>修改 JAVA_HOME 如下 :</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/<span class="keyword">module</span>/jdk1<span class="number">.8</span><span class="number">.0_144</span></span><br></pre></td></tr></table></figure>
<p>5）将 hadoop 添加到环境变量<br> （1）获取 hadoop 安装路径：<br>[root@ hadoop101 hadoop-2.7.2]# pwd<br>/opt/module/hadoop-2.7.2<br> （2）打开/etc/profile 文件：<br>[root@ hadoop101 hadoop-2.7.2]# vi /etc/profile<br>  在 profie 文件末尾添加 jdk 路径：（shitf+g） </p>
<p>##HADOOP_HOME<br>export HADOOP_HOME=/opt/module/hadoop-2.7.2<br>export PATH=$PATH:$HADOOP_HOME/bin<br>export PATH=$PATH:$HADOOP_HOME/sbin<br>（3）保存后退出：<br>:wq<br> （4）让修改后的文件生效： </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@ hadoop101 hadoop-2.7.2]# source /etc/profile</span><br></pre></td></tr></table></figure>
<p>（5）重启(如果 hadoop 命令不能用再重启)：  </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@ hadoop101 hadoop-2.7.2]# sync </span><br><span class="line">[root@ hadoop101 hadoop-2.7.2]# reboot</span><br></pre></td></tr></table></figure>
<p>6）修改/opt 目录下的所有文件所有者为 user</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# chown user:user -R /opt/</span><br></pre></td></tr></table></figure>
<p>7）切换到 user 用户 </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 opt]# su user</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hadoop生态圈</category>
      </categories>
      <tags>
        <tag>Hadoop部署</tag>
      </tags>
  </entry>
  <entry>
    <title>HotSpot虚拟机对象探秘</title>
    <url>/2019/11/07/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/</url>
    <content><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>我们以常用的虚拟机HotSpot和常用的内存区域Java堆为例，深入了解HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。</p>
<h2 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h2><p>虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有先执行相应的类加载过程。<br>    在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同把一块确定大小的内存从Java堆中划分出来。假设堆中内存是绝对完整的，所有的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，所有用过的内存都放在一边，中间放着一个值指针作为分界点的指示器，这种分配方式成为“指针碰撞”。如图：<br><img src="/2019/11/07/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/1.png" alt="图片说明"> </p>
<p>如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个实例，并更新列表上的记录，这种分配方式称为“空闲列表”。如图：<br><img src="/2019/11/07/HotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98/2.png" alt="图片说明"> </p>
<p>选择哪种分配方式是由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩功能决定。因此，在使用serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。</p>
<h2 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h2><p>在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（InstanceData）和对齐填充（Padding）。<br>HotSpot虚拟机的对象头包括两部分信息：</p>
<p><strong>第一部分</strong>：用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark Word”。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身<br><strong>第二部分</strong>:实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。<br><strong>第三部分</strong>：对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或者2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。</p>
<h2 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h2><p>建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。目前主流的访问方式有使用<strong>句柄</strong>和<strong>直接指针</strong>两种。</p>
<p>如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息，</p>
<p>如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference中存储的直接就是对象地址，</p>
]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>JConsole:Java监视与管理控制台</title>
    <url>/2019/11/09/JConsole-Java%E7%9B%91%E8%A7%86%E4%B8%8E%E7%AE%A1%E7%90%86%E6%8E%A7%E5%88%B6%E5%8F%B0/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>JConsole（Java Monitoring and Management Console）是一种基于JMX的可视化监视、管理工具。<br><strong>1.启动JConsole</strong><br>通过JDK/bin目录下的“jconsole.exe”启动JConsole后，将自动搜索出本机运行的所有虚拟机进程，不需要用户自己再使用jps来查询了，如图双击选择其中一个进程即可开始监控了，也可以使用下面的“远程进程”功能来连接远程服务器，对远程虚拟机进行监控。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573274563736_6654F5DFA4EC7329FE86D88517B8FA93" alt="图片说明" title="图片标题"> </p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573275239073_438AC748ECDB4217D868DFDE95ECFD09" alt=" " title="图片标题"> </p>
<p>从图中可以看出机器运行了几个本地虚拟机进程，其中OOMTest是我准备的“反面教材”代码双击它进入JConsole主界面，包括“概述”、“内存”、“线程”、“类”、“VM概要”、“MBean”六个页签。如图所示</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573278160273_562106A0F5A1D726229907F196EB6D40" alt="图片说明" title="图片标题"> </p>
<p>“概述”页签显示的是整个虚拟机主要运行数据的概览，其中包括“堆内存使用情况”、“类”、“CPU使用情况”四种信息的曲线图，这些曲线图是后面“内存”、“线程”、“类”页签的信息汇总。</p>
<p><strong>2.内存监控</strong><br>“内存”页签相当于可视化的jsat命令，用于监视收集器管理的虚拟机内存（Java堆和永久代）的变化趋势。我们通过运行代码“OOMTest”来体验一下监视功能，运行时设置的虚拟机参数为：-Xms100m -Xmx100m -XX:+UseSerialGC,这段代码的作用是以64KB/50毫秒的速度往Java堆中填充数据，一共填充1000次，使用JConsole的“内存”页签进行监视，观察曲线和柱状图指示图的变化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author:luomo</span><br><span class="line"> * @CreateTime:2019&#x2F;11&#x2F;9</span><br><span class="line"> * @Description:OOMTest</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class OOMTest &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 内存占位符对象，一个OOMObject大约占64KB</span><br><span class="line">     *&#x2F;</span><br><span class="line">    static class OOMobject&#123;</span><br><span class="line">        public byte[] placeholder&#x3D;new byte[64*1024];</span><br><span class="line">    &#125;</span><br><span class="line">    public static void fillHeap(int num) throws InterruptedException&#123;</span><br><span class="line">        List&lt;OOMobject&gt; list &#x3D;new ArrayList&lt;OOMobject&gt;();</span><br><span class="line">        for(int i&#x3D;0;i&lt;num;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            &#x2F;&#x2F;稍作延时，令监视曲线的变化更加明显</span><br><span class="line">            Thread.sleep(50);</span><br><span class="line">            list.add(new OOMobject());</span><br><span class="line">        &#125;</span><br><span class="line">        System.gc();</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args)throws Exception&#123;</span><br><span class="line">        fillHeap(1000);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序运行后，在“内存”页签中可以看到内存池Eden区的运行趋势呈现折线状，如图。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282506435_42550CBEF3FCED46D7B42799FAD4566A" alt="图片说明" title="图片标题"> </p>
<p>监视范围扩大至整个堆后，会发现曲线是一条向上增长的平滑曲线。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282418309_B397F1590C7ED6E011AB02405287D0D0" alt="图片说明" title="图片标题"> </p>
<p>并且从柱状图可以看出，在1000次循环执行结束，运行了System.gc()后，虽然整个新生代Eden和Survivor区都基本被清空了，但是代表老年代的柱状图仍然保持峰值状态，说明被填充进堆中的数据在System.gc（）方法执行之后仍然存活。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191109/9094293_1573282569188_69314B5631685BFF2D1BB4774E598582" alt="图片说明" title="图片标题"> </p>
<p><strong>这里有两个小问题供读者思考：</strong><br>1.虚拟机启动参数只限制了Java堆为100MB，没有指定-Xmn参数，能否从监控图中估计出新生代有多大？<br>2.为何执行了System.gc（）之后，图中代表老年代的柱状图仍然显示峰值状态，代码需要如何调整才能让System.gc（）回收掉填充到堆中的对象？<br><strong>问题一：</strong>我们可以从图中知道Eden空间大小，因为没有设置-XX：SurvivorRadio参数，所以Eden与Survivor空间比例默认值为8:1，整个新生代空间这样我们可以用Eden空间大小/占新生代空间的比例，得出新生代空间的大小。<br><strong>问题二：</strong>执行完System.gc（）之后，空间未能回收是因为List<OOMObject>list对象仍然存活，fillHeap（）方法仍然没有退出，因此list对象在System.gc（）执行时仍然处于作用域之内。如果把System.gc移动到fillHeap()方法外调用就可以回收掉全部内存。 </OOMObject></p>
]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Java线程死锁</title>
    <url>/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/</url>
    <content><![CDATA[<h1 id="Java线程死锁"><a href="#Java线程死锁" class="headerlink" title="Java线程死锁"></a>Java线程死锁</h1><blockquote><br>死锁是一种特定的程序状态，在实体之间，由于循环依赖导致彼此一直处于等待之中，没有任何个体可以继续前进。死锁不仅仅是在线程之间会发生，存在资源独占的进程之间同样也 可能出现死锁。通常来说，我们大多是聚焦在多线程场景中的死锁，指两个或多个线程之 间，由于互相持有对方需要的锁，而永久处于阻塞的状态。<br><br></blockquote>


<p><strong>Java线程死锁</strong>是一个经典的多线程问题，因为不同的线程都在等待那些根本不可能被释放的锁，从而导致所有的工作都无法完成。如图所示</p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁.png" alt="图片说明"> </p>
<hr>

<h2 id="如何去定位Java线程死锁呢？"><a href="#如何去定位Java线程死锁呢？" class="headerlink" title="如何去定位Java线程死锁呢？"></a>如何去定位Java线程死锁呢？</h2><p>定位死锁最常见的方式就是利用 jstack 等工具获取线程栈，然后定位互相之间的依赖关系，进而找到死锁。如果是比较明显的死锁，往往 jstack 等就能直接定位，类似 JConsole 甚至可以在图形界面进行有限的死锁检测。<br>既然了解了用什么工具去定位线程死锁，那我们模拟一个Java线程死锁的情况，实战定位线程死锁<br><strong>死锁代码：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * @Author:luomo</span><br><span class="line"> * @CreateTime: 2020&#x2F;3&#x2F;28</span><br><span class="line"> * @Description:模拟DeadLock</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class deadLock implements Runnable&#123;</span><br><span class="line">    public static  Object obj1&#x3D;new Object();</span><br><span class="line">    public static  Object obj2&#x3D;new Object();</span><br><span class="line">    private int flag;</span><br><span class="line">    deadLock(int flag)&#123;</span><br><span class="line">        this.flag&#x3D;flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">        if(flag&#x3D;&#x3D;0)&#123;</span><br><span class="line">            synchronized (obj1)&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;成功获取锁1&quot;);</span><br><span class="line">                try &#123;</span><br><span class="line">                    Thread.currentThread().sleep(1000);</span><br><span class="line">                &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;尝试获取锁2&quot;);</span><br><span class="line">                synchronized (obj2)&#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            synchronized (obj2)&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);</span><br><span class="line">                try &#123;</span><br><span class="line">                    Thread.currentThread().sleep(1000);</span><br><span class="line">                &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+&quot;尝试获取锁1&quot;);</span><br><span class="line">                synchronized (obj1)&#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName()+&quot;成功获取锁2&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">   deadLock d1&#x3D;new deadLock(0);</span><br><span class="line">   deadLock d2&#x3D;new deadLock(1);</span><br><span class="line">   Thread thread1&#x3D;new Thread(d1);</span><br><span class="line">   Thread thread2&#x3D;new Thread(d2);</span><br><span class="line">   thread1.start();</span><br><span class="line">   thread2.start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>代码运行：</strong></p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁1.png" alt=" "> </p>
<p>从运行结果我们看到Thread1和Thread0同时都在争用对方已经占有的锁，进而产生死锁。</p>
<hr>

<h2 id="如何定位死锁"><a href="#如何定位死锁" class="headerlink" title="如何定位死锁"></a>如何定位死锁</h2><p>如果程序发生了死锁，我们如何去定位死锁？我们可以通过JConsole工具来发现死锁。<br>打开cmd：输入 JConsole 回车<br>我们可以看到一个可视化的工具，找到死锁进程点击连接</p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁2.png" alt="图片说明"> </p>
<p>我们可以看到有检查死锁的选项</p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁3.png" alt="图片说明"> </p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁4.png" alt="图片说明"> </p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁5.png" alt="图片说明"> </p>
<p>通过上图我们可以发现产生死锁的线程，从而定位到发生死锁的代码。</p>
<p>当然我们还可以使用Jstack + pid的方式来定位问题</p>
<p><img src="/2020/03/28/Java%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/死锁6.png" alt="图片说明"> </p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>死锁</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka producer</title>
    <url>/2020/06/30/Kafka-producer/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>不管是把Kafka 作为消息队列、消息、总线还是数据存储平台来使用，总是需要有一个可以往Kafka 写入数据的生产者和一个可以从Kafka 读取数据的消费者，或者一个兼具两种角色的应用程序。</p>
<h1 id="生产者概述"><a href="#生产者概述" class="headerlink" title="生产者概述"></a>生产者概述</h1><p>一个应用程序在很多情况下需要往Kafka 写入消息： 记录用户的活动（用于审计和分析）、记录度量指标、保存日志、消息、记录智能家电的信息、与其他应用程序进行异步通信、缓冲即将写入到数据库的数据，等等。</p>
<p>在保存网站的点击信息场景里，允许丢失少量的消息或出现少量的消息重复，延迟可以高一些，只要不影响用户体验就行。换句话说，只要用户点击链接后可以马上加载页面，那么我们并不介意消息要在几秒钟之后才能到达Kafka 服务器。吞吐量则取决于网站用户使用网站的频度。<br>不同的使用场景对生产者API 的使用和配置会有直接的影响。尽管生产者API 使用起来很简单， 但消息的发送过程还是有点复杂。如图所示：</p>
<p><img src="/2020/06/30/Kafka-producer/1.png" alt></p>
<p>我们从创建一个ProducerRecord 对象开始， Producer Record 对象需要包含目标主题和要发送的内容。我们还可以指定键或分区。在发送ProducerRecord 对象时，生产者要先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。<br>接下来，数据被传给分区器。如果之前在ProducerRcord 对象里指定了分区，那么分区器就不会再做任何事情，直接把指定的分区返回。如果没有指定分区，那么分区器会根据Producer Record 对象的键来选择一个分区。选好分区以后，生产者就知道该往哪个主题和分区发送这条记录了。紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的broker 上。服务器在收到这些消息时会返回一个响应。如果消息成功写入Kafka ，就返回一个RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败， 则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败，就返回错误信息。</p>
<h1 id="Kafka的配置属性"><a href="#Kafka的配置属性" class="headerlink" title="Kafka的配置属性"></a>Kafka的配置属性</h1><p>Kafka生产者有3个<strong>必选属性</strong>：bootstrap.servers,key.serializer,value.serializer。</p>
<ul>
<li>bootstrap.servers：该属性指定broker的地址清单，地址的格式为host:port.清单里不需要包含所有的broker地址，生产者会从给定的broker 里查找到其他broker的信息。不过建议至少要提供两个broker 的信息， 一且其中一个宕机，生产者仍然能够连接到集群上。</li>
<li>key.serializer:broker ：希望接收到的消息的键和值都是字节数组。生产者接口允许使用参数化类型，因此可以把java 对象作为键和值发送给broker 。这样的代码具有良好的可读性，不过生产者需要知道如何把这些java 对象转换成字节数组。key. serializer必须被设置为一<br>个实现了org.apache.kafka.common.seialization.Serialize 接口的类，生产者会使用这个类把键对象序列化成字节数组。Kafka 客户端默认提供了ByteArraySeializer（这个只做很少的事情）、StringSerializer 和IntegerSerializer，因此，如果你只使用常见的几种java 对象类型，那么就没必要实现自己的序列化器。要注意， key.serializer 是必须设置的，就算你打算只发送值内容。</li>
<li>value.serializer：与key.serializer一样， value.serializer指定的类会将值序列化。如果键和值都是字符串，可以使用与key.serializer 一样的序列化器。如果键是整数类型而值是字符串，那么需要使用不同的序列化器。    </li>
<li>acks<br>acks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的。这个参数对消息丢失的可能性有重要影响。主参数有如下选项。<br>• 如果<strong>acks=0</strong> ， 生产者在成功写入消息之前不会等待任何来自服务器的响应。也就是说，<br>如果当中出现了问题， 导致服务器没有收到消息，那么生产者就无从得知，消息也就丢<br>失了。不过，因为生产者不需要等待服务器的响应，所以它可以以网络能够支持的最大<br>速度发送消息，从而达到很高的吞吐量。<br>如果<strong>acks=1</strong> ，只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应。如果消息无法到达首领节点（比如首领节点崩溃，新的首领还没有被选举出来），生产者会收到一个错误响应，为了避免数据丢失，生产者会重发消息。不过，如果一个没有收到消息的节点成为新首领，消息还是会丢失。这个时候的吞吐量取决于使用的是同步发送还是异步发送。如果让发送客户端等待服务器的响应（通过调用Future对象的get（）方法），显然会增加延迟（在网络上传输一个来回的延迟）。如果客户端使用回调，延迟问题就可以得到缓解，不过吞吐量还是会受发送中消息数量的限制（比如，生产者在收到服务器响应之前可以发送多少个消息）。<br>如果<strong>acks=all</strong> ，只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。这种模式是最安全的，它可以保证不止一个服务器收到消息，就算有服务器发生崩溃，整个集群仍然可以运行。不过，它的延迟比acks=1时更高，因为我们要等待不只一个服务器节点接收消息。</li>
<li>buffer.me mory<br>该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。这个时候，send （）方法调用要么被阻塞，要么抛出异常，取决于如何设置block.on.buffer. full 参数（在0. 9.0.0 版本里被替换成了max.block.ms，表示在抛出异常之前可以阻塞一段时间）。</li>
<li>compression.type<br>默认情况下，消息发送时不会被压缩。该参数可以设置为snappy 、gzip 或lz 4 ，它指定了消息被发送给broker 之前使用哪一种压缩算也进行压缩。snappy 压缩算法由Google发明，它占用较少的CPU ，却能提供较好的性能和相当可观的压缩比，如果比较关注性能和网络带宽，可以使用这种算法。gzip 压缩算法一般会占用较多的CPU ，但会提供更高的压缩比，所以如果网络带宽比较有限，可以使用这种算法。使用压缩可以降低网络传输开销和存储销，而这往往是向Kafka 发送消息的瓶颈所在。</li>
<li><p>retries<br>生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领）。在这种情况<br>下， retries 参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会放弃重试并返回错误。默认情况下，生产者会在每次重试之间等待l00 ms ，不过可以通过retry.backoff.ms 参数来改变这个时间间隔。建议在设置重试次数和重试时间间隔之前，先测试一下恢复一个崩溃节点需要多少时间（比如所有分区选举出首领需要多长时间），让总的重试时间比Kafka 集群从崩溃中恢复的时间长，否则生产者会过早地放弃重试。不过有些错误不是临时性错误，没办法通过重试来解决（比如“消息太大”错误）。一般情况下，因为生产者会自动进行重试，所以就没必要在代码逻辑里处理那些可重试的错误。你只需要处理那些不可重试的错误或重试次数超出上限的情况。</p>
</li>
<li><p>batch.size<br>当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算（而不是消息个数）。当批次被填满，批次里的所有消息会被发送出去。不过生产者井不一定都会等到批次被填满才发送，半满的批次，甚至只包含一个消息的批次也有可能被发送。所以就算把批次大小设置得很大，也不会造成延迟，只是会占用更多的内存而已。但如果设置得太小，因为生产者需要更频繁地发送消息，会增加一些额外的开销。</p>
</li>
<li>linger.ms<br>该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。KafkaProduce 会在批次填满或linger.ms达到上限时把批次发送出去。默认情况下，只要有可用的线程， 生产者就会把消息发送出去，就算批次里只有一个消息。把linger.ms 设置成比0 大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次。虽然这样会增加延迟，但也会提升吞吐量（因为一次性发送更多的消息，每个消息的开销就变小了） 。</li>
<li>client.id<br>该参数可以是任意的字符串，服务器会用它来识别消息的来源，还可以用在日志和配额指标里。</li>
<li>max.in.flight.requests.per.connection<br>该参数指定了生产者在收到服务器晌应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。把它设为1 可以保证消息是按照发送的顺序写入服务器的，即使发生了重试。</li>
<li>timeout.ms 、request.timeout.ms 和metadata. fetch. timeout. ms<br>request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间，metada.fetch.timeout.ms 指定了生产者在获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间。如果等待响应超时，那么生产者要么重试发送数据，要么返回一个错误（抛出异常或执行回调）。timeout.ms指定了broker 等待同步副本返回消息确认的时间，与asks 的配置相匹配一一如果在指定时间内没有收到同步副本的确认，那么broker 就会返回一个错误。</li>
<li>max.block.ms<br>该参数指定了在调用send （） 方法或使用partitionsFor（）方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到max.block.ms时，生产者会抛出超时异常。</li>
<li>max.request.size<br>该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为1MB ，那么可以发送的单个最大消息为1MB ，或者生产者可以在单个请求里发送一个批次，该批次包含了1000 个消息，每个消息大小为1 KB。另外， broker 对可接收的消息最大值也有自己的限制（message.max.bytes )，所以两边的配置最好可以匹配，避免生产者发送的消息被broker 拒绝。</li>
<li>receive.buffer. bytes 和send . buffer.bytes<br>这两个参数分别指定了TCP socket 接收和发送数据包的缓冲区大小。如果它们被设为－ 1 ,就使用操作系统的默认值。如果生产者或消费者与broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。</li>
</ul>
<p>下面的代码片段演示如何创建一个生产者，这里只指定了<strong>必要的属性</strong>，其他使用默认设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Properties pro = <span class="keyword">new</span> Properties();</span><br><span class="line">pro.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"broker1:9092,broker2:9092"</span>)</span><br><span class="line">pro.put(<span class="string">"key.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">pro.put(<span class="string">"value.serialization.StringSerializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">producer = <span class="keyword">new</span> KafkaProducer&lt;String,String&gt;(pro);</span><br></pre></td></tr></table></figure>
<p>实例完生产者对象后，接下来就可以开始发送消息了。发送消息有以下三种方式。</p>
<ul>
<li>发送并忘记（ fire- and-forget )<br>我们把消息发送给服务器，但井不关心它是否正常到达。大多数情况下，消息会正常到达，因为Kafka 是高可用的，而且生产者会自动尝试重发。不过，使用这种方式有时候也会丢失一些消息。</li>
<li>同步发送<br>我们使用send （） 方法发送消息， 它会返回一个Future 对象，调用get （） 方法进行等待，就可以知道悄息是否发送成功。</li>
<li>异步发送<br>我们调用send （） 方怯，并指定一个回调函数， 服务器在返回响应时调用该函数。在下面的几个例子中， 我们会介绍如何使用上述几种方式来发送消息，以及如何处理可能发生的异常情况。 </li>
</ul>
<h2 id="发送消息的三种方式实例"><a href="#发送消息的三种方式实例" class="headerlink" title="发送消息的三种方式实例"></a>发送消息的三种方式实例</h2><ul>
<li>并发并忘记</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.kafka.client;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line">public class KafkaProducerDemo &#123;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author:落墨</span><br><span class="line"> * @CreateTime:2020&#x2F;7&#x2F;1</span><br><span class="line"> * @Description:KafkaProducer</span><br><span class="line"> *&#x2F;</span><br><span class="line">    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line">        Properties kafkaPropertie &#x3D; new Properties();</span><br><span class="line">        &#x2F;&#x2F;配置broker地址，配置多个容错</span><br><span class="line">        kafkaPropertie.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node1:9093,node1:9094&quot;);</span><br><span class="line">        &#x2F;&#x2F;配置key-value允许使用参数化类型</span><br><span class="line">        kafkaPropertie.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        kafkaPropertie.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaProducer kafkaProducer &#x3D; new KafkaProducer(kafkaPropertie);</span><br><span class="line"></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;String, String&gt;(&quot;testTopic&quot;,&quot;key1&quot;,&quot;hello world&quot;);</span><br><span class="line"></span><br><span class="line">        kafkaProducer.send(record);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>同步发送消息</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kafka.client;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>:落墨</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@CreateTime</span>:2020/7/1</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:KafkaProducer</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties kafkaPropertie = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//配置broker地址，配置多个容错</span></span><br><span class="line">        kafkaPropertie.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"node1:9092,node1:9093,node1:9094"</span>);</span><br><span class="line">        <span class="comment">//配置key-value允许使用参数化类型</span></span><br><span class="line">        kafkaPropertie.put(<span class="string">"key.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        kafkaPropertie.put(<span class="string">"value.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer kafkaProducer = <span class="keyword">new</span> KafkaProducer(kafkaPropertie);</span><br><span class="line">        <span class="comment">//创建消息对象，第一个为参数topic,第二个参数为key,第三个参数为value</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"testTopic"</span>,<span class="string">"key1"</span>,<span class="string">"hello world"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//同步发送方式,get方法返回结果</span></span><br><span class="line">        RecordMetadata metadata = (RecordMetadata) kafkaProducer.send(record).get();</span><br><span class="line">        System.out.println(<span class="string">"broker返回消息发送信息"</span> + metadata);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>异步发送消息</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.kafka.client;</span><br><span class="line">import org.apache.kafka.clients.producer.Callback;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author:落墨</span><br><span class="line"> * @CreateTime:2020&#x2F;7&#x2F;1</span><br><span class="line"> * @Description:KafkaProducer</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class KafkaProducerDemo &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties kafkaPropertie &#x3D; new Properties();</span><br><span class="line">        &#x2F;&#x2F;配置broker地址，配置多个容错</span><br><span class="line">        kafkaPropertie.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node1:9093,node1:9094&quot;);</span><br><span class="line">        &#x2F;&#x2F;配置key-value允许使用参数化类型</span><br><span class="line">        kafkaPropertie.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        kafkaPropertie.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaProducer kafkaProducer &#x3D; new KafkaProducer(kafkaPropertie);</span><br><span class="line">        &#x2F;&#x2F;创建消息对象，第一个为参数topic,第二个参数为key,第三个参数为value</span><br><span class="line">        final ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;String, String&gt;(&quot;testTopic&quot;,&quot;key1&quot;,&quot;hello world&quot;);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;异步发送消息。异常时打印异常信息或发送结果</span><br><span class="line">        kafkaProducer.send(record, new Callback() &#123;</span><br><span class="line">            public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123;</span><br><span class="line">                if (e !&#x3D; null) &#123;</span><br><span class="line">                    System.out.println(e.getMessage());</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    System.out.println(&quot;接收到返回结果：&quot; + recordMetadata);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        &#x2F;&#x2F;异步发送消息时必须要flush,否则发送不成功，不会执行回调函数</span><br><span class="line">        kafkaProducer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Producer-性能调优"><a href="#Producer-性能调优" class="headerlink" title="Producer 性能调优"></a>Producer 性能调优</h1><h2 id="1-一段Kafka生产端的实例代码"><a href="#1-一段Kafka生产端的实例代码" class="headerlink" title="1.一段Kafka生产端的实例代码"></a>1.一段Kafka生产端的实例代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Properties props &#x3D; new Properties();</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 67108864);</span><br><span class="line">props.put(&quot;batch.size&quot;, 131072);</span><br><span class="line">props.put(&quot;linger.ms&quot;, 100);</span><br><span class="line">props.put(&quot;max.request.size&quot;, 10485760);</span><br><span class="line">props.put(&quot;acks&quot;, &quot;1&quot;);</span><br><span class="line">props.put(&quot;retries&quot;, 10);</span><br><span class="line">props.put(&quot;retry.backoff.ms&quot;, 500);</span><br><span class="line">KafkaProducer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;String, String&gt;(props);</span><br></pre></td></tr></table></figure>
<h2 id="2-内存缓冲的大小"><a href="#2-内存缓冲的大小" class="headerlink" title="2.内存缓冲的大小"></a>2.内存缓冲的大小</h2><p>首先我们看看“buffer.memory”这个参数是什么意思？</p>
<p>Kafka的客户端发送数据到服务器，一般都是要经过缓冲的，也就是说，你通过KafkaProducer发送出去的消息都是先进入到客户端本地的内存缓冲里，然后把很多消息收集成一个一个的Batch，再发送到Broker上去的。</p>
<p><img src="/2020/06/30/Kafka-producer/2.png" alt></p>
<p>所以这个“buffer.memory”的本质就是用来约束KafkaProducer能够使用的内存缓冲的大小的，他的默认值是32MB。</p>
<p>那么既然了解了这个含义，大家想一下，在生产项目里，这个参数应该怎么来设置呢？</p>
<p>你可以先想一下，如果这个内存缓冲设置的过小的话，可能会导致一个什么问题？</p>
<p>首先要明确一点，那就是在内存缓冲里大量的消息会缓冲在里面，形成一个一个的Batch，每个Batch里包含多条消息。</p>
<p>然后KafkaProducer有一个Sender线程会把多个Batch打包成一个Request发送到Kafka服务器上去。</p>
<p><img src="/2020/06/30/Kafka-producer/3.png" alt></p>
<p>那么如果要是内存设置的太小，可能<strong>导致一个问题</strong>：消息快速的写入内存缓冲里面，但是Sender线程来不及把Request发送到Kafka服务器。</p>
<p>这样是不是会造成内存缓冲很快就被写满？一旦被写满，就会阻塞用户线程，不让继续往Kafka写消息了。</p>
<p>所以对于“buffer.memory”这个参数应该结合自己的实际情况来进行压测，你需要测算一下在生产环境，你的用户线程会以每秒多少消息的频率来写入内存缓冲。</p>
<p>比如说每秒300条消息，那么你就需要压测一下，假设内存缓冲就32MB，每秒写300条消息到内存缓冲，是否会经常把内存缓冲写满？经过这样的压测，你可以调试出来一个合理的内存大小。</p>
<h2 id="3-多少数据打包为一个Batch合适？"><a href="#3-多少数据打包为一个Batch合适？" class="headerlink" title="3.多少数据打包为一个Batch合适？"></a>3.多少数据打包为一个Batch合适？</h2><p>接着你需要思考第二个问题，就是你的“batch.size”应该如何设置？这个东西是决定了你的每个Batch要存放多少数据就可以发送出去了。</p>
<p>比如说你要是给一个Batch设置成是16KB的大小，那么里面凑够16KB的数据就可以发送了。</p>
<p>这个参数的默认值是16KB，一般可以尝试把这个参数调节大一些，然后利用自己的生产环境发消息的负载来测试一下。</p>
<p>比如说发送消息的频率就是每秒300条，那么如果比如“batch.size”调节到了32KB，或者64KB，是否可以提升发送消息的整体吞吐量。</p>
<p>因为理论上来说，提升batch的大小，可以允许更多的数据缓冲在里面，那么一次Request发送出去的数据量就更多了，这样吞吐量可能会有所提升。</p>
<p>但是这个东西也不能无限的大，过于大了之后，要是数据老是缓冲在Batch里迟迟不发送出去，那么岂不是你发送消息的延迟就会很高。</p>
<p>比如说，一条消息进入了Batch，但是要等待5秒钟Batch才凑满了64KB，才能发送出去。那这条消息的延迟就是5秒钟。</p>
<p>所以需要在这里按照生产环境的发消息的速率，调节不同的Batch大小自己测试一下最终出去的吞吐量以及消息的 延迟，设置一个最合理的参数。</p>
<h2 id="4-要是一个Batch迟迟无法凑满怎么办？"><a href="#4-要是一个Batch迟迟无法凑满怎么办？" class="headerlink" title="4.要是一个Batch迟迟无法凑满怎么办？"></a>4.要是一个Batch迟迟无法凑满怎么办？</h2><p>要是一个Batch迟迟无法凑满，此时就需要引入另外一个参数了，“linger.ms”</p>
<p>他的含义就是说一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去了。</p>
<p>给大家举个例子，比如说batch.size是16kb，但是现在某个低峰时间段，发送消息很慢。</p>
<p>这就导致可能Batch被创建之后，陆陆续续有消息进来，但是迟迟无法凑够16KB，难道此时就一直等着吗？</p>
<p>当然不是，假设你现在设置“linger.ms”是50ms，那么只要这个Batch从创建开始到现在已经过了50ms了，哪怕他还没满16KB，也要发送他出去了。</p>
<p>所以“linger.ms”决定了你的消息一旦写入一个Batch，最多等待这么多时间，他一定会跟着Batch一起发送出去。</p>
<p>避免一个Batch迟迟凑不满，导致消息一直积压在内存里发送不出去的情况。<strong>这是一个很关键的参数。</strong></p>
<p>这个参数一般要非常慎重的来设置，要配合batch.size一起来设置。</p>
<p>举个例子，首先假设你的Batch是32KB，那么你得估算一下，正常情况下，一般多久会凑够一个Batch，比如正常来说可能20ms就会凑够一个Batch。</p>
<p>那么你的linger.ms就可以设置为25ms，也就是说，正常来说，大部分的Batch在20ms内都会凑满，但是你的linger.ms可以保证，哪怕遇到低峰时期，20ms凑不满一个Batch，还是会在25ms之后强制Batch发送出去。</p>
<p>如果要是你把linger.ms设置的太小了，比如说默认就是0ms，或者你设置个5ms，那可能导致你的Batch虽然设置了32KB，但是经常是还没凑够32KB的数据，5ms之后就直接强制Batch发送出去，这样也不太好其实，会导致你的Batch形同虚设，一直凑不满数据。</p>
<h2 id="5-最大请求大小"><a href="#5-最大请求大小" class="headerlink" title="5.最大请求大小"></a>5.最大请求大小</h2><p>“max.request.size”这个参数决定了每次发送给Kafka服务器请求的最大大小，同时也会限制你一条消息的最大大小也不能超过这个参数设置的值，这个其实可以根据你自己的消息的大小来灵活的调整。</p>
<p>给大家举个例子，你们公司发送的消息都是那种大的报文消息，每条消息都是很多的数据，一条消息可能都要20KB。</p>
<p>此时你的batch.size是不是就需要调节大一些？比如设置个512KB？然后你的buffer.memory是不是要给的大一些？比如设置个128MB？</p>
<p>只有这样，才能让你在大消息的场景下，还能使用Batch打包多条消息的机制。但是此时“max.request.size”是不是也得同步增加？</p>
<p>因为可能你的一个请求是很大的，默认他是1MB，你是不是可以适当调大一些，比如调节到5MB？</p>
<h2 id="6-重试机制"><a href="#6-重试机制" class="headerlink" title="6.重试机制"></a>6.重试机制</h2><p>“retries”和“retries.backoff.ms”决定了重试机制，也就是如果一个请求失败了可以重试几次，每次重试的间隔是多少毫秒。</p>
<p>这个大家适当设置几次重试的机会，给一定的重试间隔即可，比如给100ms的重试间隔。</p>
<h2 id="7-持久化机制"><a href="#7-持久化机制" class="headerlink" title="7.持久化机制"></a>7.持久化机制</h2><p>“acks”参数决定了发送出去的消息要采用什么样的持久化策略，这个涉及到了很多其他的概念，大家可以参考之前专门为“acks”写过的一篇文章：</p>
<p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247485069&amp;idx=1&amp;sn=abd8ddc19fbb49c11c8ad9e1ffdff147&amp;chksm=fba6ee8eccd1679874d5bd19109ed8cb3daadf59dabe67c906f8caed0714ba2dda16f5a032ce&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《简历写了会Kafka，面试官90%会让你讲讲acks参数对消息持久化的影响》</a>。</p>
<p><strong>参考文章</strong></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247485210&amp;idx=1&amp;sn=abb9930b99c054d5bd56519fa5896b42&amp;chksm=fba6ef19ccd1660f2ac0f4127e6823ab02a4607be56a55c91901a2e4fed831f49145d2d8e7c4&amp;mpshare=1&amp;scene=24&amp;srcid=#rd" target="_blank" rel="noopener">Kafka参数调优实战</a></p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Java虚拟机运行时数据区域</title>
    <url>/2019/12/02/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>&emsp;&emsp;对于从事C、C++程序开发人员来说，在内存管理领域，他们既是拥有最高权力的“皇帝”又是从事最基础工作的“劳动人民”-既拥有每一个对象的“所有权”，又担负着每一个对象生命开始到终结的维护责任。<br>对于Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不在需要为每一个new操作去写配对的delete/free代码，不容易出现内存泄漏和内存溢出问题，由虚拟机管理内存这一切看起来都很美好。然而一旦出现内存泄漏和溢出的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将异常艰难。Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的<br>用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而<br>建立和销毁。</p>
<h2 id="运行时数据区域"><a href="#运行时数据区域" class="headerlink" title="运行时数据区域"></a>运行时数据区域</h2><p>&emsp;&emsp;Java 虚拟机在执行java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间。<br><img src="/2019/12/02/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/JVM.png" alt="图片说明"> </p>
<h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>&emsp;&emsp;程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环跳转、异常处理、线程恢复等基础功能都需要依赖这个技术器来完成。</p>
<h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>&emsp;&emsp;与程序计数器一样，Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的时Java方法执行的内存模型：每个方法在执行的相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个放法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈出栈的过程。  </p>
<h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>&emsp;&emsp;本地方法栈与虚拟机栈所发挥的作用是非常相似的，他们之间的区别不过是虚拟机栈为虚拟机执行方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native法服务。在虚拟机规范中对被地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutofMemoryError异常。</p>
<h3 id="Java堆"><a href="#Java堆" class="headerlink" title="Java堆"></a>Java堆</h3><p>&emsp;&emsp;对于大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有对象实例以及数组都要在堆上分配,但是随着JIT<br>编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换[2]优化技术将会导致一些微妙的变化发生，所有的<br>对象都分配在堆上也渐渐变得不是那么“绝对”了。</p>
<h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p> &emsp;&emsp;   方法区与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名Non-Heap（非堆），目的应该是与Java堆区分开来</p>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>运行时数据区域</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 概述</title>
    <url>/2019/09/20/Kafka-%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="kafka诞生的背景"><a href="#kafka诞生的背景" class="headerlink" title="kafka诞生的背景"></a>kafka诞生的背景</h1><p>对于一个高效的组织，所有数据需要对该组织的所有服务和系统是可用的，以便挖掘出数据的最大价值。数据采集和数据使用是一个金字塔的结构，底部为以某种统一的方式捕获数据，这些数据需要以统一的方式建模，以方便读取和处理。捕获数据的工作做扎实后，在这个基础上以不同方法处理这些数据就变得得心应手。<br>数据捕获的来源主要有两种：一种是记录正在发生的事件数据。比如Web系统中的用户活动日志（用户的点击选择等）、交警行业中的违章事件等。随着传统行业业务活动的数字化，事件数据正在不断增长，而且这个趋势没有停止。这种类型的事件数据记录了已经发生的事情，往往比传统数据库应用要大好几个数量级。因此对于数据的捕获、数据的处理提出了重大的挑战；另一种是经过二次分析处理之后的数据。对捕获的数据进行二次分析处理后得到的数据也需要记录保存，这里的处理指的是利用批处理、图分析等专有的数据处理系统进行了处理，这些加工后的数据可以作为数据捕获的第二个来源。总之，捕获的数据越来越多，如何将这些巨量的数据以可靠的、完整的数据流方式传递给数据分析处理系统也变得越来越困难。</p>
<h1 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h1><p>在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。<br>1）Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。<br>2）Kafka最初是由LinkedIn公司开发，并于    2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。<br>3）Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。<br>4）无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。</p>
<h1 id="消息队列内部实现原理"><a href="#消息队列内部实现原理" class="headerlink" title="消息队列内部实现原理"></a>消息队列内部实现原理</h1><p><img src="https://uploadfiles.nowcoder.com/images/20190920/9094293_1568975462389_04F96407B00295C6D53D599B4F14DC90" alt="图片说明" title="图片标题"> </p>
<p>  （1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）<br>点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。<br>（2）发布/订阅模式（一对多，数据生产后，推送给所有订阅者）<br>发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。</p>
<h1 id="为什么需要消息队列"><a href="#为什么需要消息队列" class="headerlink" title="为什么需要消息队列"></a>为什么需要消息队列</h1><p>1）解耦：<br>　　允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。<br>2）冗余：<br>消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。<br>3）扩展性：<br>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。<br>4）灵活性 &amp; 峰值处理能力：<br>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。<br>5）可恢复性：<br>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。<br>6）顺序保证：<br>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）<br>7）缓冲：<br>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。<br>8）异步通信：<br>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p>
<h1 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h1><p><img src="https://uploadfiles.nowcoder.com/images/20190920/9094293_1568975497955_62B3F472CB3D859915FB4C3B0BBEBAAE" alt="图片说明" title="图片标题"><br>1）Producer ：消息生产者，就是向kafka broker发消息的客户端。<br>2）Consumer ：消息消费者，向kafka broker取消息的客户端<br>3）Topic ：可以理解为一个队列。<br>4） Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制-给consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。<br>5）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。<br>6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。<br>7）Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</p>
<h1 id="分布式模型"><a href="#分布式模型" class="headerlink" title="分布式模型"></a>分布式模型</h1><p>   Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本）。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本出现故障时，备份副本中的一个副本会被选择为新的主副本。因为每个分区的副本中只有主副本接受读写，所以每个服务器端都会作为某些分区的主副本，以及另外一些分区的备份副本，这样Kafka集群的所有服务端整体上对客户端是负载均衡的。<br>       Kafka的生产者和消费者相对于服务器端而言都是客户端。<br>Kafka生产者客户端发布消息到服务端的指定主题，会指定消息所属的分区。生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡；消息有键时，根据分区语义（例如hash）确保相同键的消息总是发送到同一分区。<br>       Kafka的消费者通过订阅主题来消费消息，并且每个消费者都会设置一个消费组名称。因为生产者发布到主题的每一条消息都只会发送给消费者组的一个消费者。所以，如果要实现传统消息系统的“队列”模型，可以让每个消费者都拥有相同的消费组名称，这样消息就会负责均衡到所有的消费者；如果要实现“发布-订阅”模型，则每个消费者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。<br>       分区是消费者现场模型的最小并行单位。如下图（图1）所示，生产者发布消息到一台服务器的3个分区时，只有一个消费者消费所有的3个分区。在下图（图2）中，3个分区分布在3台服务器上，同时有3个消费者分别消费不同的分区。假设每个服务器的吞吐量时300MB，在下图（图1）中分摊到每个分区只有100MB，而在下图（图2）中，集群整体的吞吐量有900MB。可以看到，增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。<br>       同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，这样每个消费者都可以分配到数量均等的分区。Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费者组，或者有消费者离开消费组，都会触发再平衡操作。<br><img src="https://uploadfiles.nowcoder.com/images/20190920/9094293_1568975534470_558AC9E4B632C16EBB53304E1BB3518A" alt="图片说明" title="图片标题"><br>Kafka的消费者消费消息时，只保证在一个分区内的消息的完全有序性，并不保证同一个主题汇中多个分区的消息顺序。而且，消费者读取一个分区消息的顺序和生产者写入到这个分区的顺序是一致的。比如，生产者写入“hello”和“Kafka”两条消息到分区P1，则消费者读取到的顺序也一定是“hello”和“Kafka”。如果业务上需要保证所有消息完全一致，只能通过设置一个分区完成，但这种做法的缺点是最多只能有一个消费者进行消费。一般来说，只需要保证每个分区的有序性，再对消息假设键来保证相同键的所有消息落入同一分区，就可以满足绝大多数的应用。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala</title>
    <url>/2020/04/27/Scala/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言<br>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。<br>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。Scala 运行在Java虚拟机上，并兼容现有的Java程序。Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库。</p>
<h1 id="Scala特性："><a href="#Scala特性：" class="headerlink" title="Scala特性："></a>Scala特性：</h1><p><strong>面向对象特性：</strong><br>Scala是一种纯面向对象的语言，每个值都是对象。每个方法都是调用。举例来说，如果你执行 <code>1+2</code>，则对于 Scala 而言，实际是在调用 Int 类里定义的名为 <code>+</code> 的方法。<br><strong>函数式编程：</strong><br>Scala也是一种函数式语言，奇函数也能成值来使用。Scala提供了轻量级的语法用以定义匿名函数，支持高级函数，并支持柯里化。Scala的case class及内置的模式匹配相当于函数式编程语言中常用的代数类型。</p>
<p>我们通过函数式编程来实现数组翻倍的例子</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val nums = Array(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">val doubleNums = nums.map(_*<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>利用Array的map函数，将所有元素2，并生成了一个新的双倍数组。map函数所做的事情是把遍历整个数组的过程，归纳并抽离出来，让我们专注于描述我们想要的是什么”<strong>_*2*</strong>“。我们传入map的是一个纯函数；它不具有任何副作用(不会改变外部状态)，它只是接收一个数字，返回*2后的值。</p>
<p>正如<strong><em>封装、继承\</em></strong>和<strong><em>多态\</em></strong>是面向对象编程的三大特性。函数式编程也有自己的语言特性<strong>数据不可变、函数是第一公民、引用透明</strong>和<strong>尾递归</strong></p>
<p><strong>数据不可变（immutable data）</strong>：变量只赋值一次，如果想改变其值就创建一个新的。</p>
<p><strong>函数是第一公民（first class method）</strong>:函数可以像普通变量一样去使用。函数可以像变量一样被创建，修改，并当成变量一样传递，返回或是在函数中嵌套函数。</p>
<p><strong>引用透明(referential transparency)</strong>： 指的是函数的运行不依赖于外部变量或“状态”，只依赖于输入的参数，任何时候只要参数相同，调用函数所得到的返回值总是相同的。天然适应并发编程，因为调用函数的结果具有一致性，所以根本不需要加锁，也就不存在死锁的问题。</p>
<p><strong>尾递归（tail call optimization）</strong>:函数调用要压栈保存现场，递归层次过深的话，压栈过多会产生性能问题。所以引入尾递归优化，每次递归时都会重用栈，提升性能。</p>
<h1 id="Scala安装"><a href="#Scala安装" class="headerlink" title="Scala安装"></a>Scala安装</h1><h2 id="Windows-上安装Scala"><a href="#Windows-上安装Scala" class="headerlink" title="Windows 上安装Scala"></a>Windows 上安装Scala</h2><p>Scala 语言可以运行在Window、Linux、Unix、 Mac OS X等系统上。</p>
<p>Scala是基于java之上，大量使用java的类库和变量，<strong>使用 Scala 之前必须先安装 Java（&gt;1.5版本）。</strong></p>
<p>这里就不介绍Java环境配置，如果还未安装可以参考相关的博客</p>
<p>接下来，我们从<a href="http://www.scala-lang.org/downloads" target="_blank" rel="noopener">Scala官网</a></p>
<p><img src="/2020/04/27/Scala/1.png" alt></p>
<p>下载后，双击 msi 文件，一步步安装即可，安装过程你可以使用默认的安装目录。</p>
<p>安装好scala后，系统会自动提示，单击 finish，完成安装。</p>
<p>右击我的电脑，单击”属性”，进入如图所示页面。下面开始配置环境变量，右击【我的电脑】–【属性】–【高级系统设置】–【环境变量】，如图：</p>
<p><img src="/2020/04/27/Scala/2.png" alt></p>
<p>设置 SCALA_HOME 变量：单击新建，在变量名栏输入：<strong>SCALA_HOME</strong>: 变量值一栏输入：E:\02_software\scala-2.11.11 也就是 Scala 的安装目录，根据个人情况有所不同</p>
<p><img src="/2020/04/27/Scala/3.png" alt></p>
<p>设置 Path 变量：找到系统变量下的”Path”如图，单击编辑。在”变量值”一栏的最前面添加如下的路径： %SCALA_HOME%\bin;%SCALA_HOME%\jre\bin;</p>
<p><strong>注意：</strong>后面的分号 <strong>；</strong> 不要漏掉。</p>
<p><img src="/2020/04/27/Scala/4.png" alt></p>
<p>设置 Classpath 变量：找到找到系统变量下的”Classpath”如图，单击编辑，如没有，则单击”新建”:</p>
<ul>
<li>“变量名”：ClassPath</li>
<li>“变量值”：.;%SCALA_HOME%\bin;%SCALA_HOME%\lib\dt.jar;%SCALA_HOME%\lib\tools.jar.;</li>
</ul>
<p><strong>注意：</strong>“变量值”最前面的 .; 不要漏掉。最后单击确定即可。</p>
<p><img src="/2020/04/27/Scala/5.png" alt></p>
<p>检查环境变量是否设置好了：调出”cmd”检查。单击 【开始】，在输入框中输入cmd，然后”回车”，输入 scala，然后回车，如环境变量设置ok，你应该能看到这些信息。</p>
<p><img src="/2020/04/27/Scala/6.png" alt></p>
]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka实现高吞吐之零拷贝</title>
    <url>/2019/12/09/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Kafka是一个非常优秀的消息开源系统，作为分布式的消息队列之所以能够实现高吞吐，其中的一个原因就是sendFile 的零拷贝</p>
<h2 id="关于零拷贝"><a href="#关于零拷贝" class="headerlink" title="关于零拷贝"></a>关于零拷贝</h2><p>&quot;零拷贝&quot;中的&quot;拷贝&quot;是操作系统在I/O操作中,将数据从一个内存区域复制到另外一个内存区域. 而&quot;零&quot;并不是指0次复制, 更多的是指在用户态和内核态之前的复制是0次.</p>
<h2 id="CPU-COPY"><a href="#CPU-COPY" class="headerlink" title="CPU COPY"></a>CPU COPY</h2><p>通过计算机的组成原理我们知道, 内存的读写操作是需要CPU的协调数据总线,地址总线和控制总线来完成的<br>因此在&quot;拷贝&quot;发生的时候,往往需要CPU暂停现有的处理逻辑,来协助内存的读写.这种我们称为CPU COPY，CPU COPY不但占用了CPU资源,还占用了总线的带宽.</p>
<h2 id="DMA-COPY"><a href="#DMA-COPY" class="headerlink" title="DMA COPY"></a>DMA COPY</h2><p>DMA(DIRECT MEMORY ACCESS)是现代计算机的重要功能. 它的一个重要 的特点就是, 当需要与外设进行数据交换时, CPU只需要初始化这个动作便可以继续执行其他指令,剩下的数据传输的动作完全由DMA来完成，可以看到DMA COPY是可以避免大量的CPU中断的</p>
<h2 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h2><p>本文中的上下文切换时指由用户态切换到内核态, 以及由内核态切换到用户态<br>存在多次拷贝的原因？</p>
<ul>
<li><p>操作系统为了保护系统不被应用程序有意或无意地破坏,为操作系统设置了用户态和内核态两种状态.用户态想要获取系统资源(例如访问硬盘), 必须通过系统调用进入到内核态, 由内核态获取到系统资源,再切换回用户态返回应用程序.</p>
</li>
<li><p>出于&quot;readahead cache&quot;和异步写入等等性能优化的需要, 操作系统在内核态中也增加了一个&quot;内核缓冲区&quot;(kernel buffer). 读取数据时并不是直接把数据读取到应用程序的buffer, 而先读取到kernel buffer, 再由kernel buffer复制到应用程序的buffer. 因此,数据在被应用程序使用之前,可能需要被多次拷贝  </p>
</li>
</ul>
<p>所有<strong>涉及到数据传输</strong>的场景, 无非就一种:从硬盘上读取文件数据, 发送到网络上去。<br>这个场景我们简化为一个模型:</p>
<p> File.read(fileDesc, buf, len);<br> Socket.send(socket, buf, len);<br>为了方便描述,上面这两行代码, 我们给它起个名字: read-send模型</p>
<p>操作系统在实现这个read-send模型时,需要有以下步骤:</p>
<ol>
<li>应用程序开始读文件的操作</li>
<li>应用程序发起系统调用, 从用户态切换到内核态(第一次上下文切换)</li>
<li>内核态中把数据从硬盘文件读取到内核中间缓冲区(kernel buf)</li>
<li>数据从内核中间缓冲区(kernel buf)复制到(用户态)应用程序缓冲区(app buf),从内核态切换回到用户态(第二次上下文切换)</li>
<li>应用程序开始发送数据到网络上</li>
<li>应用程序发起系统调用,从用户态切换到内核态(第三次上下文切换)</li>
<li>内核中把数据从应用程序(app buf)的缓冲区复制到socket的缓冲区(socket)</li>
<li>内核中再把数据从socket的缓冲区(socket buf)发送的网卡的缓冲区(NIC buf)上</li>
<li>从内核态切换回到用户态(第四次上下文切换)</li>
</ol>
<p><img src="/2019/12/09/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/kafka.png" alt="图片说明"> </p>
<p>由上图可以很清晰地看到, 一次read-send涉及到了四次拷贝:</p>
<ol>
<li>硬盘拷贝到内核缓冲区(DMA COPY)</li>
<li>内核缓冲区拷贝到应用程序缓冲区(CPU COPY)</li>
<li>应用程序缓冲区拷贝到socket缓冲区(CPU COPY)</li>
<li>socket buf拷贝到网卡的buf(DMA COPY)  </li>
</ol>
<p>其中涉及到2次cpu中断, 还有4次的上下文切换，很明显,第2次和第3次的的copy只是把数据复制到app buffer又原封不动的复制回来, 为此带来了两次的cpu copy和两次上下文切换, 是完全没有必要的，linux的零拷贝技术就是为了优化掉这两次不必要的拷贝</p>
<h2 id="sendFile"><a href="#sendFile" class="headerlink" title="sendFile"></a>sendFile</h2><p>linux内核2.1开始引入一个叫sendFile系统调用,这个系统调用可以在内核态内把数据从内核缓冲区直接复制到套接字(SOCKET)缓冲区内, 从而可以减少上下文的切换和不必要数据的复制<br>有了sendFile这个系统调用后, 我们read-send模型就可以简化为:</p>
<ol>
<li>应用程序开始读文件的操作</li>
<li>应用程序发起系统调用, 从用户态切换到内核态(第一次上下文切换)</li>
<li>内核态中把数据从硬盘文件读取到内核中间缓冲区</li>
<li>通过sendFile,在内核态中把数据从内核缓冲区复制到socket的缓冲区</li>
<li>内核中再把数据从socket的缓冲区发送的网卡的buf上</li>
<li>从内核态切换到用户态(第二次上下文切换)  </li>
</ol>
<p><img src="/2019/12/09/Kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E4%B9%8B%E9%9B%B6%E6%8B%B7%E8%B4%9D/kafka1.png" alt="图片说明"><br>如图所示：<br>涉及到数据拷贝变成:</p>
<ol>
<li>硬盘拷贝到内核缓冲区(DMA COPY)</li>
<li>内核缓冲区拷贝到socket缓冲区(CPU COPY)</li>
<li>socket缓冲区拷贝到网卡的buf(DMA COPY)</li>
</ol>
<p>可以看到,一次read-send模型中, 利用sendFile系统调用后, 可以将4次数据拷贝减少到3次, 4次上下文切换减少到2次, 2次CPU中断减少到1次，<br>相对传统I/O, 这种零拷贝技术通过减少两次上下文切换, 1次cpu copy, <strong>可以将I/O性能提高50%以上(网络数据, 未亲测)</strong>，开始的术语中说到, 所谓的<strong>零拷贝的&quot;零&quot;</strong>, <strong>是指用户态和内核态之间的拷贝次数为0</strong>, 从这个定义上来说, 现在的这个零拷贝技术已经是真正的&quot;零&quot;了</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>零拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Eagle</title>
    <url>/2020/07/03/Kafka-Eagle/</url>
    <content><![CDATA[<h1 id="kafka-Eagle"><a href="#kafka-Eagle" class="headerlink" title="kafka Eagle"></a>kafka Eagle</h1><p>Kafka Eagle是开源的可视化和管理软件。它使您可以查询，可视化，警告和浏览指标，无论它们存储在哪里。它为您提供了将kafka群集数据转换为漂亮的图形和可视化效果的工具。</p>
<h1 id="安装Kafka-Eagle"><a href="#安装Kafka-Eagle" class="headerlink" title="安装Kafka Eagle"></a>安装Kafka Eagle</h1><h2 id="1-下载并安装"><a href="#1-下载并安装" class="headerlink" title="1.下载并安装"></a>1.下载并安装</h2><p>您可以在GitHub上下载<a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">Kafka Eagle</a>源代码以自己编译或安装，也可以<a href="http://download.kafka-eagle.org/" target="_blank" rel="noopener">下载</a>二进制.tar.gz文件。</p>
<ul>
<li><a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">Github</a> </li>
<li><a href="http://download.kafka-eagle.org/" target="_blank" rel="noopener">下载</a></li>
</ul>
<blockquote>
<p>安装注意事项：</p>
<p>我们建议使用官方编译的二进制安装包。</p>
</blockquote>
<h2 id="2-安装JDK"><a href="#2-安装JDK" class="headerlink" title="2.安装JDK"></a>2.安装JDK</h2><p>如果Linux服务器上存在JDK环境，则可以忽略此步骤，并安装后续步骤。如果没有JDK，参考网上博客安装JDK。</p>
<h2 id="3-解压Kafka"><a href="#3-解压Kafka" class="headerlink" title="3.解压Kafka"></a>3.解压Kafka</h2><p>将文件解压至相关目录</p>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka-eagle-xxx-bin.tar.gz</span><br></pre></td></tr></table></figure>
</blockquote>
<p>如果您以前安装过该版本，请删除修改后的版本，然后重命名当前版本，如下所示：</p>
<blockquote>
<p>rm -rf kafka-eagle</p>
<p>mv kafka-eagle-xxx kafka-eagle</p>
</blockquote>
<p>然后，配置Kafka Eagle 配置文件</p>
<blockquote>
<p>vi /etc/profile</p>
<p>export KE_HOME = /data/soft/new/kafka-eagle</p>
<p>export PATH=$PATH:$KE_HOME/bin</p>
</blockquote>
<p>最后，我们使用 .    /etc/profile是配置立即生效。</p>
<h2 id="4-配置Kafka-Eagle-system-file"><a href="#4-配置Kafka-Eagle-system-file" class="headerlink" title="4.配置Kafka Eagle system file"></a>4.配置Kafka Eagle system file</h2><p>根据自己的Kafka集群的实际情况配置Kafka Eagle，例如Zookeeper地址，Kafka集群的版本类型（低版本为zk，高版本为kafka），启用了安全认证的Kafka集群等。</p>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">#修改配置文件</span><br><span class="line">vim system-config.properties</span><br><span class="line">#配置文件详情介绍</span><br><span class="line">#kafkazookeeper节点配置属性多个可以添加一个，cluster1 </span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=<span class="number">192.168</span><span class="number">.163</span><span class="number">.131</span>:<span class="number">2181</span></span><br><span class="line">######################################</span><br><span class="line"># zk 线程数量</span><br><span class="line">######################################</span><br><span class="line">kafka.zk.limit.size=<span class="number">25</span></span><br><span class="line"> </span><br><span class="line">######################################</span><br><span class="line"># kafka eagle 的端口</span><br><span class="line">######################################</span><br><span class="line">kafka.eagle.webui.port=<span class="number">8048</span></span><br><span class="line"> </span><br><span class="line">######################################</span><br><span class="line"># kafka offset storage</span><br><span class="line">######################################</span><br><span class="line">cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line"> </span><br><span class="line">######################################</span><br><span class="line"># enable kafka 开启图表</span><br><span class="line"># 及开始sql查询</span><br><span class="line">######################################</span><br><span class="line">kafka.eagle.metrics.charts=<span class="keyword">true</span></span><br><span class="line"> </span><br><span class="line">kafka.eagle.sql.fix.error=<span class="keyword">true</span></span><br><span class="line">######################################</span><br><span class="line"># 提醒的email</span><br><span class="line">######################################</span><br><span class="line">kafka.eagle.mail.enable=<span class="keyword">true</span></span><br><span class="line">kafka.eagle.mail.sa=alert_sa</span><br><span class="line">kafka.eagle.mail.username=alert_sa@<span class="number">163</span>.com</span><br><span class="line">kafka.eagle.mail.password=mqslimczkdqabbbh</span><br><span class="line">kafka.eagle.mail.server.host=smtp<span class="number">.163</span>.com</span><br><span class="line">kafka.eagle.mail.server.port=<span class="number">25</span></span><br><span class="line"> </span><br><span class="line">######################################</span><br><span class="line"># 删除kafka topic 的token</span><br><span class="line">######################################</span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"> </span><br><span class="line">######################################</span><br><span class="line"># kafka sasl authenticate</span><br><span class="line">######################################</span><br><span class="line">kafka.eagle.sasl.enable=<span class="keyword">false</span></span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line"> </span><br><span class="line">######################################</span><br><span class="line"># kafka jdbc 地址注意可以自己安装数据mysql也可以自带的</span><br><span class="line">######################################</span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/kafka/kafka-eagle-bin-<span class="number">1.2</span><span class="number">.4</span>/kafka-eagle-web-<span class="number">1.2</span><span class="number">.4</span>/db/ke.db</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="5-启动Kafka-Eagle服务器"><a href="#5-启动Kafka-Eagle服务器" class="headerlink" title="5.启动Kafka Eagle服务器"></a>5.启动Kafka Eagle服务器</h2><p>在<code>$KE_HOME/bin</code>目录中，有一个<code>ke.sh</code>脚本文件。执行以下启动命令：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd $&#123;KE_HOME&#125;&#x2F;bin</span><br><span class="line">chmod +x ke.sh </span><br><span class="line">.&#x2F;ke.sh start</span><br></pre></td></tr></table></figure>
<p>之后，当重新启动或停止Kafka Eagle服务器时，执行以下命令：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ke.sh restart</span><br><span class="line">ke.sh stop</span><br></pre></td></tr></table></figure>
<p>如图：</p>
<p><img src="/2020/07/03/Kafka-Eagle/1.png" alt></p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Eagle</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Demo(Serializable)</title>
    <url>/2019/10/29/Spark-Demo-Serializable/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本文中将介绍spark中Task执行序列化的开发问题</p>
<h1 id="开发环境准备"><a href="#开发环境准备" class="headerlink" title="开发环境准备"></a>开发环境准备</h1><p>本实验Spark运行在Windows上，为了开发Spark应用程序，在本地机器上需要有Jdk1.8和Maven环境。<br>确保我们的环境配置正常，我们可以使用快捷键 Win+R 输入cmd：<br>环境如下：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572341256892_8D8973B8667179A2319B041328F690BD" alt="图片说明" title="图片标题"><br>程序开发工具我们使用IDEA，创建maven项目，添加pom依赖</p>
<h1 id="编写Spark程序"><a href="#编写Spark程序" class="headerlink" title="编写Spark程序"></a>编写Spark程序</h1><h2 id="目录结构如下"><a href="#目录结构如下" class="headerlink" title="目录结构如下:"></a>目录结构如下:</h2><p><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572341490332_07386CADB58D4E8A80763A83F9F6B34D" alt="图片说明" title="图片标题"> </p>
<h2 id="创建Serializable-scala"><a href="#创建Serializable-scala" class="headerlink" title="创建Serializable.scala:"></a>创建Serializable.scala:</h2><p>首先我们需要了解RDD中的函数传递：<br>在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要主要的是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。<br>如果我们对我们自定义的类不进行序列化：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> SparkDemo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>: luomo</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>: 2019/10/29</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>: Serializable from Driver to Executor</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object Serializable &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val config:SparkConf =<span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Serializable"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(config)</span><br><span class="line"></span><br><span class="line">    val rdd:RDD[String] = sc.parallelize(Array(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"Flink"</span>))</span><br><span class="line"></span><br><span class="line">    val search = <span class="keyword">new</span> Search(<span class="string">"h"</span>)</span><br><span class="line"></span><br><span class="line">    val match1:RDD[String] =search.getMatch1(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  class Search(query:String)&#123;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">    <span class="function">def  <span class="title">isMatch</span><span class="params">(s:String)</span>:Boolean </span>=&#123;</span><br><span class="line">      s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatch1</span><span class="params">(rdd:RDD[String])</span> :RDD[String] </span>= &#123;</span><br><span class="line">      rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatche2</span><span class="params">(rdd: RDD[String])</span>: RDD[String] </span>=&#123;</span><br><span class="line">      rdd.filter(x=&gt; x.contains(query))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572346997711_619AA778B91D5D25CAB063D1C7D53FFE" alt="图片说明" title="图片标题"> </p>
<p>可见，对于自己定义的普通类，Spark是无法直接将其序列化的。<br>需要我们自定义的类继承java.io.Serializable</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> SparkDemo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Author</span>: luomo</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@CreateTime</span>: 2019/10/29</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span>: Serializable from Driver to Executor</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object Serializable &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val config:SparkConf =<span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Serializable"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Spark上下文对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(config)</span><br><span class="line"></span><br><span class="line">    val rdd:RDD[String] = sc.parallelize(Array(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"Flink"</span>))</span><br><span class="line"></span><br><span class="line">    val search = <span class="keyword">new</span> Search(<span class="string">"h"</span>)</span><br><span class="line"></span><br><span class="line">    val match1:RDD[String] =search.getMatch1(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//自定义类</span></span><br><span class="line">  class Search(query:String) extends  java.io.Serializable &#123;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">    <span class="function">def  <span class="title">isMatch</span><span class="params">(s:String)</span>:Boolean </span>=&#123;</span><br><span class="line">      s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatch1</span><span class="params">(rdd:RDD[String])</span> :RDD[String] </span>= &#123;</span><br><span class="line">      rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">    <span class="function">def <span class="title">getMatche2</span><span class="params">(rdd: RDD[String])</span>: RDD[String] </span>=&#123;</span><br><span class="line">      rdd.filter(x=&gt; x.contains(query))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h1><p>如图我们过滤出包含字符h的字符串：<br><img src="https://uploadfiles.nowcoder.com/images/20191029/9094293_1572351464995_DFD27DCB713FEB02D8628E6C67FD4273" alt="图片说明" title="图片标题"> </p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Serializable</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Error</title>
    <url>/2019/10/16/Spark-Error/</url>
    <content><![CDATA[<h1 id="配置historyserver时："><a href="#配置historyserver时：" class="headerlink" title="配置historyserver时："></a>配置historyserver时：</h1><p><img src="https://uploadfiles.nowcoder.com/images/20191016/9094293_1571227263283_97221A1B1C45BED99055FB1996A900CE" alt="图片说明" title="图片标题"> </p>
<p>因为端口被占用了</p>
<h1 id="SparkStreaming-updateStateByKeyde-使用时-–checkpoint报错"><a href="#SparkStreaming-updateStateByKeyde-使用时-–checkpoint报错" class="headerlink" title="SparkStreaming updateStateByKeyde 使用时 –checkpoint报错"></a>SparkStreaming updateStateByKeyde 使用时 –checkpoint报错</h1><p><img src="https://uploadfiles.nowcoder.com/images/20191016/9094293_1571227382925_460CCA764930F915D29C8DC785013F26" alt="图片说明" title="图片标题"> </p>
<p>原因在使用的时候，并没有设置checkPoint 检测点<br>检测点的目的就是为了保存上一次的结果数据。如果没有检测点的话，那么将无法保存上一次结果<br>如果要保证中间数据不丢失的话，可以借助其它的工具，如hdfs<br>ssc.checkpoint(“hdfs://kd0301:9000/spark_checkpoint/”)</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Error</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 概述</title>
    <url>/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="大数据处理框架"><a href="#大数据处理框架" class="headerlink" title="大数据处理框架"></a>大数据处理框架</h1><p>集群环境对于编程来说带来了很多挑战，首先就是并行化：这就要求我们以并行化的方式重写应用程序，以便我们可以利用更大范围节点的计算能力。集群环境的第二个挑战就是对单点失败的处理，节点宕机以及个别节点计算缓慢在集群环境中非常普遍，这会极大地影响程序的性能。最后一个挑战是集群在大多数情况下都会被多个用户分享，那么动态地进行计算资源的分配，也会干扰程序的执行。因此，针对集群环境出现了大量的大数据编程框架。首先我们要提到的就是Google的MapReduce，它给我们展示了一个简单通用和自动容错的批处理计算模型。<br>但是对于其他类型的计算，比如交互式和流式计算，MapReduce并不适合，这也导致了大量的不同于MapReduce的专有的数据处理模型的出现，比如Storm、Impala和GraphLab。随着新模型的不断出现，似乎对于大数据处理而言，我们应对不同类型的作业需要一系列不同的处理框架才能很好地完成。但是这些专有系统也有一些不足。</p>
<ul>
<li>重复工作：许多专有系统在解决同样的问题，比如分布式作业以及容错。举例来说，一个分布式的SQL引擎或者一个机器学习系统都需要实现并行聚合。这些问题在每个专有系统中会重复地被解决。</li>
<li>组合问题：在不同的系统之间进行组合计算是一件费力又不讨好的事情。对于特定的大数据应用程序而言，中间数据集是非常大的，而且移动的成本也非常高昂。在目前的环境中，我们需要将数据复制到稳定的存储系统中（比如HDFS），以便在不同的计算引擎中进行分享。然而，这样的复制可能比真正的计算所花费的代价要大，所以以流水线的形式将多个系统组合起来效率并不高。</li>
<li>适用范围的局限性：如果一个应用不适合一个专有的计算系统，那么使用者只能换一个系统，或者重写一个新的计算系统。</li>
<li>资源分配：在不同的计算引擎之间进行资源的动态共享是比较困难的，因为大多数的计算引擎都会假设它们在程序运行结束之前拥有相同的机器节点的资源。</li>
<li>管理问题：对于多个专有系统，需要花费更多的精力和时间来管理和部署。<br>尤其是对于终端使用者而言，他们需要学习多种API和系统模型。  </li>
</ul>
<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p>针对MapReduce及各种专有系统中出现的不足，伯克利大学推出了全新的统一大数据处理框架Spark，创新性地提出了RDD概念（一种新的抽象的弹性数据集），在某种程度上Spark是对MapReduce模型的一种扩展。要在MapReduce上实现其不擅长的计算工作（比如迭代式、交互式和流式），看上去是一件非常困难的事情，其实主要的原因是MapReduce缺乏一种特性，即在并行计算的各个阶段进行有效的数据共享，这种共享就是RDD的本质。利用这种有效的数据共享和类似MapReduce的操作接口，上述的各种专有类型计算都能够有效地表达，而且能够获得与专有系统同等的性能。<br>特别值得一提的是，从前对于集群处理的容错方式，比如MapReduce和Dryad，是将计算构建成为一个有向无环图的任务集。而这只能允许它们有效地重新计算部分DAG。在单独的计算之间（在迭代的计算步骤之间），除了复制文件，这些模型没有提供其他的存储抽象，这就显著地增加了在网络之间复制文件的代价。RDD能够适应当前大部分的数据并行算法和编程模型。  </p>
<h1 id="RDD表达能力"><a href="#RDD表达能力" class="headerlink" title="RDD表达能力"></a>RDD表达能力</h1><p>可以使用RDD实现很多现有的集群编程模型以及一些以前的模型不支持的新应用。在这些模型中，RDD能够取得和专有系统同样的性能，还能提供包括容错处理、滞后节点（straggler node）处理等这些专有系统缺乏的特性。这里会重点讨论如下四类模型。</p>
<ul>
<li>迭代算法：这是目前专有系统实现的非常普遍的一种应用场景，比如迭代算法可以用于图处理和机器学习。RDD能够很好地实现这些模型，包括Pregel、HaLoop和GraphLab等模型。</li>
<li>关系型查询：对于MapReduce来说非常重要的需求就是运行SQL查询，包括长期运行、数小时的批处理作业和交互式的查询。然而对于MapReduce而言，对比并行数据库进行交互式查询，有其内在的缺点，比如由于其容错的模型而导致速度很慢。利用RDD模型，可以通过实现许多通用的数据库引擎特性，从而获得非常好的性能。</li>
<li>MapReduce 批处理：RDD提供的接口是MapReduce的超集，所以RDD可以有效地运行利用MapReduce实现的应用程序，另外RDD还适合更加抽象的基于DAG的应用程序，比如DryadLINQ。</li>
<li>流式处理：目前的流式系统也只提供了有限的容错处理，需要消耗系统非常大的拷贝代价或者非常长的容错时间。特别是在目前的系统中，基本都是基于连续计算的模型，常驻的有状态的操作会处理到达的每一条记录。为了恢复失败的节点，它们需要为每一个操作复制两份操作，或者是将上游的数据进行代价非常大的操作重放。利用RDD实现一种新的模型——离散数据流（D-Stream），可以克服上面的这些问题。D-Stream将流式计算当作一系列的短小而确定的批处理操作，而不是常驻的有状态的操作，将两个离散流之间的状态保存在RDD中。离散流模型能够允许通过RDD的继承关系图（lineage）进行并行性的恢复而不需要进行数据拷贝。  </li>
</ul>
<h1 id="Spark子系统"><a href="#Spark子系统" class="headerlink" title="Spark子系统"></a>Spark子系统</h1><p>如果按照目前流行的大数据处理场景来划分，可以将大数据处理分为如下三种情况。</p>
<ul>
<li>复杂的批量数据处理（batch data processing），通常的时间跨度为数十分钟到数小时。</li>
<li>基于历史数据的交互式查询（interactive query），通常的时间跨度为数十秒到数分钟。</li>
<li>基于实时数据流的数据处理（streaming data processing），通常的时间跨度为数百毫秒到数秒。<br>由于RDD具有丰富的表达能力，所以伯克利在Spark Core的基础之上衍生出了能够同时处理上述三种情形的统一大数据处理平台，如图1-1所示。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/spark.png" alt="图片说明"></div></li>
<li>Spark Core：基于RDD提供了丰富的操作接口，利用DAG进行统一的任务规划，使得Spark能够更加灵活地处理类似MapReduce的批处理作业。</li>
<li>Shark/Spark SQL:兼容Hive的接口HQL，提供了比Hive高出10~100倍的查询速度的分布式SQL引擎。</li>
<li>Spark Streaming：将流式计算分解成一系列的短小的批处理作业，利用Spark轻量级和低延时的调度框架，可以很好的支持流式处理。目前已经支持的数据输入源包括Kafka、Flume、Twitter、TCP sockets。</li>
<li>GraphX：基于Spark的图计算框架，兼容Pregel和GraphLab接口，增强了图构建以及图转换功能。</li>
<li>MLlib:Spark Core 天然地非常适合于迭代式运算，MLlib就是构建在Spark上的机器学习算法库。目前已经可以支持常用的分类算法、聚类算法、推荐算法等。<br>Spark生态系统的目标就是将批处理、交互式处理、流式处理融合到同一个软件栈中。对于最终的用户或者是开发者而言，Spark生态系统有如下特性。</li>
<li>Spark生态系统兼容Hadoop生态系统。这个特性对于最终用户至关重要，虽然Spark 通用引擎在一定程度上是用来取代MapReduce系统的，但是Spark能够完美兼容Hadoop生态中的HDFS和YARN等其他组件，使得现有的Hadoop用户能够非常容易地迁移到Spark系统中。图1-2显示了Spark与Hadoop生态的兼容性。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/1-2.png" alt></div></li>
<li>Spark生态系统学习成本很低。要实现一个相对完整的端到端解决方案，以前需要部署维护多个专有系统，现在只需要一个Spark系统。另外，如果开发者对Spark Core的原理有比较深入的理解，对构架在Spark Core之上的其他组件的运用将会非常容易。图1-3对比了Spark生态和其他大数据专有系统的代码量。在图1-3中的Spark一项中，批处理对应Spark Core，交互式处理对应Shark/Spark SQL，流计算对应Spark Streaming，而图计算对应GraphX。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/1-3.png" alt></div></li>
<li>Spark 性能表现优异。由于Spark利用DAG进行调度执行规划，所以在多任务计算以及迭代计算中能够大量减少磁盘I/O的时间。另外，对于每一项任务启动一个线程，而不是启动一个进程，大大缩短了任务启动时间。</li>
<li>Spark有强大的社区支持。Spark近一年多来保持非常迅猛的发展势头，被誉为大数据处理的未来。Spark的创始团队成立了Databricks公司，全力支持Spark 生态的发展。目前Hadoop商业版本发行公司中，已经有Cloudera、Hortonworks、MapR等公司相继宣布支持Spark软件栈。图1-4显示了Spark不同版本发布时对社区做出贡献的贡献者数量变化情况。<br><div align="center"><img src="/2019/10/16/Spark-%E6%A6%82%E8%BF%B0/1-4.png" alt></div></li>
<li>Spark 支持多种语言编程接口。Spark生态本身是使用Scala语言编写的，但是考虑到其流行性，因此Spark从一开始就支持Java和Python接口，方便Spark 程序开发者自由选择。</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark压缩文件性能分析</title>
    <url>/2019/10/15/Spark%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h1 id="Spark压缩文件性能分析"><a href="#Spark压缩文件性能分析" class="headerlink" title="Spark压缩文件性能分析"></a>Spark压缩文件性能分析</h1><p>HDFS上分布式文件存储，成为大数据平台首选存储平台。而Spark往往以HDFS文件为输入，为保持兼容性，Spark支持多种格式文件读取，大数据场景下，性能瓶颈往往是IO，而不是CPU算力，所以对文件的压缩处理成为了很必要的手段。Spark为提供兼容性，同时支持多种压缩包直接读取，方便于用户使用，不用提前对压缩格式处理，但各种压缩格式各有优缺点，若不注意将导致Spark的能力无法发挥出来。故对Spark计算压缩文件做一个分析。</p>
<h1 id="支持的压缩格式"><a href="#支持的压缩格式" class="headerlink" title="支持的压缩格式"></a>支持的压缩格式</h1><p>首先来看一下Spark读取HDFS文件常用的压缩格式：<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106237246_09ACB263F52577BA2C36F0D60E304A75" alt="图片说明" title="图片标题"></p>
<h1 id="执行对比分析"><a href="#执行对比分析" class="headerlink" title="执行对比分析"></a>执行对比分析</h1><p>实验数据：同一个文件包，json格式文件数据<br>处理逻辑：增加列，然后发送到kafka中。<br>DAG逻辑划分：两个job（read动作一个job，foreach动作一个job），每个job下面各一个stage，每个stage下面task若干<br>程序执行参数：–master yarn –deploy-mode client –executor-cores 4 –executor-memory 4G –num-executors 4</p>
<h2 id="非压缩文件"><a href="#非压缩文件" class="headerlink" title="非压缩文件"></a>非压缩文件</h2><p>文件大小：33.7GB<br>运行时间：9min</p>
<h3 id="read阶段："><a href="#read阶段：" class="headerlink" title="read阶段："></a>read阶段：</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106293920_47AEB6D162BF532944DBDAB53E3D2EAB" alt="图片说明" title="图片标题"><br>可以看到所有节点都在读取，分布式读取，速度很快。<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106336406_317ADCB2DB2F35A583847F38535B7718" alt="图片说明" title="图片标题"></p>
<p>Stage里面共计分成了252个task，每个读取128MB数据。</p>
<h3 id="foreach阶段"><a href="#foreach阶段" class="headerlink" title="foreach阶段"></a>foreach阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106365520_C40B30CD2EE17428DAC6E44372D32E64" alt="图片说明" title="图片标题"><br>依然并行全力计算<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106387308_25162C0F4A622B24CE97415EFFDE198A" alt="图片说明" title="图片标题"><br>每个执行节点上4个core都在并发运行。</p>
<h2 id="GZIP"><a href="#GZIP" class="headerlink" title="GZIP"></a>GZIP</h2><p>文件大小：10.6GB<br>运行时间：2.2h</p>
<p>###read阶段<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106404340_1FCD14A02942D89F2391B8A13BF3F1BF" alt="图片说明" title="图片标题"><br>只有单节点读取</p>
<p>同时该节点上也只有一个核心在运行<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106440480_1A2BEF1D5E0E50929495F9B550115FBA" alt="图片说明" title="图片标题"><br>foreach阶段</p>
<p>也是只有单节点、单core运行</p>
<h2 id="BZIP2"><a href="#BZIP2" class="headerlink" title="BZIP2"></a>BZIP2</h2><p>文件大小：7.7GB<br>运行时间：12min</p>
<h3 id="read阶段"><a href="#read阶段" class="headerlink" title="read阶段"></a>read阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106466729_53072EE252372595149335F751CD1394" alt="图片说明" title="图片标题"><br>与非压缩一致，并行进行</p>
<h3 id="foreach阶段-1"><a href="#foreach阶段-1" class="headerlink" title="foreach阶段"></a>foreach阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106481625_67EEB8DB9FD25C46B3486090E777B5D7" alt="图片说明" title="图片标题"><br>同样并发执行</p>
<h2 id="SNAPPY"><a href="#SNAPPY" class="headerlink" title="SNAPPY"></a>SNAPPY</h2><p>文件大小：16.5GB<br>运行时间：2.1h<br>这里直接采用的整文件压缩，所以文件不可分割。</p>
<h3 id="read阶段-1"><a href="#read阶段-1" class="headerlink" title="read阶段"></a>read阶段</h3><p><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106498372_E01A7A3A8C35CDBBFDFD4A58BA865021" alt="图片说明" title="图片标题"></p>
<p>单节点单核心读取，非并行。</p>
<h3 id="foreach阶段-2"><a href="#foreach阶段-2" class="headerlink" title="foreach阶段"></a>foreach阶段</h3><p>同上类似，单节点单核心运行<br>结果<br><img src="https://uploadfiles.nowcoder.com/images/20191003/9094293_1570106539620_64157194D9DFAFAA77BE73D5A02CE37B" alt="图片说明" title="图片标题"><br>gzip和snappy无法采用并行计算，也就是说在spark平台上，这两种格式只能采用串行单进程执行，于本文开头表格对应，无法分割（splittable）的压缩格式只能顺序一个进程读取，而读取后多文件又在一个executor上，其他executor无文件导致无法并行的foreach。<br>bz2和非压缩格式支持分割，也就是说可以并行读取以及计算。<br>不可分割的压缩格式文件不可并行读取，完全无法发挥spark的并行计算优势，并且若压缩包过大，对单节点的物理性能要求较高。<br>建议<br>snappy采用分块压缩方式使其可以并行读取计算。<br>gzip格式最好提前进行分割成小文件或者换格式，因多个文件可以并行读取。另一个办法是read文件后调用repartition操作强制将读取多数据重新均匀分配到不同的executor上，但这个操作会导致大量单节点性能占用，因此该格式建议不在spark上使用。<br>bz2表现相同于非压缩，但解压操作需要耗费时间。<br><strong>非压缩性能表现最佳，但会占用过大HDFS存储。</strong><br>spark输出压缩文件<br>实际生产环境需要spark输出文件到HDFS，并且为了节省空间会使用压缩格式，以下介绍几种常用的压缩格式<br>文本文件压缩</p>
<h2 id="bzip2"><a href="#bzip2" class="headerlink" title="bzip2"></a>bzip2</h2><p>压缩率最高，压缩解压速度较慢，支持split。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">rdd.saveAsTextFile(&quot;codec&#x2F;bzip2&quot;,classOf[BZip2Codec])</span><br></pre></td></tr></table></figure>
<h2 id="snappy"><a href="#snappy" class="headerlink" title="snappy"></a>snappy</h2><p>json文本压缩率 38.2%，压缩和解压缩时间短。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">rdd.saveAsTextFile(&quot;codec&#x2F;snappy&quot;,classOf[SnappyCodec])</span><br></pre></td></tr></table></figure>
<h2 id="gzip"><a href="#gzip" class="headerlink" title="gzip"></a>gzip</h2><p>压缩率高，压缩和解压速度较快，不支持split，如果不对文件大小进行控制，下次分析可能可能会造成效率低下的问题。<br>json文本压缩率23.5%，适合使用率低，长期存储的文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import org.apache.hadoop.io.compress.GzipCodec</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">rdd.saveAsTextFile(&quot;codec&#x2F;gzip&quot;,classOf[GzipCodec])</span><br></pre></td></tr></table></figure>
<h2 id="parquet文件压缩"><a href="#parquet文件压缩" class="headerlink" title="parquet文件压缩"></a>parquet文件压缩</h2><p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！<br>转换 1 TB 数据将花费多长时间？<br>50 分钟，在一个 6 数据节点的 Spark v1.5.1 集群上可达到约 20 GB/分的吞吐量。使用的总内存约为 500GB。HDFS 上最终的 Parquet 文件的格式为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&#x2F;user&#x2F;spark&#x2F;data&#x2F;parquet&#x2F;catalog_page&#x2F;part-r-00000-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet</span><br><span class="line">&#x2F;user&#x2F;spark&#x2F;data&#x2F;parquet&#x2F;catalog_page&#x2F;part-r-00001-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="存储节省"><a href="#存储节省" class="headerlink" title="存储节省"></a>存储节省</h3><p>以下 Linux 输出显示了 TEXT 和 PARQUET 在 HDFS 上的大小比较：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% hadoop fs -du -h -s &#x2F;user&#x2F;spark&#x2F;hadoopds1000g</span><br><span class="line">897.9 G &#x2F;user&#x2F;spark&#x2F;hadoopds1000g</span><br><span class="line">% hadoop fs -du -h -s &#x2F;user&#x2F;spark&#x2F;data&#x2F;parquet</span><br><span class="line">231.4 G &#x2F;user&#x2F;spark&#x2F;data&#x2F;parquet</span><br></pre></td></tr></table></figure>
<p>1 TB 数据的存储节省了将近 75%！<br>parquet为文件提供了列式存储，查询时只会取出需要的字段和分区，对IO性能的提升非常大，同时占用空间较小，即使是parquet的uncompressed存储方式也比普通的文本要小的多。<br>spark中通过对parquet文件进行存储，spark2.0后默认使用snappy压缩，1.6.3及以前版本默认使用的gzip压缩方式。</p>
<p>dataset.write().parquet(“path”);</p>
<p>可以通过<br>spark.sql.parquet.compression.codec</p>
<p>参数或是在代码中进行修改配置压缩方式。<br>sparkConf.set(“spark.sql.parquet.compression.codec”,”gzip”)</p>
<p>parquet存储提供了<br>lzo gzip snappy uncompressed</p>
<p>参考文章<br><a href="https://zturn.cc/?p=24" target="_blank" rel="noopener">https://zturn.cc/?p=24</a><br><a href="https://blog.csdn.net/bajinsheng/article/details/100031359" target="_blank" rel="noopener">https://blog.csdn.net/bajinsheng/article/details/100031359</a><br><a href="https://www.ibm.com/developerworks/cn/analytics/blog/ba-parquet-for-spark-sql/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/analytics/blog/ba-parquet-for-spark-sql/index.html</a></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title>VisualVM:多合一故障处理工具</title>
    <url>/2019/11/10/VisualVM-%E5%A4%9A%E5%90%88%E4%B8%80%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>VisualVM (All-in-One Java Troubleshooting Tool)是到目前为止随JDK发布的功能最强大的运行监视和故障处理程序，并且可以预见在未来一段时间内都是官方助力发展的虚拟机故障处理工具。官方在VisualVM的软件说明中写上了“All in One”的描述字样，预示着它除了运行监视，故障处理 外，还提供了很多其他方面的功能。如性能分析，VisualVM的性能分析功能甚至比起JProfiler、YourKit等专业收费的Profiling工具都不会逊色多少，而且VisualVM的还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，因此它对应用程序的实际性能的影响很小，使用它可以直接应用在生产环境中。这个是JProfiler、YourKit等工具无法与之媲美的。</p>
<h2 id="Visual兼容范围与插件安装"><a href="#Visual兼容范围与插件安装" class="headerlink" title="Visual兼容范围与插件安装"></a>Visual兼容范围与插件安装</h2><p>VisualVM基于NetBeans平台开发，因此它一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM可以做到：</p>
<ul>
<li>显示虚拟机进程以及进程的配置、环境信息。</li>
<li>监视应用程序的CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。</li>
<li>dump以及分析堆转储快照。</li>
<li>方法级的程序运行性能分析，找出被调用最多、运行时间最长的方法。</li>
<li>离线程序快照：收集程序的运行时配置、线程dump、内存dump等信息建立一个快照，可以将快照发送开发者处进行Bug反馈。</li>
<li>其他plugins的无限的可能性。。<br>主要特性的兼容性见表：</li>
</ul>
<p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573376421667_0603DDC2187E3F359DFF1C4A07BF61DF" alt="图片说明" title="图片标题"> </p>
<p>首先我们在jdk1.8/bin目录下启动jvisualvm,我们先不着急找应用程序进行监测，因为现在VisualVM还没有加载任何插件，虽然基本的监视、线程面板的功能主程序都以默认插件的形式提供了但是不给VisualVM装任何扩展插件，就相当于放弃了它最精华的功能，和没有安装任何应用软件操作系统差不多。<br>插件我们进行手工安装：<br>打开“工具 -&gt; 插件 -&gt; 可用插件 “选中“Visual GC”默认进行安装<br>安装成功后：</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573377093671_1269F967A1AE25FDC458A14CB4CB4B37" alt="图片说明" title="图片标题"> </p>
<p>大家可以根据自己的工作需要和兴趣选择合适的插件，然后点击安装即可。<br>安装完插件，选择一个需要监视的程序就进入程序的主界面了，如图所示。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573377517433_B76A4E6CB4F70B4F3FD192B97C655C83" alt="图片说明" title="图片标题"> </p>
<p>VisualVM中“概述”、“监视”、“线程”、“MBeans”的功能与前面介绍的JConsole差别不大，下面介绍几个特色的功能、插件。</p>
<h2 id="2-生成、浏览堆转储快照"><a href="#2-生成、浏览堆转储快照" class="headerlink" title="2.生成、浏览堆转储快照"></a>2.生成、浏览堆转储快照</h2><p>在VisualVM中生成dump文件有两种方式，可以执行下列任一操作：</p>
<ul>
<li>在“应用程序”窗口中右键”单击应用程序节点，然后选择“堆Dump”。</li>
<li>在“应用程序”窗口中双击应用程序节点以打开应用程序标签，然后在“监视”标签中单击“堆Dump”。<br>生成dump文件之后，应用程序页签将在该堆的应用程序下增加一个以[heapdump]开头的子节点，并且在主页签中打开了该转储快照，如图。如果需要把dump文件保存或发送出去，要在heapdump节点上右键选择“另存为”菜单，否则当VisualVM关闭时，生成的dump文件会被当做临时文件删除掉。要打开一个已经存在的dump文件，通过菜单“装入”功能，选择硬盘上的dump文件即可。</li>
</ul>
<p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573378591042_FAB281188970DB420E1323E98D38BBAA" alt="图片说明" title="图片标题"> </p>
<h2 id="分析程序性能"><a href="#分析程序性能" class="headerlink" title="分析程序性能"></a>分析程序性能</h2><p>在Profiler页签中，VisualVM提供了程序运行期间方法级的CPU执行时间分析以及内存分析，做Profiling分析肯定会对程序运行性能有较大的影响，所以一般不在生产环境中使用这项功能。<br>要开始分析，先选择“CPU”和“内存”按钮中的一个，然后切换到应用程序中对程序进行操作，VisualVM会记录到这段时间中应用程序执行过的方法。如果是CPU分析，将会统计每个方法的执行次数、执行耗时；如果是内存分析，则会统计每个方法关联的对象数以及这些对象所占的空间。分析结束后，点击“停止”按钮结束监控过程，如图：</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191110/9094293_1573379709800_6C3B2CC29B95904A4B0C47224C44304A" alt="图片说明" title="图片标题"> </p>
<h2 id="4-BTrace动态日志跟踪"><a href="#4-BTrace动态日志跟踪" class="headerlink" title="4.BTrace动态日志跟踪"></a>4.BTrace动态日志跟踪</h2><p>BTrace是一个很“有趣”的VisualVM插件，本身也是可以独立运行的程序，它的作用是在不停止目标程序运行的前提下，通过HotSpot虚拟机的HotSwap技术动态缴入原本并不存在的调试代码。这项功能对实际生产中的程序很有意义：经常遇到程序出现问题，但排查错误的一些必要信息，譬如方法参数、返回值等，在开发时并没有打印到日止之中，以至于不得不停掉服务，通过调试增量来加入日志代码以解决问题。当遇到生产环境服务无法随便停止是，缺一两句日志导致排错进行不下去是一件非常郁闷的事情。<br>在VisualVM中安装了BTrace插件后，在应用程序面板中右键点击要调试的程序，会出现“Trace Application。。。”菜单，点击将进入BTrace面板。这个面板里面看起来就像一个简单的Java程序开发环境，里面还有一段Java代码。如图：</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191111/9094293_1573475795296_506A122025C95B87465048F8325621A5" alt="图片说明" title="图片标题"> </p>
<p>这里准备了一个Demo来演示BTrace的功能：产生 两个1000以内的随机整数，输出两个数字相加的结果代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.io.BufferedReader;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.io.InputStreamReader;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @Author: luomo</span><br><span class="line"> * @CreateTime: 2019&#x2F;11&#x2F;11</span><br><span class="line"> * @Description: 对BTrace插件简单的使用</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class BTraceTest &#123;</span><br><span class="line">    public int add(int a,int b)&#123;</span><br><span class="line">        return a+b;</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args)throws IOException &#123;</span><br><span class="line">        BTraceTest test&#x3D;new BTraceTest();</span><br><span class="line">        BufferedReader reader&#x3D;new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        for(int i&#x3D;0;i&lt;10;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            reader.readLine();</span><br><span class="line">            int a&#x3D;(int) Math.round(Math.random()*1000);</span><br><span class="line">            int b&#x3D;(int) Math.round(Math.random()*1000);</span><br><span class="line">            System.out.println(test.add(a,b));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序运行后，在VisualVM中打开该程序的监视，在BTrace页签填充TracingScript内容，输入的调试代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* BTrace Script Template *&#x2F;</span><br><span class="line">import com.sun.btrace.annotations.*;</span><br><span class="line">import static com.sun.btrace.BTraceUtils.*;</span><br><span class="line"></span><br><span class="line">@BTrace</span><br><span class="line">public class TracingScript &#123;</span><br><span class="line">	&#x2F;* put your code here *&#x2F;</span><br><span class="line">    @OnMethod(</span><br><span class="line">        clazz&#x3D;&quot;BTraceTest&quot;,</span><br><span class="line">        method&#x3D;&quot;add&quot;,</span><br><span class="line">        location&#x3D;@Location(Kind.RETURN)</span><br><span class="line">)</span><br><span class="line">public static void func(@Self BTraceTest instance,int a,int b,@Return int result)&#123;</span><br><span class="line">println(&quot;调用堆栈：&quot;);</span><br><span class="line">jstack();</span><br><span class="line">println(strcat(&quot;方法参数A：&quot;,str(a)));</span><br><span class="line">println(strcat(&quot;方法参数B:&quot;,str(b)));</span><br><span class="line">println(strcat(&quot;方法结果：&quot;,str(result)));</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>点击“Start”按钮后稍等片刻，编译完成后，可见Output面板上出现“BTrace code sucessfuly deployed”的字样。程序运行的时候在Output面板将会输出如图的调试信息。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20191111/9094293_1573477699767_FA5FB94755C7F76228C24ADC9E5CF1A9" alt="图片说明" title="图片标题"> </p>
<p>BTrace的用法还有许多，打印调用堆栈、参数、返回值只是最基本的应用，在他的网站上有使用BTrace进行性能监视、定位连接泄漏和内存泄漏、解决多线程竞争问题等例子。</p>
]]></content>
  </entry>
  <entry>
    <title>Zeppelin</title>
    <url>/2019/10/02/Zeppelin/</url>
    <content><![CDATA[<h1 id="什么是ApacheZeppelin？"><a href="#什么是ApacheZeppelin？" class="headerlink" title="什么是ApacheZeppelin？"></a>什么是ApacheZeppelin？</h1><p>Apache Zeppelin 是一个可以进行大数据可视化分析的交互式开发系统，可以承担数据接入、数据发现、数据分析、数据可视化、数据协作等任务，其前端提供丰富的可视化图形库，不限于SparkSQL，后端支持HBase、Flink 等大数据系统以插件扩展的方式，并支持Spark、Python、JDBC、Markdown、Shell 等各种常用Interpreter，这使得开发者可以方便地使用SQL 在 Zeppelin 中做数据开发。在 Zeppelin 中还可以完成机器学习的数据预处理、算法开发和调试、算法作业调度的工作，同时，Zeppelin 还提供了单机 Docker、分布式、K8s、Yarn 四种系统运行模式，以适应各类团队的需求。</p>
<h1 id="多功能笔记本"><a href="#多功能笔记本" class="headerlink" title="多功能笔记本"></a>多功能笔记本</h1><p>1）数据摄取<br>2）数据发现<br>3）数据可视化与协作<br><img src="https://uploadfiles.nowcoder.com/images/20191002/9094293_1570025783514_8CF155AE412A82C78071201381CEAC20" alt="图片说明" title="图片标题"> </p>
<h1 id="多语言后端"><a href="#多语言后端" class="headerlink" title="多语言后端"></a>多语言后端</h1><p>Apache Zeppelin解释器概念允许将任何语言数据/数据处理后端插入Zeppelin。当前，Apache Zeppelin支持许多解释器，例如Apache Spark，Python，JDBC，Markdown和Shell。</p>
<h1 id="Apache-Spark集成"><a href="#Apache-Spark集成" class="headerlink" title="Apache Spark集成"></a>Apache Spark集成</h1><p>特别是，Apache Zeppelin提供了内置的Apache Spark集成。不需要为其构建单独的模块，插件或库。<br><img src="https://uploadfiles.nowcoder.com/images/20191002/9094293_1570025811557_14FF85F448A85399AA74361F7B957506" alt="图片说明" title="图片标题"><br>Apache Zeppelin与Spark集成提供<br>Apache Zeppelin与Spark集成提供<br>自动SparkContext和SQLContext注入<br>从本地文件系统或Maven存储库加载运行时jar依赖项。<br>取消作业并显示其进度</p>
<h1 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h1><p>Apache Zeppelin中已经包含一些基本图表，可视化不仅限于SparkSQL查询，任何语言后端的任何输出都可以被识别和可视化<br><img src="https://uploadfiles.nowcoder.com/images/20191002/9094293_1570025841915_3FDB2A16838FA1D3A8DE66597442CDB1" alt="图片说明" title="图片标题"> </p>
<h2 id="枢轴图表"><a href="#枢轴图表" class="headerlink" title="枢轴图表"></a>枢轴图表</h2><p>Apache Zeppelin 汇总值并通过简单的拖放将其显示在数据透视图中。您可以轻松地创建具有多个汇总值的图表，包括综合，计数，平均值，最小值，最大值。<br><img src="https://uploadfiles.nowcoder.com/images/20191002/9094293_1570025877924_B5510C7235B1E872B0A8BF2CADE4D1CA" alt="图片说明" title="图片标题"><br>在Apache Zeppelin中了解有关基本显示系统和Angular API（frontend，backend）的更多信息。</p>
<h1 id="动态表格"><a href="#动态表格" class="headerlink" title="动态表格"></a>动态表格</h1><p><img src="https://uploadfiles.nowcoder.com/images/20191002/9094293_1570025899397_63E60EDEB1CCD12D3A765295F3FE0018" alt="图片说明" title="图片标题"> </p>
<h1 id="通过共享您的笔记本和段落进行协作"><a href="#通过共享您的笔记本和段落进行协作" class="headerlink" title="通过共享您的笔记本和段落进行协作"></a>通过共享您的笔记本和段落进行协作</h1><p>您的笔记本URL可以在协作者之间共享。然后，Apache Zeppelin将实时广播所有更改，就像Google文档中的协作一样。<br><img src="https://uploadfiles.nowcoder.com/images/20191002/9094293_1570025913843_7E9D16C843B30DC30C36EBED3FC80485" alt="图片说明" title="图片标题"> </p>
]]></content>
      <categories>
        <category>Zeppelin</category>
      </categories>
  </entry>
  <entry>
    <title>VMwareWorkstation pro无法在Windows上运行的问题</title>
    <url>/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于Windows系统更新问题，导致电脑上的VMwareWorkstation pro无法使用，之前有卸过Windows组件，但是系统自动更新之后又不能打开，所有只能乖乖地将我的VMwareWorkstation更新到15版本。</p>
<h3 id="首先根据系统的提示将最新的版本下载至我们的电脑中"><a href="#首先根据系统的提示将最新的版本下载至我们的电脑中" class="headerlink" title="首先根据系统的提示将最新的版本下载至我们的电脑中"></a>首先根据系统的提示将最新的版本下载至我们的电脑中</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/1.png" alt="图片说明"> </p>
<h3 id="由于我的电脑是Windows系统所以选择Windows版本"><a href="#由于我的电脑是Windows系统所以选择Windows版本" class="headerlink" title="由于我的电脑是Windows系统所以选择Windows版本"></a>由于我的电脑是Windows系统所以选择Windows版本</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/2.png" alt="图片说明"> </p>
<h3 id="当我们打开运行安装时发现"><a href="#当我们打开运行安装时发现" class="headerlink" title="当我们打开运行安装时发现"></a>当我们打开运行安装时发现</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/3.png" alt="图片说明"> </p>
<h3 id="于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载"><a href="#于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载" class="headerlink" title="于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载"></a>于是我抱着可能丢失文件的风险将我的老版本VMwareWorkstation卸载</h3><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/4.png" alt="图片说明"> </p>
<p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/5.png" alt="图片说明"> </p>
<p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/6.png" alt="图片说明"> </p>
<p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/7.png" alt="图片说明"> </p>
<h2 id="最后升级到我们的VMwareWorkstation-pro版本即可，大功告成-虚拟机又能打开了。（不过界面…）"><a href="#最后升级到我们的VMwareWorkstation-pro版本即可，大功告成-虚拟机又能打开了。（不过界面…）" class="headerlink" title="最后升级到我们的VMwareWorkstation pro版本即可，大功告成! 虚拟机又能打开了。（不过界面…）"></a>最后升级到我们的VMwareWorkstation pro版本即可，大功告成! 虚拟机又能打开了。（不过界面…）</h2><p><img src="/2019/11/23/VMwareWorkstation-pro%E6%97%A0%E6%B3%95%E5%9C%A8Windows%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98/8.png" alt="图片说明"> </p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title>Zeppelin 安装</title>
    <url>/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Apache Zeppelin 是一个可以进行大数据可视化分析的交互式开发系统，可以承担数据接入、数据发现、数据分析、数据可视化、数据协作等任务，其前端提供丰富的可视化图形库，不限于SparkSQL，后端支持HBase、Flink 等大数据系统以插件扩展的方式，并支持Spark、Python、JDBC、Markdown、Shell 等各种常用Interpreter，这使得开发者可以方便地使用SQL 在 Zeppelin 中做数据开发。</p>
<h3 id="安装环境："><a href="#安装环境：" class="headerlink" title="安装环境："></a>安装环境：</h3><p>Centos6.8、jdk1.8、zeppelin-0.8.1-bin-all</p>
<h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><p>打开下面链接进行下载:<br><a href="http://zeppelin.apache.org/download.html" target="_blank" rel="noopener">http://zeppelin.apache.org/download.html</a></p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/1.png" alt="图片说明"> </p>
<p>选择zeppelin-0.8.2-bin-all下载并将其解压缩到您选择的目录中，即可开始使用。</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/2.png" alt="图片说明"> </p>
<p>执行 bin/zeppelin-daemon.sh start<br>Zeppelin成功启动后，使用Web浏览器转到http：// localhost：8080。如图：</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/3.png" alt="图片说明"> </p>
<p>安装成功。</p>
<h3 id="如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决："><a href="#如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决：" class="headerlink" title="如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决："></a>如果遇到Zeppelin启动但是web页面打不开，我们可以尝试一下方法解决：</h3><p>1.防火墙是否关闭<br>使用以下命令永久关闭防火墙<br>chkconfig iptables off<br>2.查看是否是端口被占用<br>lsof -i:端口号 //查看是否被占用<br>如果被占用可以尝试修改默认端口 zeppelin-site.xml.template</p>
<p>vi zeppelin-site.xml.template</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/4.png" alt="图片说明"> </p>
<h2 id="这里展示一下Zeppelin连接数据库的一个demo"><a href="#这里展示一下Zeppelin连接数据库的一个demo" class="headerlink" title="这里展示一下Zeppelin连接数据库的一个demo"></a>这里展示一下Zeppelin连接数据库的一个demo</h2><p>1.首先进入Zeppelin的首页</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/5.png" alt="图片说明"> </p>
<p>2.点击 anonymous -&gt; Interpreter 输入jdbc回车</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/6.png" alt="图片说明"> </p>
<p>3.点击 edit 编辑红框选中选项</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/7.png" alt="图片说明"> </p>
<p>然后添加你的mysql驱动位置</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/8.png" alt="图片说明"> </p>
<p>4.创建一个新的node，点击 create new note：输入NoteName和Default Interpreter</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/9.png" alt="图片说明"> </p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/10.png" alt="图片说明"> </p>
<p>5.在框中输入SQL语句，点击执行</p>
<p><img src="/2019/12/21/Zeppelin-%E5%AE%89%E8%A3%85/11.png" alt="图片说明"> </p>
<p>我们还可以根据自己的需要进行选择图表的展示方式。</p>
]]></content>
      <categories>
        <category>Zeppelin</category>
      </categories>
      <tags>
        <tag>Zeppelin 安装</tag>
      </tags>
  </entry>
  <entry>
    <title>垃圾收集器</title>
    <url>/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如果说收集算法是内存回收的方法论，那么垃圾收集就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。这里讨论的收集器基于JDK1.7Update14之后的HotSpot虚拟机。</p>
<h2 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h2><p>Serial收集器是最基本、发展历史最悠久的收集器，曾经是虚拟机新生代收集的唯一选择。我们从名字就可知道，这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。<br>如图示意了Serial、Serial Old收集器的运行过程。<br><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/1.png" alt="图片说明"> </p>
<h2 id="ParNew"><a href="#ParNew" class="headerlink" title="ParNew"></a>ParNew</h2><p>ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数（例如：-XX：SurvivorRatio、——XX：PretenureSizeThreshold、-XX：HandlePromotionFailure等）、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。<br>ParNew/Serial Old示意图：</p>
<p><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/2.png" alt="ParNew/Serial Old示意图"> </p>
<h2 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h2><p>Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去和ParNew都一样，那他有什么特别之处呢?<br>Parallen Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能的缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量。所谓吞吐量<br>就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间），虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。<br>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度就能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要是和在后台运算而不需要太多交互的任务。</p>
<h2 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h2><p>Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备预案，在并发手机发生Concurrent Mode Failure时使用。<br>如图是Serial、Serial Old收集器的运行过程。</p>
<p><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/3.png" alt="图片说明"></p>
<h2 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h2><p>Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器是在JDk1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择。由于老年代Serial Old收集器在服务端应用性能上的拖累，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew和CMS的组合给力。<br>Parallel Old收集器的工作过程如图：</p>
<p><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/4.png" alt="图片说明"> </p>
<h2 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h2><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常复合这类应用的需求。<br>从名字可以看出，CMS收集器是基于“标记清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤，<br>包括：</p>
<ul>
<li>初始标记</li>
<li>并发标记</li>
<li>重新标记</li>
<li>并发清除<br>其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能<br>直接关联到的对象，速度很快，并发标记阶段就是进行GC RootsTracing的过程，而重新标记阶段则是为了修正并<br>发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初<br>始标记阶段稍长一些，但远比并发标记的时间短。<br><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/5.png" alt="图片说明"> <h2 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h2>G1（Garbage-First）收集器是当今收集器技术发展的最前沿成果之一。G1是一款面向服务端应用的垃圾收集器。HotSpot开发团队赋予它的使命是（在比较长期的）未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。</li>
<li>并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。</li>
<li>分代收集：与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更<br>好的收集效果。</li>
<li>空间整合：与CMS的“标记—清理”算法不同，G1从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但无论如何，这两种算法都意味着G1运作期间不会产生<br>内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</li>
<li>可预测的停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。<br>在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤：</li>
<li>初始标记（Initial Marking）</li>
<li>并发标记（Concurrent Marking）</li>
<li>最终标记（Final Marking）</li>
<li>筛选回收（Live Data Counting and Evacuation）<br><img src="/2019/11/08/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/6.png" alt="图片说明"> </li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>垃圾收集器</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper分布式集群部署</title>
    <url>/2019/11/17/Zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>ZooKeeper是一个开放源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。  </p>
<p>ZooKeeper是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。ZooKeeper可以保证如下分布式一致性特性。<br>今天我们来看看如何分布式安装</p>
</blockquote>
<h1 id="Zookeeper分布式集群部署"><a href="#Zookeeper分布式集群部署" class="headerlink" title="Zookeeper分布式集群部署"></a>Zookeeper分布式集群部署</h1><h2 id="下载Zookeeper"><a href="#下载Zookeeper" class="headerlink" title="下载Zookeeper"></a>下载Zookeeper</h2><p>这里可以参考官网，下载需要的版本：<br><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">https://zookeeper.apache.org/</a>  </p>
<h2 id="解压Zookeeper到我们指定的目录"><a href="#解压Zookeeper到我们指定的目录" class="headerlink" title="解压Zookeeper到我们指定的目录"></a>解压Zookeeper到我们指定的目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 software]#  tar -zxvf zookeeper-3.4.10 -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p><strong>1.修改zoo_sample.cfg zoo.cfg 为zoo.cfg</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 software]#  mv zoo_sample.cfg zoo.cfg zoo.cfg</span><br></pre></td></tr></table></figure>
<p><strong>2.修改配置文件</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 software]# vi zoo.cfg</span><br></pre></td></tr></table></figure>
<blockquote>
<p>#数据目录需要提前创建  </p>
<p>dataDir=/opt/module/zookeeper-3.4.5-cdh5.10.0/zkData  </p>
<p>#server.每个节点服务编号=服务器ip地址：集群通信端口：选举端口<br>server.1=bigdata107 :2888:3888<br>server.2=bigdata108 :2888:3888<br>server.3=bigdata109 :2888:3888</p>
</blockquote>
<p><strong>3.分发到各个节点</strong>  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 software]#   scp -r zookeeper-3.4.10&#x2F; bigdata108:&#x2F;opt&#x2F;module&#x2F;</span><br><span class="line">[root@bigdata107 software]#  scp -r zookeeper-3.4.10&#x2F; bigdata109:&#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>
<p><strong>4.创建相关目录</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata107 zookeeper-3.4.10]# mkdir zkData</span><br></pre></td></tr></table></figure></p>
<p><strong>5.在各个节点zkData目录下，创建myid</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#bigdata107 节点</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# touch myid</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# vi myid</span><br><span class="line">1</span><br><span class="line">#bigdata108 节点</span><br><span class="line">[root@bigdata108 zookeeper-3.4.10]# touch myid</span><br><span class="line">[root@bigdata108 zookeeper-3.4.10]# vi myid</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">#bigdata109 节点</span><br><span class="line">[root@bigdata109 zookeeper-3.4.10]# touch myid</span><br><span class="line">[root@bigdata109 zookeeper-3.4.10]# vi myid</span><br><span class="line">3</span><br></pre></td></tr></table></figure></p>
<p><strong>6.启动Zookeeper服务</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">#启动Zookeeper服务</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh start</span><br><span class="line">#查看各个节点服务状态 </span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh status</span><br><span class="line">#关闭各个节点服务</span><br><span class="line">[root@bigdata107 zookeeper-3.4.10]# bin/zkServer.sh stop</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>集群部署</tag>
      </tags>
  </entry>
  <entry>
    <title>垃圾收集算法</title>
    <url>/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="标记清除算法"><a href="#标记清除算法" class="headerlink" title="标记清除算法"></a>标记清除算法</h1><p><img src="/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/1.png" alt="图片说明"><br>最基础的收集算法是“标记清除”算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经介绍过了。之所以说他是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。<br>主要不足：</p>
<ul>
<li>效率问题，标记和清除两个过程的效率都不高；</li>
<li>空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能导致以后再程序过程中需要分配较大的对象时，无法找到连续内存而不得不提前触发另一次垃圾收集动作</li>
</ul>
<h1 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h1><p><img src="/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/2.png" alt="图片说明"><br>为了解决效率问题，一种称为复制的收集算法出现了，他将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完了，就将还存活着的对象复制到另一块上面，然后再把已经使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂的情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。<br>主要不足：</p>
<ul>
<li>将内存缩小为了原来的一半，在对象存活率较高时就要进行较多的复制操作，效率将会变低。</li>
</ul>
<h1 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h1><p><img src="/2019/11/06/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/3.png" alt="图片说明"><br>根据老年代的特点，有人提出了另外一种“标记-整理”算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。<br>主要不足：</p>
<ul>
<li>它对内存变动更频繁，需要整理所有存活对象的引用地址，在效率上比复制算法要差很多。</li>
</ul>
<h1 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h1><p>当前商业虚拟机的垃圾收集器都采用“分代收集”算法，这种算法并没有什么新的思想，知识根据对象存活周期的不同将内存划分为几块。分代收集算法分代收集算法（Generational Collection）严格来说并不是一种思想或理论，而是融合上述3种基础的算法思想，而产生的针对不同情况所采用不同算法的一套组合拳。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。</p>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>垃圾收集算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/2020/07/08/Hive/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hadoop 生态系统就是为处理如此大数据集而产生的一个合乎成本效益的解决方案。Hadoop 实现了一个特别的计算模型，也就是MapReduce，其可以将计算任务分割成多个处理单元然后分散到一群家用的或服务器级别的硬件机器上，从而降低成本并提供水平可伸缩性。这个计算模型的下面是一个被称为Hadoop分布式文件系统（HDFS）的分布式文件系统。这个文件系统是“可插拔的”，而且现在已经出现了几个商用的和开源的替代方案。</p>
<p>不过，仍然存在一个挑战，那就是用户如何从一个现有的数据基础架构转移到Hadoop上，而这个基础架构是基于传统关系型数据库和结构化查询语句（SQL）的。对于大量的SQL用户（包括专业数据库设计师和管理员，也包括那些使用SQL从数据仓库中抽取信息的临时用户）来说，这个问题又将如何解决呢？<br>这就是Hive 出现的原因。Hive 提供了一个被称为Hive 查询语言（简称 HiveQL或HQL）的SQL方言，来查询存储在Hadoop集群中的数据。<br>SQL知识分布广泛的一个原因是：它是一个可以有效地、合理地且直观地组织和使用数据的模型。即使对于经验丰富的Java开发工程师来说，将这些常见的数据运算对应到底层的MapReduce JavaAPI也是令人畏缩的。Hive可以帮助用户来做这些苦活，这样用户就可以集中精力关注于查询本身了。Hive可以将大多数的查询转换为MapReduce任务（job），进而在介绍一个令人熟悉的SQL抽象的同时，拓宽Hadoop的可扩展性。</p>
<p>Hive 最适合于数据仓库应用程序，使用该应用程序进行相关的静态数据分析，不需要快速响应给出结果，而且数据本身不会频繁变化。<br>Hive不是一个完整的数据库。Hadoop以及HDFS的设计本身约束和局限性地限制了Hive 所能胜任的工作。其中最大的限制就是Hive不支持记录级别的更新、插入或者删除操作。但是用户可以通过查询生成新表或者将查询结果导入到文件中。同时，因为Hadoop是一个面向批处理的系统，而MapReduce任务（job）的启动过程需要消耗较长的时间，所以Hive查询延时比较严重。传统数据库中在秒级别可以完成的查询，在Hive中，即使数据集相对较小，往往也需要执行更长的时间。最后需要说明的是，Hive不支持事务。</p>
<p>Hive是最适合数据仓库应用程序的，其可以维护海量数据，而且可以对数据进行挖掘，然后形成意见和报告等。<br>因为大多数的数据仓库应用程序是使用基于SQL的关系型数据库实现的，所以Hive降低了将这些应用程序移植到Hadoop上的障碍。用户如果懂得SQL，那么学习使用Hive将会很容易。如果没有Hive，那么这些用户就需要去重新学习新的语言和新的工具后才能进行生产。<br>同样地，相对于其他Hadoop 语言和工具来说，Hive也使得开发者将基于SQL的应用程序移植到Hadoop变得更加容易。</p>
<h1 id="Hive的体系架构"><a href="#Hive的体系架构" class="headerlink" title="Hive的体系架构"></a>Hive的体系架构</h1><p><img src="/2020/07/08/Hive/1.png" alt></p>
<h2 id="command-line-shell-amp-thrift-jdbc"><a href="#command-line-shell-amp-thrift-jdbc" class="headerlink" title="command-line shell &amp; thrift/jdbc"></a>command-line shell &amp; thrift/jdbc</h2><p>可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：</p>
<ul>
<li><strong>command-line shell</strong>：通过 hive 命令行的的方式来操作数据；</li>
<li><strong>thrift／jdbc</strong>：通过 thrift 协议按照标准的 JDBC 的方式操作数据。</li>
</ul>
<h2 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a>Metastore</h2><p>在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。</p>
<p>Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul>
<li><p>简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</p>
</li>
<li><p>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</p>
</li>
<li><p>为超大的数据集设计的计算和存储能力，集群扩展容易;</p>
</li>
<li><p>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</p>
</li>
<li><p>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Hadoop生态圈</category>
      </categories>
      <tags>
        <tag>Hive概述</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive常用DDL操作</title>
    <url>/2020/07/10/Hive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h1 id="Hive常用DDL操作"><a href="#Hive常用DDL操作" class="headerlink" title="Hive常用DDL操作"></a>Hive常用DDL操作</h1><h2 id="一、Database"><a href="#一、Database" class="headerlink" title="一、Database"></a>一、Database</h2><h3 id="1-1查看数据列表"><a href="#1-1查看数据列表" class="headerlink" title="1.1查看数据列表"></a>1.1查看数据列表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-使用数据库"><a href="#1-2-使用数据库" class="headerlink" title="1.2 使用数据库"></a>1.2 使用数据库</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">USE database_name;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-新建数据库"><a href="#1-3-新建数据库" class="headerlink" title="1.3 新建数据库"></a>1.3 新建数据库</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name   --DATABASE|SCHEMA 是等价的</span><br><span class="line">  [COMMENT database_comment] --数据库注释</span><br><span class="line">  [LOCATION hdfs_path] --存储在 HDFS 上的位置</span><br><span class="line">  [WITH DBPROPERTIES (property_name&#x3D;property_value, ...)]; --指定额外属性</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE IF NOT EXISTS hive_test</span><br><span class="line">  COMMENT &#39;hive database for test&#39;</span><br><span class="line">  WITH DBPROPERTIES (&#39;create&#39;&#x3D;&#39;heibaiying&#39;);</span><br></pre></td></tr></table></figure>
<h3 id="1-4-查看数据库信息"><a href="#1-4-查看数据库信息" class="headerlink" title="1.4 查看数据库信息"></a>1.4 查看数据库信息</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DESC DATABASE [EXTENDED] db_name; --EXTENDED 表示是否显示额外属性</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DESC DATABASE  EXTENDED hive_test;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-删除数据库"><a href="#1-5-删除数据库" class="headerlink" title="1.5 删除数据库"></a>1.5 删除数据库</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure>
<ul>
<li>默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。</li>
</ul>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DROP DATABASE IF EXISTS hive_test CASCADE;</span><br></pre></td></tr></table></figure>
<h2 id="二、创建表"><a href="#二、创建表" class="headerlink" title="二、创建表"></a>二、创建表</h2><h3 id="2-1-建表语法"><a href="#2-1-建表语法" class="headerlink" title="2.1 建表语法"></a>2.1 建表语法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name     --表名</span><br><span class="line">  [(col_name data_type [COMMENT col_comment],</span><br><span class="line">    ... [constraint_specification])]  --列名 列数据类型</span><br><span class="line">  [COMMENT table_comment]   --表描述</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  --分区表分区规则</span><br><span class="line">  [</span><br><span class="line">    CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS</span><br><span class="line">  ]  --分桶表分桶规则</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)  </span><br><span class="line">   [STORED AS DIRECTORIES] </span><br><span class="line">  ]  --指定倾斜列和值</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format]    </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line">     | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)]  </span><br><span class="line">  ]  -- 指定行分隔符、存储文件格式或采用自定义存储格式</span><br><span class="line">  [LOCATION hdfs_path]  -- 指定表的存储位置</span><br><span class="line">  [TBLPROPERTIES (property_name&#x3D;property_value, ...)]  --指定表的属性</span><br><span class="line">  [AS select_statement];   --从查询结果创建表</span><br></pre></td></tr></table></figure>
<h3 id="2-2-内部表"><a href="#2-2-内部表" class="headerlink" title="2.2 内部表"></a>2.2 内部表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE emp(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2),</span><br><span class="line">  deptno INT)</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-外部表"><a href="#2-3-外部表" class="headerlink" title="2.3 外部表"></a>2.3 外部表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_external(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2),</span><br><span class="line">  deptno INT)</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">  LOCATION &#39;&#x2F;hive&#x2F;emp_external&#39;;</span><br></pre></td></tr></table></figure>
<p>使用 <code>desc format emp_external</code> 命令可以查看表的详细信息</p>
<h3 id="2-4-分区表"><a href="#2-4-分区表" class="headerlink" title="2.4 分区表"></a>2.4 分区表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_partition(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED BY (deptno INT)   -- 按照部门编号进行分区</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">  LOCATION &#39;&#x2F;hive&#x2F;emp_partition&#39;;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-分桶表"><a href="#2-5-分桶表" class="headerlink" title="2.5 分桶表"></a>2.5 分桶表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_bucket(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2),</span><br><span class="line">  deptno INT)</span><br><span class="line">  CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS  --按照员工编号散列到四个 bucket 中</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">  LOCATION &#39;&#x2F;hive&#x2F;emp_bucket&#39;;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-倾斜表"><a href="#2-6-倾斜表" class="headerlink" title="2.6 倾斜表"></a>2.6 倾斜表</h3><p>通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_skewed(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2)</span><br><span class="line">  )</span><br><span class="line">  SKEWED BY (empno) ON (66,88,100)  --指定 empno 的倾斜值 66,88,100</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">  LOCATION &#39;&#x2F;hive&#x2F;emp_skewed&#39;;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-临时表"><a href="#2-7-临时表" class="headerlink" title="2.7 临时表"></a>2.7 临时表</h3><p>临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制：</p>
<ul>
<li>不支持分区列；</li>
<li>不支持创建索引。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE emp_temp(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2)</span><br><span class="line">  )</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-CTAS创建表"><a href="#2-8-CTAS创建表" class="headerlink" title="2.8 CTAS创建表"></a>2.8 CTAS创建表</h3><p>支持从查询语句的结果创建表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE emp_copy AS SELECT * FROM emp WHERE deptno&#x3D;&#39;20&#39;;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-复制表结构"><a href="#2-9-复制表结构" class="headerlink" title="2.9 复制表结构"></a>2.9 复制表结构</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name  --创建表表名</span><br><span class="line">   LIKE existing_table_or_view_name  --被复制表的表名</span><br><span class="line">   [LOCATION hdfs_path]; --存储位置</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TEMPORARY EXTERNAL TABLE  IF NOT EXISTS  emp_co  LIKE emp</span><br></pre></td></tr></table></figure>
<h3 id="2-10-加载数据到表"><a href="#2-10-加载数据到表" class="headerlink" title="2.10 加载数据到表"></a>2.10 加载数据到表</h3><p>加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 加载数据到 emp 表中</span><br><span class="line">load data local inpath &quot;&#x2F;usr&#x2F;file&#x2F;emp.txt&quot; into table emp;</span><br></pre></td></tr></table></figure>
<p>其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录下载：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17 00:00:00	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-02-20 00:00:00	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-02-22 00:00:00	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-04-02 00:00:00	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-09-28 00:00:00	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-05-01 00:00:00	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-06-09 00:00:00	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-04-19 00:00:00	1500.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17 00:00:00	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-09-08 00:00:00	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-05-23 00:00:00	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-03 00:00:00	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-03 00:00:00	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-01-23 00:00:00	1300.00		10</span><br></pre></td></tr></table></figure>
<h2 id="三、修改表"><a href="#三、修改表" class="headerlink" title="三、修改表"></a>三、修改表</h2><p>大多数的表属性可以通过ALTER TABLE语句来进行修改。这种操作会修改元数据，但不会修改数据本身。这些语句可用于修改表模式中出现的错误、改变分区路径，以及其他一些操作。</p>
<h3 id="3-1-重命名表"><a href="#3-1-重命名表" class="headerlink" title="3.1 重命名表"></a>3.1 重命名表</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name;</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE emp_temp RENAME TO new_emp; --把 emp_temp 表重命名为 new_emp</span><br></pre></td></tr></table></figure>
<h3 id="3-2-增加修改和删除表分区"><a href="#3-2-增加修改和删除表分区" class="headerlink" title="3.2 增加修改和删除表分区"></a>3.2 增加修改和删除表分区</h3><ul>
<li>增加分区</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE log messages ADD IF NOT EXISTS PARTITION(year&#x3D;2011,month&#x3D;1,day&#x3D;1)LOCATION&quot;&#x2F;1ogs&#x2F;2011&#x2F;01&#x2F;01&#39;</span><br><span class="line">PARTITION(year &#x3D;2011,month&#x3D;1,day&#x3D;2)LOCATION1&#x2F;logs&#x2F;2011&#x2F;01&#x2F;02&#39;</span><br><span class="line">PARTITION(year&#x3D;2011,month&#x3D;1,day&#x3D;3)LOCATION&quot;&#x2F;1ogs&#x2F;2011&#x2F;01&#x2F;03&#39;</span><br></pre></td></tr></table></figure>
<p>当使用Hive v0.8.0或其后的版本时，在同一个查询中可以同时增加多个分区。一如既往，IF NOT EXISTS也是可选的，而且含义不变。</p>
<ul>
<li>修改分区路径</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE 1og messages PARTITION (year&#x3D;2011,month&#x3D;12,day&#x3D;2)</span><br><span class="line">SET LOCATION &#39;s3n:&#x2F;&#x2F;ourbucket&#x2F;1ogs&#x2F;2011&#x2F;01&#x2F;02&#39;;</span><br></pre></td></tr></table></figure>
<ul>
<li>删除分区</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE log messages DROP IF EXISTS PARTITION(year&#x3D;2011,month&#x3D;12, day&#x3D;2);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-修改列信息"><a href="#3-3-修改列信息" class="headerlink" title="3.3 修改列信息"></a>3.3 修改列信息</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type</span><br><span class="line">  [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT];</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 修改字段名和类型</span><br><span class="line">ALTER TABLE emp_temp CHANGE empno empno_new INT;</span><br><span class="line"> </span><br><span class="line">-- 修改字段 sal 的名称 并将其放置到 empno 字段后</span><br><span class="line">ALTER TABLE emp_temp CHANGE sal sal_new decimal(7,2)  AFTER ename;</span><br><span class="line"></span><br><span class="line">-- 为字段增加注释</span><br><span class="line">ALTER TABLE emp_temp CHANGE mgr mgr_new INT COMMENT &#39;this is column mgr&#39;;</span><br></pre></td></tr></table></figure>
<h3 id="3-4-新增列"><a href="#3-4-新增列" class="headerlink" title="3.4 新增列"></a>3.4 新增列</h3><p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE emp_temp ADD COLUMNS (address STRING COMMENT &#39;home address&#39;);</span><br></pre></td></tr></table></figure>
<h3 id="3-5删除或者替换列"><a href="#3-5删除或者替换列" class="headerlink" title="3.5删除或者替换列"></a>3.5删除或者替换列</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE log messages REPLACE COLUMNS(</span><br><span class="line">hours_mins_secs INT COMMENT&#39;hour,minute,seconds from timestamp&#39;, severity STRING COMMENT &#39;The message severity&#39;</span><br><span class="line">message STRING COMMENT &#39;The rest of the message&#39;);</span><br></pre></td></tr></table></figure>
<p>这个语句实际上重命名了之前的hms字段并且从之前的表定义的模式中移除了字段server 和process_id.因为是ALTER语句，所以只有表的元数据信息改变了。</p>
<h2 id="四、清空表-删除表"><a href="#四、清空表-删除表" class="headerlink" title="四、清空表/删除表"></a>四、清空表/删除表</h2><h3 id="4-1-清空表"><a href="#4-1-清空表" class="headerlink" title="4.1 清空表"></a>4.1 清空表</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 清空整个表或表指定分区中的数据</span><br><span class="line">TRUNCATE TABLE table_name [PARTITION (partition_column &#x3D; partition_col_value,  ...)];</span><br></pre></td></tr></table></figure>
<ul>
<li>目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 <code>Cannot truncate non-managed table XXXX</code>。</li>
</ul>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TRUNCATE TABLE emp_mgt_ptn PARTITION (deptno&#x3D;20);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-删除表"><a href="#4-2-删除表" class="headerlink" title="4.2 删除表"></a>4.2 删除表</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE];</span><br></pre></td></tr></table></figure>
<p>可以选择是否使用IF EXITST关键 子。如果没有使用这个关键字而且表并不存在的话，那么将会抛出一个错误信息。</p>
<ul>
<li>内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据；</li>
<li>外部表：只会删除表的元数据，不会删除 HDFS 上的数据；</li>
</ul>
<h2 id="五、其他命令"><a href="#五、其他命令" class="headerlink" title="五、其他命令"></a>五、其他命令</h2><h3 id="5-1-Describe"><a href="#5-1-Describe" class="headerlink" title="5.1 Describe"></a>5.1 Describe</h3><p>查看数据库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DESCRIBE|Desc DATABASE [EXTENDED] db_name;  --EXTENDED 是否显示额外属性</span><br></pre></td></tr></table></figure>
<p>查看表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DESCRIBE|Desc [EXTENDED|FORMATTED] table_name --FORMATTED 以友好的展现方式查看表详情</span><br></pre></td></tr></table></figure>
<h3 id="5-2-Show"><a href="#5-2-Show" class="headerlink" title="5.2 Show"></a>5.2 Show</h3><p><strong>1. 查看数据库列表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 语法</span><br><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE &#39;identifier_with_wildcards&#39;];</span><br><span class="line"></span><br><span class="line">-- 示例：</span><br><span class="line">SHOW DATABASES like &#39;hive*&#39;;</span><br></pre></td></tr></table></figure>
<p>LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 <code>*</code>（通配符）和 <code>|</code>（条件或）两个符号。例如 <code>employees</code>，<code>emp *</code>，<code>emp * | * ees</code>，所有这些都将匹配名为 <code>employees</code> 的数据库。</p>
<p><strong>2. 查看表的列表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 语法</span><br><span class="line">SHOW TABLES [IN database_name] [&#39;identifier_with_wildcards&#39;];</span><br><span class="line"></span><br><span class="line">-- 示例</span><br><span class="line">SHOW TABLES IN default;</span><br></pre></td></tr></table></figure>
<p><strong>3. 查看视图列表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW VIEWS [IN&#x2F;FROM database_name] [LIKE &#39;pattern_with_wildcards&#39;];   --仅支持 Hive 2.2.0 +</span><br></pre></td></tr></table></figure>
<p><strong>4. 查看表的分区列表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW PARTITIONS table_name;</span><br></pre></td></tr></table></figure>
<p><strong>5. 查看表/视图的创建语句</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW CREATE TABLE ([db_name.]table_name|view_name);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">LanguageManual DDL</a></p>
<p><strong>Hive编程指南</strong></p>
]]></content>
      <categories>
        <category>Hadoop生态圈</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>如何判断对象是否死亡?</title>
    <url>/2019/11/04/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”，判断对象的生死存活都有那些算法？</p>
<h2 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h2><p>实现原理：给对象中添加一个引用计数器，每当一个地方引用它是，计数器值就加1；任何时刻计数器为0的对象就是不可能再被使用的。<br>但是主流的Java虚拟机里面没有选用引用计数算法来管理内存，其中主要的原因是它很难解决对象之间相互引用的问题。<br>如下一个简单的例子：请看测试代码：对象objA和objB都有字段instance，赋值令 objA.instance=objB及objB.instance=objA，实际上这两个对象已经不可能再被访问了，但是他们因为互相引用这对方，导致引用计数都不为0，于是引用计数算法无法通过GC收集器回收他们。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">public class ReferenceCountingGC &#123;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * @Author: luomo</span><br><span class="line">     * @CreateTime: 2019&#x2F;11&#x2F;4</span><br><span class="line">     * @Description: 引用计数算法</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public Object instance &#x3D; null;</span><br><span class="line">    private  static final int _1MB&#x3D;1024*1024;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     *  这个成员属性的唯一意义就是占点内存，以便能在GC日志中看清楚是否被回收</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private  byte[] bigSize &#x3D;new byte[2 * _1MB];</span><br><span class="line">    public static void testGC()&#123;</span><br><span class="line">        ReferenceCountingGC objA &#x3D;new  ReferenceCountingGC();</span><br><span class="line">        ReferenceCountingGC objB &#x3D;new ReferenceCountingGC();</span><br><span class="line">        objA.instance&#x3D;objB;</span><br><span class="line">        objB.instance&#x3D;objA;</span><br><span class="line">        objA &#x3D;null;</span><br><span class="line">        objB &#x3D;null;</span><br><span class="line">        System.gc();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    public  static void main(String[] args)&#123;</span><br><span class="line">        testGC();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序运行结果如下：<br><img src="/2019/11/04/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/1.png" alt=" "><br>从运行结果中可以清楚看出虚拟机并没有因为互相引用就不回收他们，这也从侧面说明虚拟机并不是通过引用计数算法来判断对象是否存活的。</p>
<h2 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h2><p>实现原理：在主流的语言的主流实现中，比如Java、C#、甚至是古老的Lisp都是使用的可达性分析算法来判断对象是否存活的。</p>
<p>这个算法的核心思路就是通过一些列的“GC Roots”对象作为起始点，从这些对象开始往下搜索，搜索所经过的路径称之为“引用链”。</p>
<p>当一个对象到GC Roots没有任何引用链相连的时候，证明此对象是可以被回收的。如下图所示：<br><img src="/2019/11/04/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E6%AD%BB%E4%BA%A1/2.png" alt="图片说明"><br>在Java中，可作为GC Roots对象的列表：</p>
<ul>
<li>Java虚拟机栈中的引用对象。</li>
<li>本地方法栈中JNI（既一般说的Native方法）引用的对象。</li>
<li>方法区中类静态常量的引用对象。</li>
<li>方法区中常量的引用对象。</li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker概念</title>
    <url>/2020/07/13/Docker%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>为什么会有Docker的出现呢？可能很多不了解的人都会有这样的疑问。最近工作中有接触到Docker所以就有了这篇文章。</p>
<p>Docker 是世界领先的软件容器平台。<br>开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。<br>运维人员利用 Docker 可以在隔离容器中并行运行和管理应用，获得更好的计算密度。<br>企业利用 Docker 可以构建敏捷的软件交付管道，以更快的速度、更高的安全性和可靠的信誉为 Linux 和 Windows Server 应用发布新功能。</p>
<h1 id="为什么需要Docker？"><a href="#为什么需要Docker？" class="headerlink" title="为什么需要Docker？"></a>为什么需要Docker？</h1><h2 id="1-环境（切换-配置）麻烦"><a href="#1-环境（切换-配置）麻烦" class="headerlink" title="1.环境（切换/配置）麻烦"></a>1.环境（切换/配置）麻烦</h2><p>一般我们写程序，能接触到好几个环境：</p>
<ul>
<li>自己写代码的环境叫做开发环境</li>
<li>给测试去跑的环境叫做测试环境</li>
<li>测试完可以对外使用的叫做生产环境</li>
</ul>
<p>其实我们在学习编程中，很多时间都浪费在“环境”上：</p>
<ul>
<li>如果我现在重装了系统，我想要跑我的<code>war/jar</code>包，我得去安装一下JDK、Tomcat、MySQL等配置各种的环境变量才能跑起来。</li>
<li>开开心心地跟着博主给出的步骤去写Demo，但总是有Bug。(这里我将<strong>版本/依赖</strong>也归纳在环境的范畴里边)。</li>
<li>好不容易在测试环境下跑起来了，在生产环境就各种出错！</li>
<li>跟着教学视频做分布式/集群的项目，跑一堆的虚拟机，每个虚拟机都要安装对应的环境。</li>
</ul>
<h2 id="2-应用之间需要隔离"><a href="#2-应用之间需要隔离" class="headerlink" title="2.应用之间需要隔离"></a>2.应用之间需要隔离</h2><p>比如我写了两个应用(网站)，这两个应用部署在同一台服务器上，那可能会出现什么问题？</p>
<ul>
<li>如果一个应用出现了问题，导致CPU占100%。那另一个应用也会受到关联，跟着一起凉凉了。</li>
<li>这两个应用是完全不同技术栈的应用，比如一个<code>PHP</code>，一个<code>.NET</code>。这两个应用<strong>各种的依赖软件</strong>都安装在同一个服务器上，可能就会造成<strong>各种冲突/无法兼容</strong>，这可能调试就非常麻烦了。</li>
</ul>
<h2 id="Docker是如何解决上述的问题"><a href="#Docker是如何解决上述的问题" class="headerlink" title="Docker是如何解决上述的问题"></a>Docker是如何解决上述的问题</h2><h2 id="1-解决环境-切换-配置"><a href="#1-解决环境-切换-配置" class="headerlink" title="1.解决环境(切换/配置)"></a>1.解决环境(切换/配置)</h2><p>不知道大家有没有装过系统，比如说装Linux虚拟机，重装Windows系统，都是需要<strong>镜像</strong>的。</p>
<p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/1.png" alt></p>
<p>有了这个镜像，我们就可以<strong>运行</strong>这个镜像，来进行安装系统的操作(此处省略N个下一步)，于是我们的系统就装好了。一般来说，我们去官方渠道下载的镜像，都是<strong>纯净</strong>的。比如去官方下载Windows镜像，装完后之后桌面只有一个回收站。</p>
<p>但有过了解装系统的同学可能就会知道，有的镜像装完可能还有360这些软件，但系统的的确确是<strong>变了</strong>。简单来说，就是这些镜像<strong>添加</strong>了其他的东西(比如360软件、腾讯、千千静听等等软件)。</p>
<p>Docker也是这种思路，可以将我们的想要的环境<strong>构建</strong>(打包)成一个镜像，然后我们可以<strong>推送</strong>(发布)到网上去。想要用这个环 境的时候，在网上<strong>拉取</strong>一份就好了。</p>
<p>有了Docker，我们在搭环境的时候，跟以前的方式就不一样了。</p>
<ul>
<li><strong>之前</strong>：在开发环境构建出了一个war包，想跑到Linux下运行。我们得先在Linux下载好Java、Tomcat、MySQL，配置好对应的环境变量，将war包丢到Tomcat的webapps文件夹下，才能跑起来。</li>
<li><strong>现在</strong>：在Linux下直接拉取一份镜像(各种环境都配好了)，将镜像运行起来，把war包丢进去就好了。</li>
</ul>
<p>将Docker的镜像运行起来就是一两秒的事情而已，十分方便的。</p>
<h2 id="2-解决应用之间隔离"><a href="#2-解决应用之间隔离" class="headerlink" title="2.解决应用之间隔离"></a>2.解决应用之间隔离</h2><p>说到这里，就得提出一个大家可能不认识的概念：LXC(Linux Containers)—&gt;Linux容器。</p>
<p><strong>Linux</strong></p>
<blockquote>
<p>在Linux内核中，提供了<strong>cgroups</strong>功能，来达成资源的区隔化。它同时也提供了名称空间(<strong>namespace</strong>)区隔化的功能，<strong>使应用程序看到的操作系统环境被区隔成独立区间</strong>，包括进程树，网络，用户id，以及挂载的文件系统。</p>
</blockquote>
<p>简单来说就是：LXC是一个为Linux内核包含特征的<strong>用户接口</strong>。通过强大的API和简单的工具，它可以让Linux用户轻松的创建和托管系统或者应用程序容器。</p>
<h1 id="虚拟机和Docker"><a href="#虚拟机和Docker" class="headerlink" title="虚拟机和Docker"></a>虚拟机和Docker</h1><p>我之前也是用过虚拟机，虚拟机也能实现对应用的隔离，安装特定的镜像也能抛出我们想要的环境。虚拟机已经发展了很久了。为什么我们还需要Docker呢？</p>
<h2 id="虚拟机和Docker的区别"><a href="#虚拟机和Docker的区别" class="headerlink" title="虚拟机和Docker的区别"></a>虚拟机和Docker的区别</h2><p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/2.png" alt></p>
<p>一句话总结：Docker容器比虚拟机<strong>轻量</strong>多了！</p>
<p>Docker可以干嘛？</p>
<ul>
<li>将一整套环境打包封装成镜像，<strong>无需重复配置环境</strong>，解决环境带来的种种问题。</li>
<li>Docker容器间是进程隔离的，谁也不会影响谁。</li>
</ul>
<h1 id="Docker-基本命令"><a href="#Docker-基本命令" class="headerlink" title="Docker 基本命令"></a>Docker 基本命令</h1><h2 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello world"></a>Hello world</h2><p>Docker 允许你在容器内运行应用程序， 使用 docker run 命令来在一个容器内运行一个应用程序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run ubuntu:15.10 &#x2F;bin&#x2F;echo &quot;Hello world&quot;</span><br></pre></td></tr></table></figure>
<p>各个参数解析：</p>
<ul>
<li>docker：Docker的二进制执行文件。</li>
<li>run: 与前面的 docker 组合来运行一个容器。</li>
<li>ubuntu:15.10 指定要运行的镜像，Docker 首先从本地主机上查找镜像是否存在，如果不存在，Docker 就会从镜像仓库 Docker Hub 下载公共镜像。</li>
<li>/bin/echo “Hello world”: 在启动的容器里执行的命令</li>
</ul>
<h2 id="运行交互式的容器"><a href="#运行交互式的容器" class="headerlink" title="运行交互式的容器"></a>运行交互式的容器</h2><p>我们通过 docker 的两个参数 -i -t，让 docker 运行的容器实现<strong>“对话”</strong>的能力：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -i -t ubuntu:15.10 &#x2F;bin&#x2F;bash</span><br><span class="line">root@0123ce188bd8:&#x2F;#</span><br></pre></td></tr></table></figure>
<p>各个参数解析：</p>
<ul>
<li>-t：在新容器内指定一个伪终端。</li>
<li><strong>-i:</strong> 允许你对容器内的标准输入 (STDIN) 进行交互。</li>
</ul>
<p>注意第二行 <strong>root@0123ce188bd8:/#</strong>，此时我们已进入一个 ubuntu15.10 系统的容器</p>
<p>我们尝试在容器中运行命令 <strong>cat /proc/version</strong>和<strong>ls</strong>分别查看当前系统的版本信息和当前目录下的文件列表</p>
<p>我们可以通过运行 exit 命令或者使用 CTRL+D 来退出容器。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@0123ce188bd8:&#x2F;#  exit</span><br><span class="line">exit</span><br><span class="line">root@runoob:~#</span><br></pre></td></tr></table></figure>
<h2 id="启动容器后台模式"><a href="#启动容器后台模式" class="headerlink" title="启动容器后台模式"></a>启动容器后台模式</h2><p>使用一下命令创建一个以进程方式运行的容器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker run -d ubuntu:15.10 &#x2F;bin&#x2F;sh -c &quot;while true; do echo hello world; sleep 1; done&quot;</span><br><span class="line">2b1b7a428627c51ab8810d541d759f072b4fc75487eed05812646b8534a2fe63</span><br></pre></td></tr></table></figure>
<p>在输出中，我们没有看到期望的 “hello world”，而是一串长字符</p>
<p><strong>2b1b7a428627c51ab8810d541d759f072b4fc75487eed05812646b8534a2fe63</strong></p>
<p>这个长字符串叫做容器 ID，对每个容器来说都是唯一的，我们可以通过容器 ID 来查看对应的容器发生了什么。</p>
<p>首先，我们需要确认容器有在运行，可以通过 <strong>docker ps</strong> 来查看：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND              ...  </span><br><span class="line">5917eac21c36        ubuntu:15.10           &quot;&#x2F;bin&#x2F;sh -c &#39;while t…&quot;    ...</span><br></pre></td></tr></table></figure>
<p>输出详情介绍：</p>
<p><strong>CONTAINER ID:</strong> 容器 ID。</p>
<p><strong>IMAGE:</strong> 使用的镜像。</p>
<p><strong>COMMAND:</strong> 启动容器时运行的命令。</p>
<p><strong>CREATED:</strong> 容器的创建时间。</p>
<p><strong>STATUS:</strong> 容器状态。</p>
<p>状态有7种：</p>
<ul>
<li>created（已创建）</li>
<li>restarting（重启中）</li>
<li>running（运行中）</li>
<li>removing（迁移中）</li>
<li>paused（暂停）</li>
<li>exited（停止）</li>
<li>dead（死亡）</li>
</ul>
<p><strong>PORTS:</strong> 容器的端口信息和使用的连接类型（tcp\udp）。</p>
<p><strong>NAMES:</strong> 自动分配的容器名称。</p>
<p>在宿主主机内使用 <strong>docker logs</strong> 命令，查看容器内的标准输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker logs 2b1b7a428627</span><br></pre></td></tr></table></figure>
<h2 id="Docker容器使用"><a href="#Docker容器使用" class="headerlink" title="Docker容器使用"></a>Docker容器使用</h2><ul>
<li>Docker:我们可以直接输入docker命令查看到Docker客户端的所有命令选项</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~# docker</span><br></pre></td></tr></table></figure>
<ul>
<li>docker stats –help 更深入的了解指定的Docker命令使用方法   </li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~# docker stats --help</span><br></pre></td></tr></table></figure>
<ul>
<li>docker pull ubuntu：我们可以使用docker pull 命令来载入ubuntu镜像</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull ubuntu</span><br></pre></td></tr></table></figure>
<ul>
<li>启动容器，以下命令使用ubuntu镜像启动一个容器，参数为一命令行模式进入该容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -it ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<p>-i：交互式操作。</p>
<p>-t：终端</p>
<p>ubuntu：ubuntu 镜像。</p>
<p>/bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。</p>
<h3 id="启动已停止运行的容器"><a href="#启动已停止运行的容器" class="headerlink" title="启动已停止运行的容器"></a>启动已停止运行的容器</h3><ul>
<li>查看所有的容器命令如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/3.png" alt="image-20200714104353774"></p>
<ul>
<li>使用 docker start 启动一个已停止的容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker start b750bbbcfd88</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/4.png" alt="image-20200714104443037"></p>
<ul>
<li>后台运行，在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 <strong>-d</strong> 指定容器的运行模式。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -itd --name ubuntu-test ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/5.png" alt="image-20200714104607300"></p>
<ul>
<li>停止一个容器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker stop &lt;容器 ID&gt;</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/6.png" alt="image-20200714104701993"></p>
<ul>
<li>restart容器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker restart &lt;容器 ID&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>进入容器</li>
</ul>
<p>在使用 <strong>-d</strong> 参数时，容器启动后会进入后台。此时想要进入容器，可以通过以下指令进入：</p>
<p><strong>docker attach</strong></p>
<p><strong>docker exec</strong>：推荐大家使用 docker exec 命令，因为此退出容器终端，不会导致容器的停止。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker attach 1e560fca3906</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong> 如果从这个容器退出，会导致容器的停止。</p>
<p><strong>docker exec</strong>：推荐大家使用 docker exec 命令，因为此退出容器终端，不会导致容器的停止。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it 243c32535da7 &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong> 如果从这个容器退出，不会导致容器的停止，这就是为什么推荐大家使用 <strong>docker exec</strong> 的原因。</p>
<p>更多参数说明请使用 <strong>docker exec –help</strong> 命令查看。</p>
<ul>
<li>导出容器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker export 1e560fca3906 &gt; ubuntu.tar</span><br></pre></td></tr></table></figure>
<p>导出容器 1e560fca3906 快照到本地文件 ubuntu.tar。</p>
<ul>
<li>导入容器快照</li>
</ul>
<p>可以使用 docker import 从容器快照文件中再导入为镜像，以下实例将快照文件 ubuntu.tar 导入到镜像 test/ubuntu:v1:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat docker&#x2F;ubuntu.tar | docker import - test&#x2F;ubuntu:v1</span><br></pre></td></tr></table></figure>
<p>此外，也可以通过指定 URL 或者某个目录来导入，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker import http:&#x2F;&#x2F;example.com&#x2F;exampleimage.tgz example&#x2F;imagerepo</span><br></pre></td></tr></table></figure>
<ul>
<li>删除容器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker rm -f 1e560fca3906</span><br></pre></td></tr></table></figure>
<p>下面的命令可以清理掉所有处于终止状态的容器。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker container prune</span><br></pre></td></tr></table></figure>
<h2 id="运行一个web应用"><a href="#运行一个web应用" class="headerlink" title="运行一个web应用"></a>运行一个web应用</h2><p>前面我们运行的容器并没有一些什么特别的用处。</p>
<p>接下来让我们尝试使用 docker 构建一个 web 应用程序。</p>
<p>我们将在docker容器中运行一个 Python Flask 应用来运行一个web应用。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~# docker pull training&#x2F;webapp  # 载入镜像</span><br><span class="line">runoob@runoob:~# docker run -d -P training&#x2F;webapp python app.py</span><br></pre></td></tr></table></figure>
<ul>
<li>查看web 应用容器</li>
</ul>
<p>使用 docker ps 来查看我们正在运行的容器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~#  docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             ...        PORTS                 </span><br><span class="line">d3d5e39ed9d3        training&#x2F;webapp     &quot;python app.py&quot;     ...        0.0.0.0:32769-&gt;5000&#x2F;tcp</span><br></pre></td></tr></table></figure>
<p>这里多了端口信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PORTS</span><br><span class="line">0.0.0.0:32769-&gt;5000&#x2F;tcp</span><br></pre></td></tr></table></figure>
<p>Docker 开放了 5000 端口（默认 Python Flask 端口）映射到主机端口 32769 上。</p>
<p>这时我们可以通过浏览器访问WEB应用</p>
<p><img src="/2020/07/13/Docker%E6%A6%82%E5%BF%B5/7.png" alt="image-20200714105844095"></p>
<p>我们也可以通过 -p 参数来设置不一样的端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker run -d -p 5000:5000 training&#x2F;webapp python app.py</span><br></pre></td></tr></table></figure>
<ul>
<li>网络端口的快捷方式</li>
</ul>
<p>通过 <strong>docker ps</strong> 命令可以查看到容器的端口映射，<strong>docker</strong> 还提供了另一个快捷方式 <strong>docker port</strong>，使用 <strong>docker port</strong> 可以查看指定 （ID 或者名字）容器的某个确定端口映射到宿主机的端口号。</p>
<p>上面我们创建的 web 应用容器 ID 为 <strong>bf08b7f2cd89</strong> 名字为 <strong>wizardly_chandrasekhar</strong>。</p>
<p>我可以使用 <strong>docker port bf08b7f2cd89</strong> 或 <strong>docker port wizardly_chandrasekhar</strong> 来查看容器端口的映射情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker port bf08b7f2cd89</span><br><span class="line">5000&#x2F;tcp -&gt; 0.0.0.0:5000</span><br><span class="line">runoob@runoob:~$ docker port wizardly_chandrasekhar</span><br><span class="line">5000&#x2F;tcp -&gt; 0.0.0.0:5000</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>查看WEB应用程序日志</strong></li>
</ul>
<p>docker logs [ID或者名字] 可以查看容器内部的标准输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker logs -f bf08b7f2cd89</span><br><span class="line"> * Running on http:&#x2F;&#x2F;0.0.0.0:5000&#x2F; (Press CTRL+C to quit)</span><br><span class="line">192.168.239.1 - - [09&#x2F;May&#x2F;2016 16:30:37] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 -</span><br><span class="line">192.168.239.1 - - [09&#x2F;May&#x2F;2016 16:30:37] &quot;GET &#x2F;favicon.ico HTTP&#x2F;1.1&quot; 404 -</span><br></pre></td></tr></table></figure>
<p><strong>-f:</strong> 让 <strong>docker logs</strong> 像使用 <strong>tail -f</strong> 一样来输出容器内部的标准输出。</p>
<p>从上面，我们可以看到应用程序使用的是 5000 端口并且能够查看到应用程序的访问日志。</p>
<ul>
<li>查看WEB应用程序容器的进程</li>
</ul>
<p>我们还可以使用 docker top 来查看容器内部运行的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker top wizardly_chandrasekhar</span><br><span class="line">UID     PID         PPID          ...       TIME                CMD</span><br><span class="line">root    23245       23228         ...       00:00:00            python app.py</span><br></pre></td></tr></table></figure>
<ul>
<li>检查WEB应用程序</li>
</ul>
<p>使用<strong>docker inspect</strong> 来查看 Docker 的底层信息。它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker inspect wizardly_chandrasekhar</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2018-09-17T01:41:26.174228707Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;python&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;app.py&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;running&quot;,</span><br><span class="line">            &quot;Running&quot;: true,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 23245,</span><br><span class="line">            &quot;ExitCode&quot;: 0,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2018-09-17T01:41:26.494185806Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<ul>
<li>停止WEB应用容器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker stop wizardly_chandrasekhar   </span><br><span class="line">wizardly_chandrasekhar</span><br></pre></td></tr></table></figure>
<ul>
<li>重启WEB应用容器</li>
</ul>
<p>已经停止的容器，我们可以使用命令 docker start 来启动。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker start wizardly_chandrasekhar</span><br><span class="line">wizardly_chandrasekhar</span><br></pre></td></tr></table></figure>
<ul>
<li>docker ps -l 查询最后一次创建的容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#  docker ps -l </span><br><span class="line">CONTAINER ID        IMAGE                             PORTS                     NAMES</span><br><span class="line">bf08b7f2cd89        training&#x2F;webapp     ...        0.0.0.0:5000-&gt;5000&#x2F;tcp    wizardly_chandrasekhar</span><br></pre></td></tr></table></figure>
<ul>
<li>移除WEB应用容器</li>
</ul>
<p>我们可以使用 docker rm 命令来删除不需要的容器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker rm wizardly_chandrasekhar  </span><br><span class="line">wizardly_chandrasekhar</span><br></pre></td></tr></table></figure>
<p>删除容器时，容器必须是停止状态，否则会报如下错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker rm wizardly_chandrasekhar</span><br><span class="line">Error response from daemon: You cannot remove a running container bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85. Stop the container before attempting removal or force remove</span><br></pre></td></tr></table></figure>
<h1 id="Docker镜像使用"><a href="#Docker镜像使用" class="headerlink" title="Docker镜像使用"></a>Docker镜像使用</h1><p>当运行容器时，使用的镜像如果在本地中不存在，docker 就会自动从 docker 镜像仓库中下载，默认是从 Docker Hub 公共镜像源下载。</p>
<ul>
<li>列出镜像列表</li>
</ul>
<p>我们可以使用 <strong>docker images</strong> 来列出本地主机上的镜像。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker images           </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu              14.04               90d5884b1ee0        5 days ago          188 MB</span><br><span class="line">php                 5.6                 f40e9e0f10c8        9 days ago          444.8 MB</span><br><span class="line">nginx               latest              6f8d099c3adc        12 days ago         182.7 MB</span><br><span class="line">mysql               5.6                 f2e8d6c772c0        3 weeks ago         324.6 MB</span><br><span class="line">httpd               latest              02ef73cf1bc0        3 weeks ago         194.4 MB</span><br><span class="line">ubuntu              15.10               4e3b13c8a266        4 weeks ago         136.3 MB</span><br><span class="line">hello-world         latest              690ed74de00f        6 months ago        960 B</span><br><span class="line">training&#x2F;webapp     latest              6fae60ef3446        11 months ago       348.8 MB</span><br></pre></td></tr></table></figure>
<p>各个选项说明:</p>
<p><strong>REPOSITORY：</strong>表示镜像的仓库源</p>
<p><strong>TAG：</strong>镜像的标签</p>
<p><strong>IMAGE ID：</strong>镜像ID</p>
<p><strong>CREATED：</strong>镜像创建时间</p>
<p><strong>SIZE：</strong>镜像大小</p>
<ul>
<li>获取一个新的镜像</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Crunoob@runoob:~$ docker pull ubuntu:13.10</span><br></pre></td></tr></table></figure>
<ul>
<li>查找镜像</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$  docker search httpd</span><br></pre></td></tr></table></figure>
<ul>
<li>使用镜像</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">runoob@runoob:~$ docker run httpd</span><br></pre></td></tr></table></figure>
<ul>
<li>删除镜像</li>
</ul>
<p>镜像删除使用 <strong>docker rmi</strong> 命令，比如我们删除 hello-world 镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker rmi hello-world</span><br></pre></td></tr></table></figure>
<h1 id="Docker-DockerFile"><a href="#Docker-DockerFile" class="headerlink" title="Docker DockerFile"></a>Docker DockerFile</h1><p><strong>什么是DockerFile？</strong></p>
<p>Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。</p>
<p><strong>使用Dockerfile定制镜像</strong></p>
<p>1.下面以定制一个nginx镜像构建好的镜像内会有一个/usr/share/nginx/html/index.html文件</p>
<p>在一个空目录下，新建一个名为 Dockerfile 文件，并在文件内添加以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM nginx</span><br><span class="line">RUN echo &#39;这是一个本地构建的nginx镜像&#39; &gt; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html</span><br></pre></td></tr></table></figure>
<p><strong>2、FROM 和 RUN 指令的作用</strong></p>
<p><strong>FROM</strong>：定制的镜像都是基于 FROM 的镜像，这里的 nginx 就是定制需要的基础镜像。后续的操作都是基于 nginx。</p>
<p><strong>RUN</strong>：用于执行后面跟着的命令行命令。有以下俩种格式：</p>
<p><strong>注意</strong>：Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM centos</span><br><span class="line">RUN yum install wget</span><br><span class="line">RUN wget -O redis.tar.gz &quot;http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-5.0.3.tar.gz&quot;</span><br><span class="line">RUN tar -xvf redis.tar.gz</span><br><span class="line">以上执行会创建 3 层镜像。可简化为以下格式：</span><br><span class="line">FROM centos</span><br><span class="line">RUN yum install wget \</span><br><span class="line">    &amp;&amp; wget -O redis.tar.gz &quot;http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-5.0.3.tar.gz&quot; \</span><br><span class="line">    &amp;&amp; tar -xvf redis.tar.gz</span><br></pre></td></tr></table></figure>
<h1 id="开始构建镜像"><a href="#开始构建镜像" class="headerlink" title="开始构建镜像"></a>开始构建镜像</h1><p>在 Dockerfile 文件的存放目录下，执行构建动作。</p>
<p>以下示例，通过目录下的 Dockerfile 构建一个 nginx:test（镜像名称:镜像标签）。</p>
<p><strong>注</strong>：最后的 <strong>.</strong> 代表本次执行的上下文路径，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker build -t nginx:test .</span><br></pre></td></tr></table></figure>
<p><img src="https://www.runoob.com/wp-content/uploads/2019/11/dockerfile2.png" alt="img"></p>
<p>以上显示，说明已经构建成功。</p>
<h3 id="上下文路径"><a href="#上下文路径" class="headerlink" title="上下文路径"></a>上下文路径</h3><p>上一节中，有提到指令最后一个 <strong>.</strong> 是上下文路径，那么什么是上下文路径呢？</p>
<p>$ docker build -t nginx:test .</p>
<p>上下文路径，是指 docker 在构建镜像，有时候想要使用到本机的文件（比如复制），docker build 命令得知这个路径后，会将路径下的所有内容打包。</p>
<h2 id="Dockerfile基本结构"><a href="#Dockerfile基本结构" class="headerlink" title="Dockerfile基本结构"></a>Dockerfile基本结构</h2><p>Dockerfile 由一行行命令语句组成，并且支持已 # 开头的注释行。</p>
<p>一般而言，Dockerfile 的内容分为四个部分：<br><strong>基础镜像信息</strong></p>
<p><strong>维护者信息</strong></p>
<p><strong>镜像操作指令</strong></p>
<p><strong>容器启动时执行指令</strong></p>
<p><strong>Dockerfile完整demo</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># This dockerfile demo for project build to docker images</span><br><span class="line"># VERSION 2</span><br><span class="line"># Author: Shawn_xiao</span><br><span class="line"># Command format: Instruction [arguments &#x2F; command] …</span><br><span class="line"># 2018&#x2F;10&#x2F;10- firstversion: xiao</span><br><span class="line"># 2018&#x2F;10&#x2F;11- chanege the tomcat version</span><br><span class="line"></span><br><span class="line"># 第一行必须指定基础容器，建议使用aipln类型的小容器</span><br><span class="line">FROM tomcat:8</span><br><span class="line"></span><br><span class="line"># 维护者信息(可选)</span><br><span class="line">MAINTAINER xiaojianjun xiaojianjun@tansun.com.cn</span><br><span class="line"></span><br><span class="line"># LABEL (可选) 标签信息(自定义信息,多标签放一行)</span><br><span class="line">LABEL app.maintainer&#x3D;xiaojianjun</span><br><span class="line">LABEL app.version&#x3D;&quot;1.0&quot; app.host&#x3D;&#39;bestxiao.cn&#39; description&#x3D;&quot;这个app产品构建&quot;</span><br><span class="line"></span><br><span class="line"># ENV  (可选)环境变量(指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持 </span><br><span class="line">ENV JAVA_HOME &#x2F;opt&#x2F;java_jdk&#x2F;bin</span><br><span class="line">ENV PG_VERSION 9.3.4</span><br><span class="line">ENV PATH &#x2F;usr&#x2F;local&#x2F;postgres-$PG_MAJOR&#x2F;bin:$PATH</span><br><span class="line"></span><br><span class="line"># USER (可选) 指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户,前面的RUN 不受影响</span><br><span class="line"># RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres </span><br><span class="line">USER postgres</span><br><span class="line"></span><br><span class="line"># WORKDIT 后续的 RUN、CMD、ENTRYPOINT 指令配置容器内的工作目录</span><br><span class="line">WORKDIR &#x2F;path&#x2F;to&#x2F;workdir</span><br><span class="line"></span><br><span class="line"># ADD&#x2F;COPY 将外部文件copy到容器中。区别是ADD可以使用URL，还可以是tar</span><br><span class="line"># COPY只能使用dockerfile所在目录</span><br><span class="line"># ADD &lt;src&gt; &lt;dest&gt;</span><br><span class="line"># COPY &lt;src&gt; &lt;dest&gt;</span><br><span class="line">COPY target&#x2F;tomcat-release.war &#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps&#x2F;</span><br><span class="line"></span><br><span class="line"># RUN 镜像的操作指令</span><br><span class="line"># RUN &lt;command&gt; [“executable”, “param1”, “param2”]。</span><br><span class="line">RUN echo “deb http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; raring main universe” &gt;&gt; &#x2F;etc&#x2F;apt&#x2F;sources.list</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y nginx</span><br><span class="line">RUN mkdir &#x2F;opt&#x2F;deploy&#x2F;</span><br><span class="line">RUN echo “\ndaemon off;” &gt;&gt; &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line"></span><br><span class="line"># EXPOSE 容器启动后需要暴露的端口</span><br><span class="line">EXPOSE 22 80 8443 8080</span><br><span class="line"></span><br><span class="line"># VOLUME 本地或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。</span><br><span class="line">#VOLUME [&quot;&#x2F;data&quot;]</span><br><span class="line">VOLUME [&quot;&#x2F;data&#x2F;postgres&quot;, &quot;&#x2F;other&#x2F;path&#x2F;&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ENTRYPOINT  容器启动后执行命令，不会被docker run提供的参数覆盖，只能有一个ENTRYPOINT,</span><br><span class="line"># 多个ENTRYPOINT，以最后一个为准</span><br><span class="line">#ENTRYPOINT [“executable”, “param1”, “param2”]</span><br><span class="line">#ENTRYPOINT command param param2</span><br><span class="line">ENTRYPOINT echo &quot;helloDocker&quot;  </span><br><span class="line"></span><br><span class="line"># 容器启动时执行指令,每个 Dockerfile 只能有一条 CMD 命令</span><br><span class="line">#CMD [“executable”, “param1”, “param2”] 使用 exec 执行，推荐方式。</span><br><span class="line">#CMD command param1 param2 在 &#x2F;bin&#x2F;sh 中执行，提供给需要交互的应用。</span><br><span class="line">#CMD [“param1”, “param2”] 提供给 ENTRYPOINT 的默认参数。</span><br><span class="line">CMD &#x2F;usr&#x2F;sbin&#x2F;nginx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ONBUILD 配置当所创建的镜像作为其他新创建镜像的基础镜像时，所执行的操作指令。例如，Dockerfile 使用如下的内容创建了镜像 image-A。-- 很少使用</span><br><span class="line"></span><br><span class="line"># ONBUILD ADD . &#x2F;app&#x2F;src</span><br><span class="line"># ONBUILD RUN &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python-build –dir &#x2F;app&#x2F;src</span><br></pre></td></tr></table></figure>
<p><strong>简短demo</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM centos</span><br><span class="line">LABEL version&#x3D;&quot;1.0&quot; description&#x3D;&quot;centos7&quot; by&#x3D;&quot;测试&quot;</span><br><span class="line">ENV MYPATH &#x2F;usr&#x2F;local</span><br><span class="line">WORKDIR $MYPATH</span><br><span class="line"></span><br><span class="line">RUN yum -y install java-1.8.0-openjdk </span><br><span class="line"></span><br><span class="line">EXPOSE 80</span><br><span class="line"></span><br><span class="line">CMD echo &quot;------success------OK------&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定镜像</span><br><span class="line">FROM primetoninc&#x2F;jdk:1.8</span><br><span class="line">#拷贝宿主机的app.jar</span><br><span class="line">COPY app.jar app.jar</span><br><span class="line">#配置端口</span><br><span class="line">EXPOSE 8080</span><br><span class="line">#运行app.jar包</span><br><span class="line">ENTRYPOINT exec java -jar app.jar</span><br></pre></td></tr></table></figure>
<h2 id="DockerFile关键指令"><a href="#DockerFile关键指令" class="headerlink" title="DockerFile关键指令"></a>DockerFile关键指令</h2><h3 id="FROM"><a href="#FROM" class="headerlink" title="FROM"></a><strong>FROM</strong></h3><p>格式为 FROM <image> 或 FROM<image>:<tag>。<br>Dockerfile 的第一条指令必须为 FROM 指令。并且，如果在同一个 Dockerfile 中创建多个镜像时，可以使用多个 FROM 指令。</tag></image></image></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 第一行必须指定基础容器，这里的是tomcat8</span><br><span class="line">FROM tomcat:8</span><br></pre></td></tr></table></figure>
<h3 id="MAINTAINER"><a href="#MAINTAINER" class="headerlink" title="MAINTAINER"></a><strong>MAINTAINER</strong></h3><p>格式为 MAINTAINER <name>，指定维护者信息。</name></p>
<p>注意：MAINTAINER 指令已经被抛弃，建议使用 LABEL 指令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 维护者信息(可选)建议用LABEL 指令</span><br><span class="line">MAINTAINER xiaojianjun xiaojianjun@tansun.com.cn</span><br></pre></td></tr></table></figure>
<h3 id="LABEL"><a href="#LABEL" class="headerlink" title="LABEL"></a><strong>LABEL</strong></h3><p>LABEL 指令为镜像添加标签。一个 LABEL 就是一个键值对，也可以一行指定多个键值对。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#多行指定信息</span><br><span class="line">LABEL com.example.label-with-value&#x3D;&quot;foo&quot;</span><br><span class="line">LABEL version&#x3D;&quot;1.0&quot;</span><br><span class="line">LABEL description&#x3D;&quot;This text illustrates \that label-values can span multiple lines.&quot;</span><br><span class="line"></span><br><span class="line">#一行指定多个键值对</span><br><span class="line">LABEL app.version&#x3D;&quot;1.0&quot; app.host&#x3D;&#39;bestxiao.cn&#39; description&#x3D;&quot;这个app产品构建&quot;</span><br></pre></td></tr></table></figure>
<p><strong>如果新添加的 LABEL 和已有的 LABEL 同名，则新值会覆盖掉旧值。</strong></p>
<h3 id="RUN"><a href="#RUN" class="headerlink" title="RUN"></a><strong>RUN</strong></h3><p>每条 RUN 指令将在当前镜像的基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用 \ 来换行。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RUN yum -y install java-1.8.0-openjdk</span><br></pre></td></tr></table></figure>
<h3 id="CMD"><a href="#CMD" class="headerlink" title="CMD"></a><strong>CMD</strong></h3><p>指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条 CMD 命令，只有最后一条会被执行。如果用户在启动容器时指定了要运行的命令，则会覆盖掉 CMD 指定的命令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMD echo &quot;------success------OK------&quot;</span><br></pre></td></tr></table></figure>
<h3 id="EXPOSE"><a href="#EXPOSE" class="headerlink" title="EXPOSE"></a><strong>EXPOSE</strong></h3><p>告诉 Docker 服务，容器需要暴露的端口号，供互联系统使用。在启动容器时需要通过 -P 参数让 Docker 主机分配一个端口转发到指定的端口。使用 -p 参数则可以具体指定主机上哪个端口映射过来。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">EXPOSE 22 80 8443 8080</span><br></pre></td></tr></table></figure>
<h3 id="ENV"><a href="#ENV" class="headerlink" title="ENV"></a><strong>ENV</strong></h3><p>格式为 ENV <key> <value>。指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。</value></key></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ENV PG_MAJOR 9.3</span><br><span class="line">ENV PG_VERSION 9.3.4</span><br><span class="line">RUN curl -SL http:&#x2F;&#x2F;example.com&#x2F;postgres-$PG_VERSION.tar.xz | tar -xJC &#x2F;usr&#x2F;src&#x2F;postgress &amp;&amp; …</span><br><span class="line">ENV PATH &#x2F;usr&#x2F;local&#x2F;postgres-$PG_MAJOR&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>
<h3 id="ADD"><a href="#ADD" class="headerlink" title="ADD"></a><strong>ADD</strong></h3><p>该命令将复制指定的 <src> 到容器中的 <dest>。其中 <src> 可以是 Dockerfile 所在目录的一个相对路径(文件或目录)；也可以是一个 URL；还可以是一个 tar 文件(自动解压为目录)。</src></dest></src></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ADD &lt;src&gt; &lt;dest&gt;</span><br></pre></td></tr></table></figure>
<h3 id="COPY"><a href="#COPY" class="headerlink" title="COPY"></a><strong>COPY</strong></h3><p>复制本地主机的 <src> (为 Dockerfile 所在目录的相对路径，文件或目录) 为容器中的 <dest>。目标路径不存在时，会自动创建。当使用本地目录为源目录时，推荐使用 COPY。</dest></src></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">COPY &lt;src&gt; &lt;dest&gt;</span><br></pre></td></tr></table></figure>
<h3 id="ENTRYPOINT"><a href="#ENTRYPOINT" class="headerlink" title="ENTRYPOINT"></a><strong>ENTRYPOINT</strong></h3><p>配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。<br>每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个 ENTRYPOINT 时，只有最后一个生效。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ENTRYPOINT [“executable”, “param1”, “param2”]</span><br><span class="line">ENTRYPOINT command param1 param2 (shell 中执行)</span><br></pre></td></tr></table></figure>
<h3 id="VOLUME"><a href="#VOLUME" class="headerlink" title="VOLUME"></a><strong>VOLUME</strong></h3><p>使用 VOLUME 指令添加多个数据卷，创建一个可以从本地或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">VOLUME [&quot;&#x2F;data&quot;]</span><br><span class="line">VOLUME [&quot;&#x2F;data1&quot;, &quot;&#x2F;data2&quot;]</span><br></pre></td></tr></table></figure>
<h3 id="USER"><a href="#USER" class="headerlink" title="USER"></a><strong>USER</strong></h3><p>指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">USER daemon</span><br><span class="line">或</span><br><span class="line">RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres</span><br></pre></td></tr></table></figure>
<h3 id="WORKDIR"><a href="#WORKDIR" class="headerlink" title="WORKDIR"></a><strong>WORKDIR</strong></h3><p>为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># WORKDIT 后续的 RUN、CMD、ENTRYPOINT 指令配置容器内的工作目录</span><br><span class="line">WORKDIR &#x2F;path&#x2F;to&#x2F;workdir</span><br></pre></td></tr></table></figure>
<h3 id="ONBUILD"><a href="#ONBUILD" class="headerlink" title="ONBUILD"></a><strong>ONBUILD</strong></h3><p>配置当所创建的镜像作为其他新创建镜像的基础镜像时，所执行的操作指令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM image-A#automatically run the followingADD ONBUILD ADD . &#x2F;app&#x2F;srcONBUILD RUN &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python-build –dir &#x2F;app&#x2F;src</span><br></pre></td></tr></table></figure>
<p>如果基于 image-A 创建新的镜像时，新的 Dockerfile 中使用 FROM image-A 指定基础镜像时，会自动执行 ONBUILD 指令内容，等价于在后面添加了两条指令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM image-A#automatically run the followingADD . &#x2F;app&#x2F;srcRUN &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python-build –dir &#x2F;app&#x2F;src</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot Hello World</title>
    <url>/2020/07/26/SpringBoot-Hello-World/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>标签</tag>
      </tags>
  </entry>
</search>
